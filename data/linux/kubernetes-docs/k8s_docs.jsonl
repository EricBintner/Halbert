{"text": "Command line tool (kubectl) | Kubernetes\nCommand line tool (kubectl)\nKubernetes provides a command line tool for communicating with a Kubernetes cluster's\ncontrol plane\n,\nusing the Kubernetes API.\nThis tool is named\nkubectl\n.\nFor configuration,\nkubectl\nlooks for a file named\nconfig\nin the\n$HOME/.kube\ndirectory.\nYou can specify other\nkubeconfig\nfiles by setting the\nKUBECONFIG\nenvironment variable or by setting the\n--kubeconfig\nflag.\nThis overview covers\nkubectl\nsyntax, describes the command operations, and provides common examples.\nFor details about each command, including all the supported flags and subcommands, see the\nkubectl\nreference documentation.\nFor installation instructions, see\nInstalling kubectl\n;\nfor a quick guide, see the\ncheat sheet\n.\nIf you're used to using the\ndocker\ncommand-line tool,\nkubectl\nfor Docker Users\nexplains some equivalent commands for Kubernetes.\nSyntax\nUse the following syntax to run\nkubectl\ncommands from your terminal window:\nkubectl\n[\ncommand\n]\n[\nTYPE\n]\n[\nNAME\n]\n[\nflags\n]\nwhere\ncommand\n,\nTYPE\n,\nNAME\n, and\nflags\nare:\ncommand\n: Specifies the operation that you want to perform on one or more resources,\nfor example\ncreate\n,\nget\n,\ndescribe\n,\ndelete\n.\nTYPE\n: Specifies the\nresource type\n. Resource types are case-insensitive and\nyou can specify the singular, plural, or abbreviated forms.\nFor example, the following commands produce the same output:\nkubectl get pod pod1\nkubectl get pods pod1\nkubectl get po pod1\nNAME\n: Specifies the name of the resource. Names are case-sensitive. If the name is omitted,\ndetails for all resources are displayed, for example\nkubectl get pods\n.\nWhen performing an operation on multiple resources, you can specify each resource by\ntype and name or specify one or more files:\nTo specify resources by type and name:\nTo group resources if they are all the same type:\nTYPE1 name1 name2 name<#>\n.\nExample:\nkubectl get pod example-pod1 example-pod2\nTo specify multiple resource types individually:\nTYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE<#>/name<#>\n.\nExample:\nkubectl get pod/example-pod1 replicationcontroller/example-rc1\nTo specify resources with one or more files:\n-f file1 -f file2 -f file<#>\nUse YAML rather than JSON\nsince YAML tends to be more user-friendly, especially for configuration files.\nExample:\nkubectl get -f ./pod.yaml\nflags\n: Specifies optional flags. For example, you can use the\n-s\nor\n--server\nflags\nto specify the address and port of the Kubernetes API server.\nCaution:\nFlags that you specify from the command line override default values and any corresponding environment variables.\nIf you need help, run\nkubectl help\nfrom the terminal window.\nIn-cluster authentication and namespace overrides\nBy default\nkubectl\nwill first determine if it is running within a pod, and thus in a cluster.\nIt starts by checking for the\nKUBERNETES_SERVICE_HOST\nand\nKUBERNETES_SERVICE_PORT\nenvironment\nvariables and the existence of a service account token file at\n/var/run/secrets/kubernetes.io/serviceaccount/token\n.\nIf all three are found in-cluster authentication is assumed.\nTo maintain backwards compatibility, if the\nPOD_NAMESPACE\nenvironment variable is set\nduring in-cluster authentication it will override the default namespace from the\nservice account token. Any manifests or tools relying on namespace defaulting will be affected by this.\nPOD_NAMESPACE\nenvironment variable\nIf the\nPOD_NAMESPACE\nenvironment variable is set, cli operations on namespaced resources\nwill default to the variable value. For example, if the variable is set to\nseattle\n,\nkubectl get pods\nwould return pods in the\nseattle\nnamespace. This is because pods are\na namespaced resource, and no namespace was provided in the command. Review the output\nof\nkubectl api-resources\nto determine if a resource is namespaced.\nExplicit use of\n--namespace <value>\noverrides this behavior.\nHow kubectl handles ServiceAccount tokens\nIf:\nthere is Kubernetes service account token file mounted at\n/var/run/secrets/kubernetes.io/serviceaccount/token\n, and\nthe\nKUBERNETES_SERVICE_HOST\nenvironment variable is set, and\nthe\nKUBERNETES_SERVICE_PORT\nenvironment variable is set, and\nyou don't explicitly specify a namespace on the kubectl command line\nthen kubectl assumes it is running in your cluster. The kubectl tool looks up the\nnamespace of that ServiceAccount (this is the same as the namespace of the Pod)\nand acts against that namespace. This is different from what happens outside of a\ncluster; when kubectl runs outside a cluster and you don't specify a namespace,\nthe kubectl command acts against the namespace set for the current context in your\nclient configuration. To change the default namespace for your kubectl you can use the\nfollowing command:\nkubectl config set-context --current --namespace\n=\n<namespace-name>\nOperations\nThe following table includes short descriptions and the general syntax for all of the\nkubectl\noperations:\nOperation\nSyntax\nDescription\nalpha\nkubectl alpha SUBCOMMAND [flags]\nList the available commands that correspond to alpha features, which are not enabled in Kubernetes clusters by default.\nannotate\nkubectl annotate (-f FILENAME | TYPE NAME | TYPE/NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--overwrite] [--all] [--resource-version=version] [flags]\nAdd or update the annotations of one or more resources.\napi-resources\nkubectl api-resources [flags]\nList the API resources that are available.\napi-versions\nkubectl api-versions [flags]\nList the API versions that are available.\napply\nkubectl apply -f FILENAME [flags]\nApply a configuration change to a resource from a file or stdin.\nattach\nkubectl attach POD -c CONTAINER [-i] [-t] [flags]\nAttach to a running container either to view the output stream or interact with the container (stdin).\nauth\nkubectl auth [flags] [options]\nInspect authorization.\nautoscale\nkubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [flags]\nAutomatically scale the set of pods that are managed by a replication controller.\ncertificate\nkubectl certificate SUBCOMMAND [options]\nModify certificate resources.\ncluster-info\nkubectl cluster-info [flags]\nDisplay endpoint information about the master and services in the cluster.\ncompletion\nkubectl completion SHELL [options]\nOutput shell completion code for the specified shell (bash or zsh).\nconfig\nkubectl config SUBCOMMAND [flags]\nModifies kubeconfig files. See the individual subcommands for details.\nconvert\nkubectl convert -f FILENAME [options]\nConvert config files between different API versions. Both YAML and JSON formats are accepted. Note - requires\nkubectl-convert\nplugin to be installed.\ncordon\nkubectl cordon NODE [options]\nMark node as unschedulable.\ncp\nkubectl cp <file-spec-src> <file-spec-dest> [options]\nCopy files and directories to and from containers.\ncreate\nkubectl create -f FILENAME [flags]\nCreate one or more resources from a file or stdin.\ndelete\nkubectl delete (-f FILENAME | TYPE [NAME | /NAME | -l label | --all]) [flags]\nDelete resources either from a file, stdin, or specifying label selectors, names, resource selectors, or resources.\ndescribe\nkubectl describe (-f FILENAME | TYPE [NAME_PREFIX | /NAME | -l label]) [flags]\nDisplay the detailed state of one or more resources.\ndiff\nkubectl diff -f FILENAME [flags]\nDiff file or stdin against live configuration.\ndrain\nkubectl drain NODE [options]\nDrain node in preparation for maintenance.\nedit\nkubectl edit (-f FILENAME | TYPE NAME | TYPE/NAME) [flags]\nEdit and update the definition of one or more resources on the server by using the default editor.\nevents\nkubectl events\nList events\nexec\nkubectl exec POD [-c CONTAINER] [-i] [-t] [flags] [-- COMMAND [args...]]\nExecute a command against a container in a pod.\nexplain\nkubectl explain TYPE [--recursive=false] [flags]\nGet documentation of various resources. For instance pods, nodes, services, etc.\nexpose\nkubectl expose (-f FILENAME | TYPE NAME | TYPE/NAME) [--port=port] [--protocol=TCP|UDP] [--target-port=number-or-name] [--name=name] [--external-ip=external-ip-of-service] [--type=type] [flags]\nExpose a replication controller, service, or pod as a new Kubernetes service.\nget\nkubectl get (-f FILENAME | TYPE [NAME | /NAME | -l label]) [--watch] [--sort-by=FIELD] [[-o | --output]=OUTPUT_FORMAT] [flags]\nList one or more resources.\nkustomize\nkubectl kustomize <dir> [flags] [options]\nList a set of API resources generated from instructions in a kustomization.yaml file. The argument must be the path to the directory containing the file, or a git repository URL with a path suffix specifying same with respect to the repository root.\nlabel\nkubectl label (-f FILENAME | TYPE NAME | TYPE/NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--overwrite] [--all] [--resource-version=version] [flags]\nAdd or update the labels of one or more resources.\nlogs\nkubectl logs POD [-c CONTAINER] [--follow] [flags]\nPrint the logs for a container in a pod.\noptions\nkubectl options\nList of global command-line options, which apply to all commands.\npatch\nkubectl patch (-f FILENAME | TYPE NAME | TYPE/NAME) --patch PATCH [flags]\nUpdate one or more fields of a resource by using the strategic merge patch process.\nplugin\nkubectl plugin [flags] [options]\nProvides utilities for interacting with plugins.\nport-forward\nkubectl port-forward POD [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N] [flags]\nForward one or more local ports to a pod.\nproxy\nkubectl proxy [--port=PORT] [--www=static-dir] [--www-prefix=prefix] [--api-prefix=prefix] [flags]\nRun a proxy to the Kubernetes API server.\nreplace\nkubectl replace -f FILENAME\nReplace a resource from a file or stdin.\nrollout\nkubectl rollout SUBCOMMAND [options]\nManage the rollout of a resource. Valid resource types include: deployments, daemonsets and statefulsets.\nrun\nkubectl run NAME --image=image [--env=\"key=value\"] [--port=port] [--dry-run=server|client|none] [--overrides=inline-json] [flags]\nRun a specified image on the cluster.\nscale\nkubectl scale (-f FILENAME | TYPE NAME | TYPE/NAME) --replicas=COUNT [--resource-version=version] [--current-replicas=count] [flags]\nUpdate the size of the specified replication controller.\nset\nkubectl set SUBCOMMAND [options]\nConfigure application resources.\ntaint\nkubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N [options]\nUpdate the taints on one or more nodes.\ntop\nkubectl top (POD | NODE) [flags] [options]\nDisplay Resource (CPU/Memory/Storage) usage of pod or node.\nuncordon\nkubectl uncordon NODE [options]\nMark node as schedulable.\nversion\nkubectl version [--client] [flags]\nDisplay the Kubernetes version running on the client and server.\nwait\nkubectl wait ([-f FILENAME] | resource.group/resource.name | resource.group [(-l label | --all)]) [--for=delete|--for condition=available] [options]\nExperimental: Wait for a specific condition on one or many resources.\nTo learn more about command operations, see the\nkubectl\nreference documentation.\nResource types\nThe following table includes a list of all the supported resource types and their abbreviated aliases.\n(This output can be retrieved from\nkubectl api-resources\n, and was accurate as of Kubernetes 1.25.0)\nNAME\nSHORTNAMES\nAPIVERSION\nNAMESPACED\nKIND\nbindings\nv1\ntrue\nBinding\ncomponentstatuses\ncs\nv1\nfalse\nComponentStatus\nconfigmaps\ncm\nv1\ntrue\nConfigMap\nendpoints\nep\nv1\ntrue\nEndpoints\nevents\nev\nv1\ntrue\nEvent\nlimitranges\nlimits\nv1\ntrue\nLimitRange\nnamespaces\nns\nv1\nfalse\nNamespace\nnodes\nno\nv1\nfalse\nNode\npersistentvolumeclaims\npvc\nv1\ntrue\nPersistentVolumeClaim\npersistentvolumes\npv\nv1\nfalse\nPersistentVolume\npods\npo\nv1\ntrue\nPod\npodtemplates\nv1\ntrue\nPodTemplate\nreplicationcontrollers\nrc\nv1\ntrue\nReplicationController\nresourcequotas\nquota\nv1\ntrue\nResourceQuota\nsecrets\nv1\ntrue\nSecret\nserviceaccounts\nsa\nv1\ntrue\nServiceAccount\nservices\nsvc\nv1\ntrue\nService\nmutatingwebhookconfigurations\nadmissionregistration.k8s.io/v1\nfalse\nMutatingWebhookConfiguration\nvalidatingwebhookconfigurations\nadmissionregistration.k8s.io/v1\nfalse\nValidatingWebhookConfiguration\ncustomresourcedefinitions\ncrd,crds\napiextensions.k8s.io/v1\nfalse\nCustomResourceDefinition\napiservices\napiregistration.k8s.io/v1\nfalse\nAPIService\ncontrollerrevisions\napps/v1\ntrue\nControllerRevision\ndaemonsets\nds\napps/v1\ntrue\nDaemonSet\ndeployments\ndeploy\napps/v1\ntrue\nDeployment\nreplicasets\nrs\napps/v1\ntrue\nReplicaSet\nstatefulsets\nsts\napps/v1\ntrue\nStatefulSet\ntokenreviews\nauthentication.k8s.io/v1\nfalse\nTokenReview\nlocalsubjectaccessreviews\nauthorization.k8s.io/v1\ntrue\nLocalSubjectAccessReview\nselfsubjectaccessreviews\nauthorization.k8s.io/v1\nfalse\nSelfSubjectAccessReview\nselfsubjectrulesreviews\nauthorization.k8s.io/v1\nfalse\nSelfSubjectRulesReview\nsubjectaccessreviews\nauthorization.k8s.io/v1\nfalse\nSubjectAccessReview\nhorizontalpodautoscalers\nhpa\nautoscaling/v2\ntrue\nHorizontalPodAutoscaler\ncronjobs\ncj\nbatch/v1\ntrue\nCronJob\njobs\nbatch/v1\ntrue\nJob\ncertificatesigningrequests\ncsr\ncertificates.k8s.io/v1\nfalse\nCertificateSigningRequest\nleases\ncoordination.k8s.io/v1\ntrue\nLease\nendpointslices\ndiscovery.k8s.io/v1\ntrue\nEndpointSlice\nevents\nev\nevents.k8s.io/v1\ntrue\nEvent\nflowschemas\nflowcontrol.apiserver.k8s.io/v1beta2\nfalse\nFlowSchema\nprioritylevelconfigurations\nflowcontrol.apiserver.k8s.io/v1beta2\nfalse\nPriorityLevelConfiguration\ningressclasses\nnetworking.k8s.io/v1\nfalse\nIngressClass\ningresses\ning\nnetworking.k8s.io/v1\ntrue\nIngress\nnetworkpolicies\nnetpol\nnetworking.k8s.io/v1\ntrue\nNetworkPolicy\nruntimeclasses\nnode.k8s.io/v1\nfalse\nRuntimeClass\npoddisruptionbudgets\npdb\npolicy/v1\ntrue\nPodDisruptionBudget\npodsecuritypolicies\npsp\npolicy/v1beta1\nfalse\nPodSecurityPolicy\nclusterrolebindings\nrbac.authorization.k8s.io/v1\nfalse\nClusterRoleBinding\nclusterroles\nrbac.authorization.k8s.io/v1\nfalse\nClusterRole\nrolebindings\nrbac.authorization.k8s.io/v1\ntrue\nRoleBinding\nroles\nrbac.authorization.k8s.io/v1\ntrue\nRole\npriorityclasses\npc\nscheduling.k8s.io/v1\nfalse\nPriorityClass\ncsidrivers\nstorage.k8s.io/v1\nfalse\nCSIDriver\ncsinodes\nstorage.k8s.io/v1\nfalse\nCSINode\ncsistoragecapacities\nstorage.k8s.io/v1\ntrue\nCSIStorageCapacity\nstorageclasses\nsc\nstorage.k8s.io/v1\nfalse\nStorageClass\nvolumeattachments\nstorage.k8s.io/v1\nfalse\nVolumeAttachment\nOutput options\nUse the following sections for information about how you can format or sort the output\nof certain commands. For details about which commands support the various output options,\nsee the\nkubectl\nreference documentation.\nFormatting output\nThe default output format for all\nkubectl\ncommands is the human readable plain-text format.\nTo output details to your terminal window in a specific format, you can add either the\n-o\nor\n--output\nflags to a supported\nkubectl\ncommand.\nSyntax\nkubectl\n[\ncommand\n]\n[\nTYPE\n]\n[\nNAME\n]\n-o <output_format>\nDepending on the\nkubectl\noperation, the following output formats are supported:\nOutput format\nDescription\n-o custom-columns=<spec>\nPrint a table using a comma separated list of\ncustom columns\n.\n-o custom-columns-file=<filename>\nPrint a table using the\ncustom columns\ntemplate in the\n<filename>\nfile.\n-o json\nOutput a JSON formatted API object.\n-o jsonpath=<template>\nPrint the fields defined in a\njsonpath\nexpression.\n-o jsonpath-file=<filename>\nPrint the fields defined by the\njsonpath\nexpression in the\n<filename>\nfile.\n-o kyaml\nOutput a KYAML formatted API object (alpha, requires environment variable\nKUBECTL_KYAML=\"true\"\n).\n-o name\nPrint only the resource name and nothing else.\n-o wide\nOutput in the plain-text format with any additional information. For pods, the node name is included.\n-o yaml\nOutput a YAML formatted API object. KYAML is an experimental Kubernetes-specific dialect of YAML, and can be parsed as YAML.\nExample\nIn this example, the following command outputs the details for a single pod as a YAML formatted object:\nkubectl get pod web-pod-13je7 -o yaml\nRemember: See the\nkubectl\nreference documentation\nfor details about which output format is supported by each command.\nCustom columns\nTo define custom columns and output only the details that you want into a table, you can use the\ncustom-columns\noption.\nYou can choose to define the custom columns inline or use a template file:\n-o custom-columns=<spec>\nor\n-o custom-columns-file=<filename>\n.\nExamples\nInline:\nkubectl get pods <pod-name> -o custom-columns\n=\nNAME:.metadata.name,RSRC:.metadata.resourceVersion\nTemplate file:\nkubectl get pods <pod-name> -o custom-columns-file\n=\ntemplate.txt\nwhere the\ntemplate.txt\nfile contains:\nNAME          RSRC\nmetadata.name metadata.resourceVersion\nThe result of running either command is similar to:\nNAME           RSRC\nsubmit-queue   610995\nServer-side columns\nkubectl\nsupports receiving specific column information from the server about objects.\nThis means that for any given resource, the server will return columns and rows relevant to that resource, for the client to print.\nThis allows for consistent human-readable output across clients used against the same cluster, by having the server encapsulate the details of printing.\nThis feature is enabled by default. To disable it, add the\n--server-print=false\nflag to the\nkubectl get\ncommand.\nExamples\nTo print information about the status of a pod, use a command like the following:\nkubectl get pods <pod-name> --server-print\n=\nfalse\nThe output is similar to:\nNAME       AGE\npod-name   1m\nSorting list objects\nTo output objects to a sorted list in your terminal window, you can add the\n--sort-by\nflag\nto a supported\nkubectl\ncommand. Sort your objects by specifying any numeric or string field\nwith the\n--sort-by\nflag. To specify a field, use a\njsonpath\nexpression.\nSyntax\nkubectl\n[\ncommand\n]\n[\nTYPE\n]\n[\nNAME\n]\n--sort-by\n=\n<jsonpath_exp>\nExample\nTo print a list of pods sorted by name, you run:\nkubectl get pods --sort-by\n=\n.metadata.name\nExamples: Common operations\nUse the following set of examples to help you familiarize yourself with running the commonly used\nkubectl\noperations:\nkubectl apply\n- Apply or Update a resource from a file or stdin.\n# Create a service using the definition in example-service.yaml.\nkubectl apply -f example-service.yaml\n# Create a replication controller using the definition in example-controller.yaml.\nkubectl apply -f example-controller.yaml\n# Create the objects that are defined in any .yaml, .yml, or .json file within the <directory> directory.\nkubectl apply -f <directory>\nkubectl get\n- List one or more resources.\n# List all pods in plain-text output format.\nkubectl get pods\n# List all pods in plain-text output format and include additional information (such as node name).\nkubectl get pods -o wide\n# List the replication controller with the specified name in plain-text output format. Tip: You can shorten and replace the 'replicationcontroller' resource type with the alias 'rc'.\nkubectl get replicationcontroller <rc-name>\n# List all replication controllers and services together in plain-text output format.\nkubectl get rc,services\n# List all daemon sets in plain-text output format.\nkubectl get ds\n# List all pods running on node server01\nkubectl get pods --field-selector\n=\nspec.nodeName\n=\nserver01\nkubectl describe\n- Display detailed state of one or more resources, including the uninitialized ones by default.\n# Display the details of the node with name <node-name>.\nkubectl describe nodes <node-name>\n# Display the details of the pod with name <pod-name>.\nkubectl describe pods/<pod-name>\n# Display the details of all the pods that are managed by the replication controller named <rc-name>.\n# Remember: Any pods that are created by the replication controller get prefixed with the name of the replication controller.\nkubectl describe pods <rc-name>\n# Describe all pods\nkubectl describe pods\nNote:\nThe\nkubectl get\ncommand is usually used for retrieving one or more\nresources of the same resource type. It features a rich set of flags that allows\nyou to customize the output format using the\n-o\nor\n--output\nflag, for example.\nYou can specify the\n-w\nor\n--watch\nflag to start watching updates to a particular\nobject. The\nkubectl describe\ncommand is more focused on describing the many\nrelated aspects of a specified resource. It may invoke several API calls to the\nAPI server to build a view for the user. For example, the\nkubectl describe node\ncommand retrieves not only the information about the node, but also a summary of\nthe pods running on it, the events generated for the node etc.\nkubectl delete\n- Delete resources either from a file, stdin, or specifying label selectors, names, resource selectors, or resources.\n# Delete a pod using the type and name specified in the pod.yaml file.\nkubectl delete -f pod.yaml\n# Delete all the pods and services that have the label '<label-key>=<label-value>'.\nkubectl delete pods,services -l <label-key>\n=\n<label-value>\n# Delete all pods, including uninitialized ones.\nkubectl delete pods --all\nkubectl exec\n- Execute a command against a container in a pod.\n# Get output from running 'date' from pod <pod-name>. By default, output is from the first container.\nkubectl\nexec\n<pod-name> -- date\n# Get output from running 'date' in container <container-name> of pod <pod-name>.\nkubectl\nexec\n<pod-name> -c <container-name> -- date\n# Get an interactive TTY and run /bin/bash from pod <pod-name>. By default, output is from the first container.\nkubectl\nexec\n-ti <pod-name> -- /bin/bash\nkubectl logs\n- Print the logs for a container in a pod.\n# Return a snapshot of the logs from pod <pod-name>.\nkubectl logs <pod-name>\n# Start streaming the logs from pod <pod-name>. This is similar to the 'tail -f' Linux command.\nkubectl logs -f <pod-name>\nkubectl diff\n- View a diff of the proposed updates to a cluster.\n# Diff resources included in \"pod.json\".\nkubectl diff -f pod.json\n# Diff file read from stdin.\ncat service.yaml | kubectl diff -f -\nExamples: Creating and using plugins\nUse the following set of examples to help you familiarize yourself with writing and using\nkubectl\nplugins:\n# create a simple plugin in any language and name the resulting executable file\n# so that it begins with the prefix \"kubectl-\"\ncat ./kubectl-hello\n#!/bin/sh\n# this plugin prints the words \"hello world\"\necho\n\"hello world\"\nWith a plugin written, let's make it executable:\nchmod a+x ./kubectl-hello\n# and move it to a location in our PATH\nsudo mv ./kubectl-hello /usr/local/bin\nsudo chown root:root /usr/local/bin\n# You have now created and \"installed\" a kubectl plugin.\n# You can begin using this plugin by invoking it from kubectl as if it were a regular command\nkubectl hello\nhello world\n# You can \"uninstall\" a plugin, by removing it from the folder in your\n# $PATH where you placed it\nsudo rm /usr/local/bin/kubectl-hello\nIn order to view all of the plugins that are available to\nkubectl\n, use\nthe\nkubectl plugin list\nsubcommand:\nkubectl plugin list\nThe output is similar to:\nThe following kubectl-compatible plugins are available:\n/usr/local/bin/kubectl-hello\n/usr/local/bin/kubectl-foo\n/usr/local/bin/kubectl-bar\nkubectl plugin list\nalso warns you about plugins that are not\nexecutable, or that are shadowed by other plugins; for example:\nsudo chmod -x /usr/local/bin/kubectl-foo\n# remove execute permission\nkubectl plugin list\nThe following kubectl-compatible plugins are available:\n/usr/local/bin/kubectl-hello\n/usr/local/bin/kubectl-foo\n- warning: /usr/local/bin/kubectl-foo identified as a plugin, but it is not executable\n/usr/local/bin/kubectl-bar\nerror: one plugin warning was found\nYou can think of plugins as a means to build more complex functionality on top\nof the existing kubectl commands:\ncat ./kubectl-whoami\nThe next few examples assume that you already made\nkubectl-whoami\nhave\nthe following contents:\n#!/bin/bash\n# this plugin makes use of the `kubectl config` command in order to output\n# information about the current user, based on the currently selected context\nkubectl config view --template\n=\n'{{ range .contexts }}{{ if eq .name \"'\n$(\nkubectl config current-context\n)\n'\" }}Current user: {{ printf \"%s\\n\" .context.user }}{{ end }}{{ end }}'\nRunning the above command gives you an output containing the user for the\ncurrent context in your KUBECONFIG file:\n# make the file executable\nsudo chmod +x ./kubectl-whoami\n# and move it into your PATH\nsudo mv ./kubectl-whoami /usr/local/bin\nkubectl whoami\nCurrent user: plugins-user\nWhat's next\nRead the\nkubectl\nreference documentation:\nthe kubectl\ncommand reference\nthe\ncommand line arguments\nreference\nLearn about\nkubectl\nusage conventions\nRead about\nJSONPath support\nin kubectl\nRead about how to\nextend kubectl with plugins\nTo find out more about plugins, take a look at the\nexample CLI plugin\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified November 24, 2025 at 9:22 PM PST:\nMigrate K8s Configuration Good Practices to blog (b90c59015c)\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/reference/kubectl/"}}
{"text": "kubectl Quick Reference | Kubernetes\nkubectl Quick Reference\nThis page contains a list of commonly used\nkubectl\ncommands and flags.\nNote:\nThese instructions are for Kubernetes v1.34. To check the version, use the\nkubectl version\ncommand.\nKubectl autocomplete\nBASH\nsource\n<\n(\nkubectl completion bash\n)\n# set up autocomplete in bash into the current shell, bash-completion package should be installed first.\necho\n\"source <(kubectl completion bash)\"\n>> ~/.bashrc\n# add autocomplete permanently to your bash shell.\nYou can also use a shorthand alias for\nkubectl\nthat also works with completion:\nalias\nk\n=\nkubectl\ncomplete\n-o default -F __start_kubectl k\nZSH\nsource\n<\n(\nkubectl completion zsh\n)\n# set up autocomplete in zsh into the current shell\necho\n'[[ $commands[kubectl] ]] && source <(kubectl completion zsh)'\n>> ~/.zshrc\n# add autocomplete permanently to your zsh shell\nFISH\nNote:\nRequires kubectl version 1.23 or above.\necho\n'kubectl completion fish | source'\n> ~/.config/fish/completions/kubectl.fish\n&&\nsource\n~/.config/fish/completions/kubectl.fish\nA note on\n--all-namespaces\nAppending\n--all-namespaces\nhappens frequently enough that you should be aware of the shorthand for\n--all-namespaces\n:\nkubectl -A\nKubectl context and configuration\nSet which Kubernetes cluster\nkubectl\ncommunicates with and modifies configuration\ninformation. See\nAuthenticating Across Clusters with kubeconfig\ndocumentation for\ndetailed config file information.\nkubectl config view\n# Show Merged kubeconfig settings.\n# use multiple kubeconfig files at the same time and view merged config\nKUBECONFIG\n=\n~/.kube/config:~/.kube/kubconfig2\nkubectl config view\n# Show merged kubeconfig settings and raw certificate data and exposed secrets\nkubectl config view --raw\n# get the password for the e2e user\nkubectl config view -o\njsonpath\n=\n'{.users[?(@.name == \"e2e\")].user.password}'\n# get the certificate for the e2e user\nkubectl config view --raw -o\njsonpath\n=\n'{.users[?(.name == \"e2e\")].user.client-certificate-data}'\n| base64 -d\nkubectl config view -o\njsonpath\n=\n'{.users[].name}'\n# display the first user\nkubectl config view -o\njsonpath\n=\n'{.users[*].name}'\n# get a list of users\nkubectl config get-contexts\n# display list of contexts\nkubectl config get-contexts -o name\n# get all context names\nkubectl config current-context\n# display the current-context\nkubectl config use-context my-cluster-name\n# set the default context to my-cluster-name\nkubectl config set-cluster my-cluster-name\n# set a cluster entry in the kubeconfig\n# configure the URL to a proxy server to use for requests made by this client in the kubeconfig\nkubectl config set-cluster my-cluster-name --proxy-url\n=\nmy-proxy-url\n# add a new user to your kubeconf that supports basic auth\nkubectl config set-credentials kubeuser/foo.kubernetes.com --username\n=\nkubeuser --password\n=\nkubepassword\n# permanently save the namespace for all subsequent kubectl commands in that context.\nkubectl config set-context --current --namespace\n=\nggckad-s2\n# set a context utilizing a specific username and namespace.\nkubectl config set-context gce --user\n=\ncluster-admin --namespace\n=\nfoo\n\\\n&&\nkubectl config use-context gce\nkubectl config\nunset\nusers.foo\n# delete user foo\n# short alias to set/show context/namespace (only works for bash and bash-compatible shells, current context to be set before using kn to set namespace)\nalias\nkx\n=\n'f() { [ \"$1\" ] && kubectl config use-context $1 || kubectl config current-context ; } ; f'\nalias\nkn\n=\n'f() { [ \"$1\" ] && kubectl config set-context --current --namespace $1 || kubectl config view --minify | grep namespace | cut -d\" \" -f6 ; } ; f'\nKubectl apply\napply\nmanages applications through files defining Kubernetes resources. It creates and updates resources in a cluster through running\nkubectl apply\n. This is the recommended way of managing Kubernetes applications on production. See\nKubectl Book\n.\nCreating objects\nKubernetes manifests can be defined in YAML or JSON. The file extension\n.yaml\n,\n.yml\n, and\n.json\ncan be used.\nkubectl apply -f ./my-manifest.yaml\n# create resource(s)\nkubectl apply -f ./my1.yaml -f ./my2.yaml\n# create from multiple files\nkubectl apply -f ./dir\n# create resource(s) in all manifest files in dir\nkubectl apply -f https://example.com/manifest.yaml\n# create resource(s) from url (Note: this is an example domain and does not contain a valid manifest)\nkubectl create deployment nginx --image\n=\nnginx\n# start a single instance of nginx\n# create a Job which prints \"Hello World\"\nkubectl create job hello --image\n=\nbusybox:1.28 --\necho\n\"Hello World\"\n# create a CronJob that prints \"Hello World\" every minute\nkubectl create cronjob hello --image\n=\nbusybox:1.28   --schedule\n=\n\"*/1 * * * *\"\n--\necho\n\"Hello World\"\nkubectl explain pods\n# get the documentation for pod manifests\n# Create multiple YAML objects from stdin\nkubectl apply -f -\n<<EOF\napiVersion: v1\nkind: Pod\nmetadata:\nname: busybox-sleep\nspec:\ncontainers:\n- name: busybox\nimage: busybox:1.28\nargs:\n- sleep\n- \"1000000\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: busybox-sleep-less\nspec:\ncontainers:\n- name: busybox\nimage: busybox:1.28\nargs:\n- sleep\n- \"1000\"\nEOF\n# Create a secret with several keys\nkubectl apply -f -\n<<EOF\napiVersion: v1\nkind: Secret\nmetadata:\nname: mysecret\ntype: Opaque\ndata:\npassword: $(echo -n \"s33msi4\" | base64 -w0)\nusername: $(echo -n \"jane\" | base64 -w0)\nEOF\nViewing and finding resources\n# Get commands with basic output\nkubectl get services\n# List all services in the namespace\nkubectl get pods --all-namespaces\n# List all pods in all namespaces\nkubectl get pods -o wide\n# List all pods in the current namespace, with more details\nkubectl get deployment my-dep\n# List a particular deployment\nkubectl get pods\n# List all pods in the namespace\nkubectl get pod my-pod -o yaml\n# Get a pod's YAML\n# Describe commands with verbose output\nkubectl describe nodes my-node\nkubectl describe pods my-pod\n# List Services Sorted by Name\nkubectl get services --sort-by\n=\n.metadata.name\n# List pods Sorted by Restart Count\nkubectl get pods --sort-by\n=\n'.status.containerStatuses[0].restartCount'\n# List PersistentVolumes sorted by capacity\nkubectl get pv --sort-by\n=\n.spec.capacity.storage\n# Get the version label of all pods with label app=cassandra\nkubectl get pods --selector\n=\napp\n=\ncassandra -o\n\\\njsonpath\n=\n'{.items[*].metadata.labels.version}'\n# Retrieve the value of a key with dots, e.g. 'ca.crt'\nkubectl get configmap myconfig\n\\\n-o\njsonpath\n=\n'{.data.ca\\.crt}'\n# Retrieve a base64 encoded value with dashes instead of underscores.\nkubectl get secret my-secret --template\n=\n'{{index .data \"key-name-with-dashes\"}}'\n# Get all worker nodes (use a selector to exclude results that have a label\n# named 'node-role.kubernetes.io/control-plane')\nkubectl get node --selector\n=\n'!node-role.kubernetes.io/control-plane'\n# Get all running pods in the namespace\nkubectl get pods --field-selector\n=\nstatus.phase\n=\nRunning\n# Get ExternalIPs of all nodes\nkubectl get nodes -o\njsonpath\n=\n'{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}'\n# List Names of Pods that belong to Particular RC\n# \"jq\" command useful for transformations that are too complex for jsonpath, it can be found at https://jqlang.github.io/jq/\nsel\n=\n${\n$(\nkubectl get rc my-rc --output\n=\njson | jq -j\n'.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"'\n)\n%?\n}\necho\n$(\nkubectl get pods --selector\n=\n$sel\n--output\n=\njsonpath\n={\n.items..metadata.name\n}\n)\n# Show labels for all pods (or any other Kubernetes object that supports labelling)\nkubectl get pods --show-labels\n# Check which nodes are ready\nJSONPATH\n=\n'{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'\n\\\n&&\nkubectl get nodes -o\njsonpath\n=\n\"\n$JSONPATH\n\"\n| grep\n\"Ready=True\"\n# Check which nodes are ready with custom-columns\nkubectl get node -o custom-columns\n=\n'NODE_NAME:.metadata.name,STATUS:.status.conditions[?(@.type==\"Ready\")].status'\n# Output decoded secrets without external tools\nkubectl get secret my-secret -o go-template\n=\n'{{range $k,$v := .data}}{{\"### \"}}{{$k}}{{\"\\n\"}}{{$v|base64decode}}{{\"\\n\\n\"}}{{end}}'\n# List all Secrets currently in use by a pod\nkubectl get pods -o json | jq\n'.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name'\n| grep -v null | sort | uniq\n# List all containerIDs of initContainer of all pods\n# Helpful when cleaning up stopped containers, while avoiding removal of initContainers.\nkubectl get pods --all-namespaces -o\njsonpath\n=\n'{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}'\n| cut -d/ -f3\n# List Events sorted by timestamp\nkubectl get events --sort-by\n=\n.metadata.creationTimestamp\n# List all warning events\nkubectl events --types\n=\nWarning\n# Compares the current state of the cluster against the state that the cluster would be in if the manifest was applied.\nkubectl diff -f ./my-manifest.yaml\n# Produce a period-delimited tree of all keys returned for nodes\n# Helpful when locating a key within a complex nested JSON structure\nkubectl get nodes -o json | jq -c\n'paths|join(\".\")'\n# Produce a period-delimited tree of all keys returned for pods, etc\nkubectl get pods -o json | jq -c\n'paths|join(\".\")'\n# Produce ENV for all pods, assuming you have a default container for the pods, default namespace and the `env` command is supported.\n# Helpful when running any supported command across all pods, not just `env`\nfor\npod in\n$(\nkubectl get po --output\n=\njsonpath\n={\n.items..metadata.name\n}\n)\n;\ndo\necho\n$pod\n&&\nkubectl\nexec\n-it\n$pod\n-- env;\ndone\n# Get a deployment's status subresource\nkubectl get deployment nginx-deployment --subresource\n=\nstatus\nUpdating resources\nkubectl\nset\nimage deployment/frontend\nwww\n=\nimage:v2\n# Rolling update \"www\" containers of \"frontend\" deployment, updating the image\nkubectl rollout\nhistory\ndeployment/frontend\n# Check the history of deployments including the revision\nkubectl rollout undo deployment/frontend\n# Rollback to the previous deployment\nkubectl rollout undo deployment/frontend --to-revision\n=\n2\n# Rollback to a specific revision\nkubectl rollout status -w deployment/frontend\n# Watch rolling update status of \"frontend\" deployment until completion\nkubectl rollout restart deployment/frontend\n# Rolling restart of the \"frontend\" deployment\ncat pod.json | kubectl replace -f -\n# Replace a pod based on the JSON passed into stdin\n# Force replace, delete and then re-create the resource. Will cause a service outage.\nkubectl replace --force -f ./pod.json\n# Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000\nkubectl expose rc nginx --port\n=\n80\n--target-port\n=\n8000\n# Update a single-container pod's image version (tag) to v4\nkubectl get pod mypod -o yaml | sed\n's/\\(image: myimage\\):.*$/\\1:v4/'\n| kubectl replace -f -\nkubectl label pods my-pod new-label\n=\nawesome\n# Add a Label\nkubectl label pods my-pod new-label-\n# Remove a label\nkubectl label pods my-pod new-label\n=\nnew-value --overwrite\n# Overwrite an existing value\nkubectl annotate pods my-pod icon-url\n=\nhttp://goo.gl/XXBTWq\n# Add an annotation\nkubectl annotate pods my-pod icon-url-\n# Remove annotation\nkubectl autoscale deployment foo --min\n=\n2\n--max\n=\n10\n# Auto scale a deployment \"foo\"\nPatching resources\n# Partially update a node\nkubectl patch node k8s-node-1 -p\n'{\"spec\":{\"unschedulable\":true}}'\n# Update a container's image; spec.containers[*].name is required because it's a merge key\nkubectl patch pod valid-pod -p\n'{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}'\n# Update a container's image using a json patch with positional arrays\nkubectl patch pod valid-pod --type\n=\n'json'\n-p\n=\n'[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]'\n# Disable a deployment livenessProbe using a json patch with positional arrays\nkubectl patch deployment valid-deployment  --type json   -p\n=\n'[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]'\n# Add a new element to a positional array\nkubectl patch sa default --type\n=\n'json'\n-p\n=\n'[{\"op\": \"add\", \"path\": \"/secrets/1\", \"value\": {\"name\": \"whatever\" } }]'\n# Update a deployment's replica count by patching its scale subresource\nkubectl patch deployment nginx-deployment --subresource\n=\n'scale'\n--type\n=\n'merge'\n-p\n'{\"spec\":{\"replicas\":2}}'\nEditing resources\nEdit any API resource in your preferred editor.\nkubectl edit svc/docker-registry\n# Edit the service named docker-registry\nKUBE_EDITOR\n=\n\"nano\"\nkubectl edit svc/docker-registry\n# Use an alternative editor\nScaling resources\nkubectl scale --replicas\n=\n3\nrs/foo\n# Scale a replicaset named 'foo' to 3\nkubectl scale --replicas\n=\n3\n-f foo.yaml\n# Scale a resource specified in \"foo.yaml\" to 3\nkubectl scale --current-replicas\n=\n2\n--replicas\n=\n3\ndeployment/mysql\n# If the deployment named mysql's current size is 2, scale mysql to 3\nkubectl scale --replicas\n=\n5\nrc/foo rc/bar rc/baz\n# Scale multiple replication controllers\nDeleting resources\nkubectl delete -f ./pod.json\n# Delete a pod using the type and name specified in pod.json\nkubectl delete pod unwanted --now\n# Delete a pod with no grace period\nkubectl delete pod,service baz foo\n# Delete pods and services with same names \"baz\" and \"foo\"\nkubectl delete pods,services -l\nname\n=\nmyLabel\n# Delete pods and services with label name=myLabel\nkubectl -n my-ns delete pod,svc --all\n# Delete all pods and services in namespace my-ns,\n# Delete all pods matching the awk pattern1 or pattern2\nkubectl get pods  -n mynamespace --no-headers\n=\ntrue\n| awk\n'/pattern1|pattern2/{print $1}'\n| xargs  kubectl delete -n mynamespace pod\nInteracting with running Pods\nkubectl logs my-pod\n# dump pod logs (stdout)\nkubectl logs -l\nname\n=\nmyLabel\n# dump pod logs, with label name=myLabel (stdout)\nkubectl logs my-pod --previous\n# dump pod logs (stdout) for a previous instantiation of a container\nkubectl logs my-pod -c my-container\n# dump pod container logs (stdout, multi-container case)\nkubectl logs -l\nname\n=\nmyLabel -c my-container\n# dump pod container logs, with label name=myLabel (stdout)\nkubectl logs my-pod -c my-container --previous\n# dump pod container logs (stdout, multi-container case) for a previous instantiation of a container\nkubectl logs -f my-pod\n# stream pod logs (stdout)\nkubectl logs -f my-pod -c my-container\n# stream pod container logs (stdout, multi-container case)\nkubectl logs -f -l\nname\n=\nmyLabel --all-containers\n# stream all pods logs with label name=myLabel (stdout)\nkubectl run -i --tty busybox --image\n=\nbusybox:1.28 -- sh\n# Run pod as interactive shell\nkubectl run nginx --image\n=\nnginx -n mynamespace\n# Start a single instance of nginx pod in the namespace of mynamespace\nkubectl run nginx --image\n=\nnginx --dry-run\n=\nclient -o yaml > pod.yaml\n# Generate spec for running pod nginx and write it into a file called pod.yaml\nkubectl attach my-pod -i\n# Attach to Running Container\nkubectl port-forward my-pod 5000:6000\n# Listen on port 5000 on the local machine and forward to port 6000 on my-pod\nkubectl\nexec\nmy-pod -- ls /\n# Run command in existing pod (1 container case)\nkubectl\nexec\n--stdin --tty my-pod -- /bin/sh\n# Interactive shell access to a running pod (1 container case)\nkubectl\nexec\nmy-pod -c my-container -- ls /\n# Run command in existing pod (multi-container case)\nkubectl debug my-pod -it --image\n=\nbusybox:1.28\n# Create an interactive debugging session within existing pod and immediately attach to it\nkubectl debug node/my-node -it --image\n=\nbusybox:1.28\n# Create an interactive debugging session on a node and immediately attach to it\nkubectl top pod\n# Show metrics for all pods in the default namespace\nkubectl top pod POD_NAME --containers\n# Show metrics for a given pod and its containers\nkubectl top pod POD_NAME --sort-by\n=\ncpu\n# Show metrics for a given pod and sort it by 'cpu' or 'memory'\nCopying files and directories to and from containers\nkubectl cp /tmp/foo_dir my-pod:/tmp/bar_dir\n# Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the current namespace\nkubectl cp /tmp/foo my-pod:/tmp/bar -c my-container\n# Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container\nkubectl cp /tmp/foo my-namespace/my-pod:/tmp/bar\n# Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace my-namespace\nkubectl cp my-namespace/my-pod:/tmp/foo /tmp/bar\n# Copy /tmp/foo from a remote pod to /tmp/bar locally\nNote:\nkubectl cp\nrequires that the 'tar' binary is present in your container image. If 'tar' is not present,\nkubectl cp\nwill fail.\nFor advanced use cases, such as symlinks, wildcard expansion or file mode preservation consider using\nkubectl exec\n.\ntar cf - /tmp/foo | kubectl\nexec\n-i -n my-namespace my-pod -- tar xf - -C /tmp/bar\n# Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace my-namespace\nkubectl\nexec\n-n my-namespace my-pod -- tar cf - /tmp/foo | tar xf - -C /tmp/bar\n# Copy /tmp/foo from a remote pod to /tmp/bar locally\nInteracting with Deployments and Services\nkubectl logs deploy/my-deployment\n# dump Pod logs for a Deployment (single-container case)\nkubectl logs deploy/my-deployment -c my-container\n# dump Pod logs for a Deployment (multi-container case)\nkubectl port-forward svc/my-service\n5000\n# listen on local port 5000 and forward to port 5000 on Service backend\nkubectl port-forward svc/my-service 5000:my-service-port\n# listen on local port 5000 and forward to Service target port with name <my-service-port>\nkubectl port-forward deploy/my-deployment 5000:6000\n# listen on local port 5000 and forward to port 6000 on a Pod created by <my-deployment>\nkubectl\nexec\ndeploy/my-deployment -- ls\n# run command in first Pod and first container in Deployment (single- or multi-container cases)\nInteracting with Nodes and cluster\nkubectl cordon my-node\n# Mark my-node as unschedulable\nkubectl drain my-node\n# Drain my-node in preparation for maintenance\nkubectl uncordon my-node\n# Mark my-node as schedulable\nkubectl top node\n# Show metrics for all nodes\nkubectl top node my-node\n# Show metrics for a given node\nkubectl cluster-info\n# Display addresses of the master and services\nkubectl cluster-info dump\n# Dump current cluster state to stdout\nkubectl cluster-info dump --output-directory\n=\n/path/to/cluster-state\n# Dump current cluster state to /path/to/cluster-state\n# View existing taints on which exist on current nodes.\nkubectl get nodes -o\n=\n'custom-columns=NodeName:.metadata.name,TaintKey:.spec.taints[*].key,TaintValue:.spec.taints[*].value,TaintEffect:.spec.taints[*].effect'\n# If a taint with that key and effect already exists, its value is replaced as specified.\nkubectl taint nodes foo\ndedicated\n=\nspecial-user:NoSchedule\nResource types\nList all supported resource types along with their shortnames,\nAPI group\n, whether they are\nnamespaced\n, and\nkind\n:\nkubectl api-resources\nOther operations for exploring API resources:\nkubectl api-resources --namespaced\n=\ntrue\n# All namespaced resources\nkubectl api-resources --namespaced\n=\nfalse\n# All non-namespaced resources\nkubectl api-resources -o name\n# All resources with simple output (only the resource name)\nkubectl api-resources -o wide\n# All resources with expanded (aka \"wide\") output\nkubectl api-resources --verbs\n=\nlist,get\n# All resources that support the \"list\" and \"get\" request verbs\nkubectl api-resources --api-group\n=\nextensions\n# All resources in the \"extensions\" API group\nFormatting output\nTo output details to your terminal window in a specific format, add the\n-o\n(or\n--output\n) flag to a supported\nkubectl\ncommand.\nOutput format\nDescription\n-o=custom-columns=<spec>\nPrint a table using a comma separated list of custom columns\n-o=custom-columns-file=<filename>\nPrint a table using the custom columns template in the\n<filename>\nfile\n-o=go-template=<template>\nPrint the fields defined in a\ngolang template\n-o=go-template-file=<filename>\nPrint the fields defined by the\ngolang template\nin the\n<filename>\nfile\n-o=json\nOutput a JSON formatted API object\n-o=jsonpath=<template>\nPrint the fields defined in a\njsonpath\nexpression\n-o=jsonpath-file=<filename>\nPrint the fields defined by the\njsonpath\nexpression in the\n<filename>\nfile\n-o=kyaml\nOutput a KYAML formatted API object (alpha, requires environment variable\nKUBECTL_KYAML=\"true\"\n). KYAML is an experimental Kubernetes-specific dialect of YAML, and can be parsed as YAML.\n-o=name\nPrint only the resource name and nothing else\n-o=wide\nOutput in the plain-text format with any additional information, and for pods, the node name is included\n-o=yaml\nOutput a YAML formatted API object\nExamples using\n-o=custom-columns\n:\n# All images running in a cluster\nkubectl get pods -A -o\n=\ncustom-columns\n=\n'DATA:spec.containers[*].image'\n# All images running in namespace: default, grouped by Pod\nkubectl get pods --namespace default --output\n=\ncustom-columns\n=\n\"NAME:.metadata.name,IMAGE:.spec.containers[*].image\"\n# All images excluding \"registry.k8s.io/coredns:1.6.2\"\nkubectl get pods -A -o\n=\ncustom-columns\n=\n'DATA:spec.containers[?(@.image!=\"registry.k8s.io/coredns:1.6.2\")].image'\n# All fields under metadata regardless of name\nkubectl get pods -A -o\n=\ncustom-columns\n=\n'DATA:metadata.*'\nMore examples in the kubectl\nreference documentation\n.\nKubectl output verbosity and debugging\nKubectl verbosity is controlled with the\n-v\nor\n--v\nflags followed by an integer representing the log level. General Kubernetes logging conventions and the associated log levels are described\nhere\n.\nVerbosity\nDescription\n--v=0\nGenerally useful for this to\nalways\nbe visible to a cluster operator.\n--v=1\nA reasonable default log level if you don't want verbosity.\n--v=2\nUseful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems.\n--v=3\nExtended information about changes.\n--v=4\nDebug level verbosity.\n--v=5\nTrace level verbosity.\n--v=6\nDisplay requested resources.\n--v=7\nDisplay HTTP request headers.\n--v=8\nDisplay HTTP request contents.\n--v=9\nDisplay HTTP request contents without truncation of contents.\nWhat's next\nRead the\nkubectl overview\nand learn about\nJsonPath\n.\nSee\nkubectl\noptions.\nSee\nkuberc\noptions.\nAlso read\nkubectl Usage Conventions\nto understand how to use kubectl in reusable scripts.\nSee more community\nkubectl cheatsheets\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 01, 2025 at 8:58 AM PST:\nDocument KYAML (9f26f83533)\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/reference/kubectl/cheatsheet/"}}
{"text": "Overview | Kubernetes\nOverview\nKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nThis page is an overview of Kubernetes.\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation\nresults from counting the eight letters between the \"K\" and the \"s\". Google open-sourced the\nKubernetes project in 2014. Kubernetes combines\nover 15 years of Google's experience\nrunning\nproduction workloads at scale with best-of-breed ideas and practices from the community.\nWhy you need Kubernetes and what it can do\nContainers are a good way to bundle and run your applications. In a production\nenvironment, you need to manage the containers that run the applications and\nensure that there is no downtime. For example, if a container goes down, another\ncontainer needs to start. Wouldn't it be easier if this behavior was handled by a system?\nThat's how Kubernetes comes to the rescue! Kubernetes provides you with a framework\nto run distributed systems resiliently. It takes care of scaling and failover for\nyour application, provides deployment patterns, and more. For example: Kubernetes\ncan easily manage a canary deployment for your system.\nKubernetes provides you with:\nService discovery and load balancing\nKubernetes can expose a container using the DNS name or using their own IP address.\nIf traffic to a container is high, Kubernetes is able to load balance and distribute\nthe network traffic so that the deployment is stable.\nStorage orchestration\nKubernetes allows you to automatically mount a storage system of your choice, such as\nlocal storages, public cloud providers, and more.\nAutomated rollouts and rollbacks\nYou can describe the desired state for your deployed containers using Kubernetes,\nand it can change the actual state to the desired state at a controlled rate.\nFor example, you can automate Kubernetes to create new containers for your\ndeployment, remove existing containers and adopt all their resources to the new container.\nAutomatic bin packing\nYou provide Kubernetes with a cluster of nodes that it can use to run containerized tasks.\nYou tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit\ncontainers onto your nodes to make the best use of your resources.\nSelf-healing\nKubernetes restarts containers that fail, replaces containers, kills containers that don't\nrespond to your user-defined health check, and doesn't advertise them to clients until they\nare ready to serve.\nSecret and configuration management\nKubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens,\nand SSH keys. You can deploy and update secrets and application configuration without\nrebuilding your container images, and without exposing secrets in your stack configuration.\nBatch execution\nIn addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\nHorizontal scaling\nScale your application up and down with a simple command, with a UI, or automatically based on CPU usage.\nIPv4/IPv6 dual-stack\nAllocation of IPv4 and IPv6 addresses to Pods and Services\nDesigned for extensibility\nAdd features to your Kubernetes cluster without changing upstream source code.\nWhat Kubernetes is not\nKubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system.\nSince Kubernetes operates at the container level rather than at the hardware level,\nit provides some generally applicable features common to PaaS offerings, such as\ndeployment, scaling, load balancing, and lets users integrate their logging, monitoring,\nand alerting solutions. However, Kubernetes is not monolithic, and these default solutions\nare optional and pluggable. Kubernetes provides the building blocks for building developer\nplatforms, but preserves user choice and flexibility where it is important.\nKubernetes:\nDoes not limit the types of applications supported. Kubernetes aims to support an\nextremely diverse variety of workloads, including stateless, stateful, and data-processing\nworkloads. If an application can run in a container, it should run great on Kubernetes.\nDoes not deploy source code and does not build your application. Continuous Integration,\nDelivery, and Deployment (CI/CD) workflows are determined by organization cultures and\npreferences as well as technical requirements.\nDoes not provide application-level services, such as middleware (for example, message buses),\ndata-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor\ncluster storage systems (for example, Ceph) as built-in services. Such components can run on\nKubernetes, and/or can be accessed by applications running on Kubernetes through portable\nmechanisms, such as the\nOpen Service Broker\n.\nDoes not dictate logging, monitoring, or alerting solutions. It provides some integrations\nas proof of concept, and mechanisms to collect and export metrics.\nDoes not provide nor mandate a configuration language/system (for example, Jsonnet). It provides\na declarative API that may be targeted by arbitrary forms of declarative specifications.\nDoes not provide nor adopt any comprehensive machine configuration, maintenance, management,\nor self-healing systems.\nAdditionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need\nfor orchestration. The technical definition of orchestration is execution of a defined workflow:\nfirst do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable\ncontrol processes that continuously drive the current state towards the provided desired state.\nIt shouldn't matter how you get from A to C. Centralized control is also not required. This\nresults in a system that is easier to use and more powerful, robust, resilient, and extensible.\nHistorical context for Kubernetes\nLet's take a look at why Kubernetes is so useful by going back in time.\nTraditional deployment era:\nEarly on, organizations ran applications on physical servers. There was no way to define\nresource boundaries for applications in a physical server, and this caused resource\nallocation issues. For example, if multiple applications run on a physical server, there\ncan be instances where one application would take up most of the resources, and as a result,\nthe other applications would underperform. A solution for this would be to run each application\non a different physical server. But this did not scale as resources were underutilized, and it\nwas expensive for organizations to maintain many physical servers.\nVirtualized deployment era:\nAs a solution, virtualization was introduced. It allows you\nto run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization\nallows applications to be isolated between VMs and provides a level of security as the\ninformation of one application cannot be freely accessed by another application.\nVirtualization allows better utilization of resources in a physical server and allows\nbetter scalability because an application can be added or updated easily, reduces\nhardware costs, and much more. With virtualization you can present a set of physical\nresources as a cluster of disposable virtual machines.\nEach VM is a full machine running all the components, including its own operating\nsystem, on top of the virtualized hardware.\nContainer deployment era:\nContainers are similar to VMs, but they have relaxed\nisolation properties to share the Operating System (OS) among the applications.\nTherefore, containers are considered lightweight. Similar to a VM, a container\nhas its own filesystem, share of CPU, memory, process space, and more. As they\nare decoupled from the underlying infrastructure, they are portable across clouds\nand OS distributions.\nContainers have become popular because they provide extra benefits, such as:\nAgile application creation and deployment: increased ease and efficiency of\ncontainer image creation compared to VM image use.\nContinuous development, integration, and deployment: provides for reliable\nand frequent container image build and deployment with quick and efficient\nrollbacks (due to image immutability).\nDev and Ops separation of concerns: create application container images at\nbuild/release time rather than deployment time, thereby decoupling\napplications from infrastructure.\nObservability: not only surfaces OS-level information and metrics, but also\napplication health and other signals.\nEnvironmental consistency across development, testing, and production: runs\nthe same on a laptop as it does in the cloud.\nCloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises,\non major public clouds, and anywhere else.\nApplication-centric management: raises the level of abstraction from running an\nOS on virtual hardware to running an application on an OS using logical resources.\nLoosely coupled, distributed, elastic, liberated micro-services: applications are\nbroken into smaller, independent pieces and can be deployed and managed dynamically \nnot a monolithic stack running on one big single-purpose machine.\nResource isolation: predictable application performance.\nResource utilization: high efficiency and density.\nWhat's next\nTake a look at the\nKubernetes Components\nTake a look at the\nThe Kubernetes API\nTake a look at the\nCluster Architecture\nReady to\nGet Started\n?\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 11, 2024 at 3:17 PM PST:\nRemoved duplicated paragraph (7e71096044)\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/overview/"}}
{"text": "Pods | Kubernetes\nPods\nPods\nare the smallest deployable units of computing that you can create and manage in Kubernetes.\nA\nPod\n(as in a pod of whales or pea pod) is a group of one or more\ncontainers\n, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and\nco-scheduled, and run in a shared context. A Pod models an\napplication-specific \"logical host\": it contains one or more application\ncontainers which are relatively tightly coupled.\nIn non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.\nAs well as application containers, a Pod can contain\ninit containers\nthat run\nduring Pod startup. You can also inject\nephemeral containers\nfor debugging a running Pod.\nWhat is a Pod?\nNote:\nYou need to install a\ncontainer runtime\ninto each node in the cluster so that Pods can run there.\nThe shared context of a Pod is a set of Linux namespaces, cgroups, and\npotentially other facets of isolation - the same things that isolate a\ncontainer\n. Within a Pod's context, the individual applications may have\nfurther sub-isolations applied.\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.\nPods in a Kubernetes cluster are used in two main ways:\nPods that run a single container\n. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\nPods that run multiple containers that need to work together\n. A Pod can\nencapsulate an application composed of\nmultiple co-located containers\nthat are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit.\nGrouping multiple co-located and co-managed containers in a single Pod is a\nrelatively advanced use case. You should use this pattern only in specific\ninstances in which your containers are tightly coupled.\nYou don't need to run multiple containers to provide replication (for resilience\nor capacity); if you need multiple replicas, see\nWorkload management\n.\nUsing Pods\nThe following is an example of a Pod which consists of a container running the image\nnginx:1.14.2\n.\npods/simple-pod.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nnginx\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:1.14.2\nports\n:\n-\ncontainerPort\n:\n80\nTo create the Pod shown above, run the following command:\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\nPods are generally not created directly and are created using workload resources.\nSee\nWorking with Pods\nfor more information on how Pods are used\nwith workload resources.\nWorkload resources for managing pods\nUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as\nDeployment\nor\nJob\n.\nIf your Pods need to track state, consider the\nStatefulSet\nresource.\nEach Pod is meant to run a single instance of a given application. If you want to\nscale your application horizontally (to provide more overall resources by running\nmore instances), you should use multiple Pods, one for each instance. In\nKubernetes, this is typically referred to as\nreplication\n.\nReplicated Pods are usually created and managed as a group by a workload resource\nand its\ncontroller\n.\nSee\nPods and controllers\nfor more information on how\nKubernetes uses workload resources, and their controllers, to implement application\nscaling and auto-healing.\nPods natively provide two kinds of shared resources for their constituent containers:\nnetworking\nand\nstorage\n.\nWorking with Pods\nYou'll rarely create individual Pods directly in Kuberneteseven singleton Pods. This\nis because Pods are designed as relatively ephemeral, disposable entities. When\na Pod gets created (directly by you, or indirectly by a\ncontroller\n), the new Pod is\nscheduled to run on a\nNode\nin your cluster.\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted,\nthe Pod is\nevicted\nfor lack of resources, or the node fails.\nNote:\nRestarting a container in a Pod should not be confused with restarting a Pod. A Pod\nis not a process, but an environment for running container(s). A Pod persists until\nit is deleted.\nThe name of a Pod must be a valid\nDNS subdomain\nvalue, but this can produce unexpected results for the Pod hostname. For best compatibility,\nthe name should follow the more restrictive rules for a\nDNS label\n.\nPod OS\nFEATURE STATE:\nKubernetes v1.25 [stable]\nYou should set the\n.spec.os.name\nfield to either\nwindows\nor\nlinux\nto indicate the OS on\nwhich you want the pod to run. These two are the only operating systems supported for now by\nKubernetes. In the future, this list may be expanded.\nIn Kubernetes v1.34, the value of\n.spec.os.name\ndoes not affect\nhow the\nkube-scheduler\npicks a node for the Pod to run on. In any cluster where there is more than one operating system for\nrunning nodes, you should set the\nkubernetes.io/os\nlabel correctly on each node, and define pods with a\nnodeSelector\nbased on the operating system\nlabel. The kube-scheduler assigns your pod to a node based on other criteria and may or may not\nsucceed in picking a suitable node placement where the node OS is right for the containers in that Pod.\nThe\nPod security standards\nalso use this\nfield to avoid enforcing policies that aren't relevant to the operating system.\nPods and controllers\nYou can use workload resources to create and manage multiple Pods for you. A controller\nfor the resource handles replication and rollout and automatic healing in case of\nPod failure. For example, if a Node fails, a controller notices that Pods on that\nNode have stopped working and creates a replacement Pod. The scheduler places the\nreplacement Pod onto a healthy Node.\nHere are some examples of workload resources that manage one or more Pods:\nDeployment\nStatefulSet\nDaemonSet\nPod templates\nControllers for\nworkload\nresources create Pods\nfrom a\npod template\nand manage those Pods on your behalf.\nPodTemplates are specifications for creating Pods, and are included in workload resources such as\nDeployments\n,\nJobs\n, and\nDaemonSets\n.\nEach controller for a workload resource uses the\nPodTemplate\ninside the workload\nobject to make actual Pods. The\nPodTemplate\nis part of the desired state of whatever\nworkload resource you used to run your app.\nWhen you create a Pod, you can include\nenvironment variables\nin the Pod template for the containers that run in the Pod.\nThe sample below is a manifest for a simple Job with a\ntemplate\nthat starts one\ncontainer. The container in that Pod prints a message then pauses.\napiVersion\n:\nbatch/v1\nkind\n:\nJob\nmetadata\n:\nname\n:\nhello\nspec\n:\ntemplate\n:\n# This is the pod template\nspec\n:\ncontainers\n:\n-\nname\n:\nhello\nimage\n:\nbusybox:1.28\ncommand\n:\n[\n'sh'\n,\n'-c'\n,\n'echo \"Hello, Kubernetes!\" && sleep 3600'\n]\nrestartPolicy\n:\nOnFailure\n# The pod template ends here\nModifying the pod template or switching to a new pod template has no direct effect\non the Pods that already exist. If you change the pod template for a workload\nresource, that resource needs to create replacement Pods that use the updated template.\nFor example, the StatefulSet controller ensures that the running Pods match the current\npod template for each StatefulSet object. If you edit the StatefulSet to change its pod\ntemplate, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.\nEach workload resource implements its own rules for handling changes to the Pod template.\nIf you want to read more about StatefulSet specifically, read\nUpdate strategy\nin the StatefulSet Basics tutorial.\nOn Nodes, the\nkubelet\ndoes not\ndirectly observe or manage any of the details around pod templates and updates; those\ndetails are abstracted away. That abstraction and separation of concerns simplifies\nsystem semantics, and makes it feasible to extend the cluster's behavior without\nchanging existing code.\nPod update and replacement\nAs mentioned in the previous section, when the Pod template for a workload\nresource is changed, the controller creates new Pods based on the updated\ntemplate instead of updating or patching the existing Pods.\nKubernetes doesn't prevent you from managing Pods directly. It is possible to\nupdate some fields of a running Pod, in place. However, Pod update operations\nlike\npatch\n, and\nreplace\nhave some limitations:\nMost of the metadata about a Pod is immutable. For example, you cannot\nchange the\nnamespace\n,\nname\n,\nuid\n, or\ncreationTimestamp\nfields.\nIf the\nmetadata.deletionTimestamp\nis set, no new entry can be added to the\nmetadata.finalizers\nlist.\nPod updates may not change fields other than\nspec.containers[*].image\n,\nspec.initContainers[*].image\n,\nspec.activeDeadlineSeconds\n,\nspec.terminationGracePeriodSeconds\n,\nspec.tolerations\nor\nspec.schedulingGates\n. For\nspec.tolerations\n, you can only add new entries.\nWhen updating the\nspec.activeDeadlineSeconds\nfield, two types of updates\nare allowed:\nsetting the unassigned field to a positive number;\nupdating the field from a positive number to a smaller, non-negative\nnumber.\nPod subresources\nThe above update rules apply to regular pod updates, but other pod fields can be updated through\nsubresources\n.\nResize:\nThe\nresize\nsubresource allows container resources (\nspec.containers[*].resources\n) to be updated.\nSee\nResize Container Resources\nfor more details.\nEphemeral Containers:\nThe\nephemeralContainers\nsubresource allows\nephemeral containers\nto be added to a Pod.\nSee\nEphemeral Containers\nfor more details.\nStatus:\nThe\nstatus\nsubresource allows the pod status to be updated.\nThis is typically only used by the Kubelet and other system controllers.\nBinding:\nThe\nbinding\nsubresource allows setting the pod's\nspec.nodeName\nvia a\nBinding\nrequest.\nThis is typically only used by the\nscheduler\n.\nPod generation\nThe\nmetadata.generation\nfield is unique. It will be automatically set by the\nsystem such that new pods have a\nmetadata.generation\nof 1, and every update to\nmutable fields in the pod's spec will increment the\nmetadata.generation\nby 1.\nFEATURE STATE:\nKubernetes v1.34 [beta]\n(enabled by default)\nobservedGeneration\nis a field that is captured in the\nstatus\nsection of the Pod\nobject. If the feature gate\nPodObservedGenerationTracking\nis set, the Kubelet will set\nstatus.observedGeneration\nto track the pod state to the current pod status. The pod's\nstatus.observedGeneration\nwill reflect the\nmetadata.generation\nof the pod at the point that the pod status is being reported.\nNote:\nThe\nstatus.observedGeneration\nfield is managed by the kubelet and external controllers should\nnot\nmodify this field.\nDifferent status fields may either be associated with the\nmetadata.generation\nof the current sync loop, or with the\nmetadata.generation\nof the previous sync loop. The key distinction is whether a change in the\nspec\nis reflected\ndirectly in the\nstatus\nor is an indirect result of a running process.\nDirect Status Updates\nFor status fields where the allocated spec is directly reflected, the\nobservedGeneration\nwill\nbe associated with the current\nmetadata.generation\n(Generation N).\nThis behavior applies to:\nResize Status\n: The status of a resource resize operation.\nAllocated Resources\n: The resources allocated to the Pod after a resize.\nEphemeral Containers\n: When a new ephemeral container is added, and it is in\nWaiting\nstate.\nIndirect Status Updates\nFor status fields that are an indirect result of running the spec, the\nobservedGeneration\nwill be associated\nwith the\nmetadata.generation\nof the previous sync loop (Generation N-1).\nThis behavior applies to:\nContainer Image\n: The\nContainerStatus.ImageID\nreflects the image from the previous generation until the new image\nis pulled and the container is updated.\nActual Resources\n: During an in-progress resize, the actual resources in use still belong to the previous generation's\nrequest.\nContainer state\n: During an in-progress resize, with require restart policy reflects the previous generation's\nrequest.\nactiveDeadlineSeconds\n&\nterminationGracePeriodSeconds\n&\ndeletionTimestamp\n: The effects of these fields on the\nPod's status are a result of the previously observed specification.\nResource sharing and communication\nPods enable data sharing and communication among their constituent\ncontainers.\nStorage in Pods\nA Pod can specify a set of shared storage\nvolumes\n. All containers\nin the Pod can access the shared volumes, allowing those containers to\nshare data. Volumes also allow persistent data in a Pod to survive\nin case one of the containers within needs to be restarted. See\nStorage\nfor more information on how\nKubernetes implements shared storage and makes it available to Pods.\nPod networking\nEach Pod is assigned a unique IP address for each address family. Every\ncontainer in a Pod shares the network namespace, including the IP address and\nnetwork ports. Inside a Pod (and\nonly\nthen), the containers that belong to the Pod\ncan communicate with one another using\nlocalhost\n. When containers in a Pod communicate\nwith entities\noutside the Pod\n,\nthey must coordinate how they use the shared network resources (such as ports).\nWithin a Pod, containers share an IP address and port space, and\ncan find each other via\nlocalhost\n. The containers in a Pod can also communicate\nwith each other using standard inter-process communications like SystemV semaphores\nor POSIX shared memory. Containers in different Pods have distinct IP addresses\nand can not communicate by OS-level IPC without special configuration.\nContainers that want to interact with a container running in a different Pod can\nuse IP networking to communicate.\nContainers within the Pod see the system hostname as being the same as the configured\nname\nfor the Pod. There's more about this in the\nnetworking\nsection.\nPod security settings\nTo set security constraints on Pods and containers, you use the\nsecurityContext\nfield in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. For example:\nDrop specific Linux capabilities to avoid the impact of a CVE.\nForce all processes in the Pod to run as a non-root user or as a specific\nuser or group ID.\nSet a specific seccomp profile.\nSet Windows security options, such as whether containers run as HostProcess.\nCaution:\nYou can also use the Pod securityContext to enable\nprivileged mode\nin Linux containers. Privileged mode overrides many of the other security\nsettings in the securityContext. Avoid using this setting unless you can't grant\nthe equivalent permissions by using other fields in the securityContext.\nIn Kubernetes 1.26 and later, you can run Windows containers in a similarly\nprivileged mode by setting the\nwindowsOptions.hostProcess\nflag on the\nsecurity context of the Pod spec. For details and instructions, see\nCreate a Windows HostProcess Pod\n.\nTo learn about kernel-level security constraints that you can use,\nsee\nLinux kernel security constraints for Pods and containers\n.\nTo learn more about the Pod security context, see\nConfigure a Security Context for a Pod or Container\n.\nStatic Pods\nStatic Pods\nare managed directly by the kubelet daemon on a specific node,\nwithout the\nAPI server\nobserving them.\nWhereas most Pods are managed by the control plane (for example, a\nDeployment\n), for static\nPods, the kubelet directly supervises each static Pod (and restarts it if it fails).\nStatic Pods are always bound to one\nKubelet\non a specific node.\nThe main use for static Pods is to run a self-hosted control plane: in other words,\nusing the kubelet to supervise the individual\ncontrol plane components\n.\nThe kubelet automatically tries to create a\nmirror Pod\non the Kubernetes API server for each static Pod.\nThis means that the Pods running on a node are visible on the API server,\nbut cannot be controlled from there. See the guide\nCreate static Pods\nfor more information.\nNote:\nThe\nspec\nof a static Pod cannot refer to other API objects\n(e.g.,\nServiceAccount\n,\nConfigMap\n,\nSecret\n, etc).\nPods with multiple containers\nPods are designed to support multiple cooperating processes (as containers) that form\na cohesive unit of service. The containers in a Pod are automatically co-located and\nco-scheduled on the same physical or virtual machine in the cluster. The containers\ncan share resources and dependencies, communicate with one another, and coordinate\nwhen and how they are terminated.\nPods in a Kubernetes cluster are used in two main ways:\nPods that run a single container\n. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\nPods that run multiple containers that need to work together\n. A Pod can\nencapsulate an application composed of\nmultiple co-located containers that are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit of servicefor example, one container serving data\nstored in a shared volume to the public, while a separate\nsidecar container\nrefreshes or updates those files.\nThe Pod wraps these containers, storage resources, and an ephemeral network\nidentity together as a single unit.\nFor example, you might have a container that\nacts as a web server for files in a shared volume, and a separate\nsidecar container\nthat updates those files from a remote source, as in the following diagram:\nSome Pods have\ninit containers\nas well as\napp containers\n.\nBy default, init containers run and complete before the app containers are started.\nYou can also have\nsidecar containers\nthat provide auxiliary services to the main application Pod (for example: a service mesh).\nFEATURE STATE:\nKubernetes v1.33 [stable]\n(enabled by default)\nEnabled by default, the\nSidecarContainers\nfeature gate\nallows you to specify\nrestartPolicy: Always\nfor init containers.\nSetting the\nAlways\nrestart policy ensures that the containers where you set it are\ntreated as\nsidecars\nthat are kept running during the entire lifetime of the Pod.\nContainers that you explicitly define as sidecar containers\nstart up before the main application Pod and remain running until the Pod is\nshut down.\nContainer probes\nA\nprobe\nis a diagnostic performed periodically by the kubelet on a container.\nTo perform a diagnostic, the kubelet can invoke different actions:\nExecAction\n(performed with the help of the container runtime)\nTCPSocketAction\n(checked directly by the kubelet)\nHTTPGetAction\n(checked directly by the kubelet)\nYou can read more about\nprobes\nin the Pod Lifecycle documentation.\nWhat's next\nLearn about the\nlifecycle of a Pod\n.\nLearn about\nRuntimeClass\nand how you can use it to\nconfigure different Pods with different container runtime configurations.\nRead about\nPodDisruptionBudget\nand how you can use it to manage application availability during disruptions.\nPod is a top-level resource in the Kubernetes REST API.\nThe\nPod\nobject definition describes the object in detail.\nThe Distributed System Toolkit: Patterns for Composite Containers\nexplains common layouts for Pods with more than one container.\nRead about\nPod topology spread constraints\nTo understand the context for why Kubernetes wraps a common Pod API in other resources\n(such as\nStatefulSets\nor\nDeployments\n),\nyou can read about the prior art, including:\nAurora\nBorg\nMarathon\nOmega\nTupperware\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 28, 2025 at 3:15 PM PST:\nFix broken link in workloads/pods/_index.md (c09d15ee70)\nPod API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/workloads/pods/"}}
{"text": "Service | Kubernetes\nService\nExpose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.\nIn Kubernetes, a Service is a method for exposing a network application that is running as one or more\nPods\nin your cluster.\nA key aim of Services in Kubernetes is that you don't need to modify your existing\napplication to use an unfamiliar service discovery mechanism.\nYou can run code in Pods, whether this is a code designed for a cloud-native world, or\nan older app you've containerized. You use a Service to make that set of Pods available\non the network so that clients can interact with it.\nIf you use a\nDeployment\nto run your app,\nthat Deployment can create and destroy Pods dynamically. From one moment to the next,\nyou don't know how many of those Pods are working and healthy; you might not even know\nwhat those healthy Pods are named.\nKubernetes\nPods\nare created and destroyed\nto match the desired state of your cluster. Pods are ephemeral resources (you should not\nexpect that an individual Pod is reliable and durable).\nEach Pod gets its own IP address (Kubernetes expects network plugins to ensure this).\nFor a given Deployment in your cluster, the set of Pods running in one moment in\ntime could be different from the set of Pods running that application a moment later.\nThis leads to a problem: if some set of Pods (call them \"backends\") provides\nfunctionality to other Pods (call them \"frontends\") inside your cluster,\nhow do the frontends find out and keep track of which IP address to connect\nto, so that the frontend can use the backend part of the workload?\nEnter\nServices\n.\nServices in Kubernetes\nThe Service API, part of Kubernetes, is an abstraction to help you expose groups of\nPods over a network. Each Service object defines a logical set of endpoints (usually\nthese endpoints are Pods) along with a policy about how to make those pods accessible.\nFor example, consider a stateless image-processing backend which is running with\n3 replicas. Those replicas are fungiblefrontends do not care which backend\nthey use. While the actual Pods that compose the backend set may change, the\nfrontend clients should not need to be aware of that, nor should they need to keep\ntrack of the set of backends themselves.\nThe Service abstraction enables this decoupling.\nThe set of Pods targeted by a Service is usually determined\nby a\nselector\nthat you\ndefine.\nTo learn about other ways to define Service endpoints,\nsee\nServices\nwithout\nselectors\n.\nIf your workload speaks HTTP, you might choose to use an\nIngress\nto control how web traffic\nreaches that workload.\nIngress is not a Service type, but it acts as the entry point for your\ncluster. An Ingress lets you consolidate your routing rules into a single resource, so\nthat you can expose multiple components of your workload, running separately in your\ncluster, behind a single listener.\nThe\nGateway\nAPI for Kubernetes\nprovides extra capabilities beyond Ingress and Service. You can add Gateway to your cluster -\nit is a family of extension APIs, implemented using\nCustomResourceDefinitions\n-\nand then use these to configure access to network services that are running in your cluster.\nCloud-native service discovery\nIf you're able to use Kubernetes APIs for service discovery in your application,\nyou can query the\nAPI server\nfor matching EndpointSlices. Kubernetes updates the EndpointSlices for a Service\nwhenever the set of Pods in a Service changes.\nFor non-native applications, Kubernetes offers ways to place a network port or load\nbalancer in between your application and the backend Pods.\nEither way, your workload can use these\nservice discovery\nmechanisms to find the target it wants to connect to.\nDefining a Service\nA Service is an\nobject\n(the same way that a Pod or a ConfigMap is an object). You can create,\nview or modify Service definitions using the Kubernetes API. Usually\nyou use a tool such as\nkubectl\nto make those API calls for you.\nFor example, suppose you have a set of Pods that each listen on TCP port 9376\nand are labelled as\napp.kubernetes.io/name=MyApp\n. You can define a Service to\npublish that TCP listener:\nservice/simple-service.yaml\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nmy-service\nspec\n:\nselector\n:\napp.kubernetes.io/name\n:\nMyApp\nports\n:\n-\nprotocol\n:\nTCP\nport\n:\n80\ntargetPort\n:\n9376\nApplying this manifest creates a new Service named \"my-service\" with the default\nClusterIP\nservice type\n. The Service\ntargets TCP port 9376 on any Pod with the\napp.kubernetes.io/name: MyApp\nlabel.\nKubernetes assigns this Service an IP address (the\ncluster IP\n),\nthat is used by the virtual IP address mechanism. For more details on that mechanism,\nread\nVirtual IPs and Service Proxies\n.\nThe controller for that Service continuously scans for Pods that\nmatch its selector, and then makes any necessary updates to the set of\nEndpointSlices for the Service.\nThe name of a Service object must be a valid\nRFC 1035 label name\n.\nNote:\nA Service can map\nany\nincoming\nport\nto a\ntargetPort\n. By default and\nfor convenience, the\ntargetPort\nis set to the same value as the\nport\nfield.\nRelaxed naming requirements for Service objects\nFEATURE STATE:\nKubernetes v1.34 [alpha]\n(disabled by default)\nThe\nRelaxedServiceNameValidation\nfeature gate allows Service object names to start with a digit. When this feature gate is enabled, Service object names must be valid\nRFC 1123 label names\n.\nPort definitions\nPort definitions in Pods have names, and you can reference these names in the\ntargetPort\nattribute of a Service. For example, we can bind the\ntargetPort\nof the Service to the Pod port in the following way:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nnginx\nlabels\n:\napp.kubernetes.io/name\n:\nproxy\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:stable\nports\n:\n-\ncontainerPort\n:\n80\nname\n:\nhttp-web-svc\n---\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nnginx-service\nspec\n:\nselector\n:\napp.kubernetes.io/name\n:\nproxy\nports\n:\n-\nname\n:\nname-of-service-port\nprotocol\n:\nTCP\nport\n:\n80\ntargetPort\n:\nhttp-web-svc\nThis works even if there is a mixture of Pods in the Service using a single\nconfigured name, with the same network protocol available via different\nport numbers. This offers a lot of flexibility for deploying and evolving\nyour Services. For example, you can change the port numbers that Pods expose\nin the next version of your backend software, without breaking clients.\nThe default protocol for Services is\nTCP\n; you can also\nuse any other\nsupported protocol\n.\nBecause many Services need to expose more than one port, Kubernetes supports\nmultiple port definitions\nfor a single Service.\nEach port definition can have the same\nprotocol\n, or a different one.\nServices without selectors\nServices most commonly abstract access to Kubernetes Pods thanks to the selector,\nbut when used with a corresponding set of\nEndpointSlices\nobjects and without a selector, the Service can abstract other kinds of backends,\nincluding ones that run outside the cluster.\nFor example:\nYou want to have an external database cluster in production, but in your\ntest environment you use your own databases.\nYou want to point your Service to a Service in a different\nNamespace\nor on another cluster.\nYou are migrating a workload to Kubernetes. While evaluating the approach,\nyou run only a portion of your backends in Kubernetes.\nIn any of these scenarios you can define a Service\nwithout\nspecifying a\nselector to match Pods. For example:\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nmy-service\nspec\n:\nports\n:\n-\nname\n:\nhttp\nprotocol\n:\nTCP\nport\n:\n80\ntargetPort\n:\n9376\nBecause this Service has no selector, the corresponding EndpointSlice\nobjects are not created automatically. You can map the Service\nto the network address and port where it's running, by adding an EndpointSlice\nobject manually. For example:\napiVersion\n:\ndiscovery.k8s.io/v1\nkind\n:\nEndpointSlice\nmetadata\n:\nname\n:\nmy-service-1\n# by convention, use the name of the Service\n# as a prefix for the name of the EndpointSlice\nlabels\n:\n# You should set the \"kubernetes.io/service-name\" label.\n# Set its value to match the name of the Service\nkubernetes.io/service-name\n:\nmy-service\naddressType\n:\nIPv4\nports\n:\n-\nname\n:\nhttp\n# should match with the name of the service port defined above\nappProtocol\n:\nhttp\nprotocol\n:\nTCP\nport\n:\n9376\nendpoints\n:\n-\naddresses\n:\n-\n\"10.4.5.6\"\n-\naddresses\n:\n-\n\"10.1.2.3\"\nCustom EndpointSlices\nWhen you create an\nEndpointSlice\nobject for a Service, you can\nuse any name for the EndpointSlice. Each EndpointSlice in a namespace must have a\nunique name. You link an EndpointSlice to a Service by setting the\nkubernetes.io/service-name\nlabel\non that EndpointSlice.\nNote:\nThe endpoint IPs\nmust not\nbe: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or\nlink-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).\nThe endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services,\nbecause\nkube-proxy\ndoesn't support virtual IPs\nas a destination.\nFor an EndpointSlice that you create yourself, or in your own code,\nyou should also pick a value to use for the label\nendpointslice.kubernetes.io/managed-by\n.\nIf you create your own controller code to manage EndpointSlices, consider using a\nvalue similar to\n\"my-domain.example/name-of-controller\"\n. If you are using a third\nparty tool, use the name of the tool in all-lowercase and change spaces and other\npunctuation to dashes (\n-\n).\nIf people are directly using a tool such as\nkubectl\nto manage EndpointSlices,\nuse a name that describes this manual management, such as\n\"staff\"\nor\n\"cluster-admins\"\n. You should\navoid using the reserved value\n\"controller\"\n, which identifies EndpointSlices\nmanaged by Kubernetes' own control plane.\nAccessing a Service without a selector\nAccessing a Service without a selector works the same as if it had a selector.\nIn the\nexample\nfor a Service without a selector,\ntraffic is routed to one of the two endpoints defined in\nthe EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.\nNote:\nThe Kubernetes API server does not allow proxying to endpoints that are not mapped to\npods. Actions such as\nkubectl port-forward service/<service-name> forwardedPort:servicePort\nwhere the service has no\nselector will fail due to this constraint. This prevents the Kubernetes API server\nfrom being used as a proxy to endpoints the caller may not be authorized to access.\nAn\nExternalName\nService is a special case of Service that does not have\nselectors and uses DNS names instead. For more information, see the\nExternalName\nsection.\nEndpointSlices\nFEATURE STATE:\nKubernetes v1.21 [stable]\nEndpointSlices\nare objects that\nrepresent a subset (a\nslice\n) of the backing network endpoints for a Service.\nYour Kubernetes cluster tracks how many endpoints each EndpointSlice represents.\nIf there are so many endpoints for a Service that a threshold is reached, then\nKubernetes adds another empty EndpointSlice and stores new endpoint information\nthere.\nBy default, Kubernetes makes a new EndpointSlice once the existing EndpointSlices\nall contain at least 100 endpoints. Kubernetes does not make the new EndpointSlice\nuntil an extra endpoint needs to be added.\nSee\nEndpointSlices\nfor more\ninformation about this API.\nEndpoints (deprecated)\nFEATURE STATE:\nKubernetes v1.33 [deprecated]\nThe EndpointSlice API is the evolution of the older\nEndpoints\nAPI. The deprecated Endpoints API has several problems relative to\nEndpointSlice:\nIt does not support dual-stack clusters.\nIt does not contain information needed to support newer features, such as\ntrafficDistribution\n.\nIt will truncate the list of endpoints if it is too long to fit in a single object.\nBecause of this, it is recommended that all clients use the\nEndpointSlice API rather than Endpoints.\nOver-capacity endpoints\nKubernetes limits the number of endpoints that can fit in a single Endpoints\nobject. When there are over 1000 backing endpoints for a Service, Kubernetes\ntruncates the data in the Endpoints object. Because a Service can be linked\nwith more than one EndpointSlice, the 1000 backing endpoint limit only\naffects the legacy Endpoints API.\nIn that case, Kubernetes selects at most 1000 possible backend endpoints to store\ninto the Endpoints object, and sets an\nannotation\non the Endpoints:\nendpoints.kubernetes.io/over-capacity: truncated\n.\nThe control plane also removes that annotation if the number of backend Pods drops below 1000.\nTraffic is still sent to backends, but any load balancing mechanism that relies on the\nlegacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints.\nThe same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.\nApplication protocol\nFEATURE STATE:\nKubernetes v1.20 [stable]\nThe\nappProtocol\nfield provides a way to specify an application protocol for\neach Service port. This is used as a hint for implementations to offer\nricher behavior for protocols that they understand.\nThe value of this field is mirrored by the corresponding\nEndpoints and EndpointSlice objects.\nThis field follows standard Kubernetes label syntax. Valid values are one of:\nIANA standard service names\n.\nImplementation-defined prefixed names such as\nmycompany.com/my-custom-protocol\n.\nKubernetes-defined prefixed names:\nProtocol\nDescription\nkubernetes.io/h2c\nHTTP/2 over cleartext as described in\nRFC 7540\nkubernetes.io/ws\nWebSocket over cleartext as described in\nRFC 6455\nkubernetes.io/wss\nWebSocket over TLS as described in\nRFC 6455\nMulti-port Services\nFor some Services, you need to expose more than one port.\nKubernetes lets you configure multiple port definitions on a Service object.\nWhen using multiple ports for a Service, you must give all of your ports names\nso that these are unambiguous.\nFor example:\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nmy-service\nspec\n:\nselector\n:\napp.kubernetes.io/name\n:\nMyApp\nports\n:\n-\nname\n:\nhttp\nprotocol\n:\nTCP\nport\n:\n80\ntargetPort\n:\n9376\n-\nname\n:\nhttps\nprotocol\n:\nTCP\nport\n:\n443\ntargetPort\n:\n9377\nNote:\nAs with Kubernetes\nnames\nin general, names for ports\nmust only contain lowercase alphanumeric characters and\n-\n. Port names must\nalso start and end with an alphanumeric character.\nFor example, the names\n123-abc\nand\nweb\nare valid, but\n123_abc\nand\n-web\nare not.\nService type\nFor some parts of your application (for example, frontends) you may want to expose a\nService onto an external IP address, one that's accessible from outside of your\ncluster.\nKubernetes Service types allow you to specify what kind of Service you want.\nThe available\ntype\nvalues and their behaviors are:\nClusterIP\nExposes the Service on a cluster-internal IP. Choosing this value\nmakes the Service only reachable from within the cluster. This is the\ndefault that is used if you don't explicitly specify a\ntype\nfor a Service.\nYou can expose the Service to the public internet using an\nIngress\nor a\nGateway\n.\nNodePort\nExposes the Service on each Node's IP at a static port (the\nNodePort\n).\nTo make the node port available, Kubernetes sets up a cluster IP address,\nthe same as if you had requested a Service of\ntype: ClusterIP\n.\nLoadBalancer\nExposes the Service externally using an external load balancer. Kubernetes\ndoes not directly offer a load balancing component; you must provide one, or\nyou can integrate your Kubernetes cluster with a cloud provider.\nExternalName\nMaps the Service to the contents of the\nexternalName\nfield (for example,\nto the hostname\napi.foo.bar.example\n). The mapping configures your cluster's\nDNS server to return a\nCNAME\nrecord with that external hostname value.\nNo proxying of any kind is set up.\nThe\ntype\nfield in the Service API is designed as nested functionality - each level\nadds to the previous. However there is an exception to this nested design. You can\ndefine a\nLoadBalancer\nService by\ndisabling the load balancer\nNodePort\nallocation\n.\ntype: ClusterIP\nThis default Service type assigns an IP address from a pool of IP addresses that\nyour cluster has reserved for that purpose.\nSeveral of the other types for Service build on the\nClusterIP\ntype as a\nfoundation.\nIf you define a Service that has the\n.spec.clusterIP\nset to\n\"None\"\nthen\nKubernetes does not assign an IP address. See\nheadless Services\nfor more information.\nChoosing your own IP address\nYou can specify your own cluster IP address as part of a\nService\ncreation\nrequest. To do this, set the\n.spec.clusterIP\nfield. For example, if you\nalready have an existing DNS entry that you wish to reuse, or legacy systems\nthat are configured for a specific IP address and difficult to re-configure.\nThe IP address that you choose must be a valid IPv4 or IPv6 address from within the\nservice-cluster-ip-range\nCIDR range that is configured for the API server.\nIf you try to create a Service with an invalid\nclusterIP\naddress value, the API\nserver will return a 422 HTTP status code to indicate that there's a problem.\nRead\navoiding collisions\nto learn how Kubernetes helps reduce the risk and impact of two different Services\nboth trying to use the same IP address.\ntype: NodePort\nIf you set the\ntype\nfield to\nNodePort\n, the Kubernetes control plane\nallocates a port from a range specified by\n--service-node-port-range\nflag (default: 30000-32767).\nEach node proxies that port (the same port number on every Node) into your Service.\nYour Service reports the allocated port in its\n.spec.ports[*].nodePort\nfield.\nUsing a NodePort gives you the freedom to set up your own load balancing solution,\nto configure environments that are not fully supported by Kubernetes, or even\nto expose one or more nodes' IP addresses directly.\nFor a node port Service, Kubernetes additionally allocates a port (TCP, UDP or\nSCTP to match the protocol of the Service). Every node in the cluster configures\nitself to listen on that assigned port and to forward traffic to one of the ready\nendpoints associated with that Service. You'll be able to contact the\ntype: NodePort\nService, from outside the cluster, by connecting to any node using the appropriate\nprotocol (for example: TCP), and the appropriate port (as assigned to that Service).\nChoosing your own port\nIf you want a specific port number, you can specify a value in the\nnodePort\nfield. The control plane will either allocate you that port or report that\nthe API transaction failed.\nThis means that you need to take care of possible port collisions yourself.\nYou also have to use a valid port number, one that's inside the range configured\nfor NodePort use.\nHere is an example manifest for a Service of\ntype: NodePort\nthat specifies\na NodePort value (30007, in this example):\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nmy-service\nspec\n:\ntype\n:\nNodePort\nselector\n:\napp.kubernetes.io/name\n:\nMyApp\nports\n:\n-\nport\n:\n80\n# By default and for convenience, the `targetPort` is set to\n# the same value as the `port` field.\ntargetPort\n:\n80\n# Optional field\n# By default and for convenience, the Kubernetes control plane\n# will allocate a port from a range (default: 30000-32767)\nnodePort\n:\n30007\nReserve Nodeport ranges to avoid collisions\nThe policy for assigning ports to NodePort services applies to both the auto-assignment and\nthe manual assignment scenarios. When a user wants to create a NodePort service that\nuses a specific port, the target port may conflict with another port that has already been assigned.\nTo avoid this problem, the port range for NodePort services is divided into two bands.\nDynamic port assignment uses the upper band by default, and it may use the lower band once the\nupper band has been exhausted. Users can then allocate from the lower band with a lower risk of port collision.\nCustom IP address configuration for\ntype: NodePort\nServices\nYou can set up nodes in your cluster to use a particular IP address for serving node port\nservices. You might want to do this if each node is connected to multiple networks (for example:\none network for application traffic, and another network for traffic between nodes and the\ncontrol plane).\nIf you want to specify particular IP address(es) to proxy the port, you can set the\n--nodeport-addresses\nflag for kube-proxy or the equivalent\nnodePortAddresses\nfield of the\nkube-proxy configuration file\nto particular IP block(s).\nThis flag takes a comma-delimited list of IP blocks (e.g.\n10.0.0.0/8\n,\n192.0.2.0/25\n)\nto specify IP address ranges that kube-proxy should consider as local to this node.\nFor example, if you start kube-proxy with the\n--nodeport-addresses=127.0.0.0/8\nflag,\nkube-proxy only selects the loopback interface for NodePort Services.\nThe default for\n--nodeport-addresses\nis an empty list.\nThis means that kube-proxy should consider all available network interfaces for NodePort.\n(That's also compatible with earlier Kubernetes releases.)\nNote:\nThis Service is visible as\n<NodeIP>:spec.ports[*].nodePort\nand\n.spec.clusterIP:spec.ports[*].port\n.\nIf the\n--nodeport-addresses\nflag for kube-proxy or the equivalent field\nin the kube-proxy configuration file is set,\n<NodeIP>\nwould be a filtered\nnode IP address (or possibly IP addresses).\ntype: LoadBalancer\nOn cloud providers which support external load balancers, setting the\ntype\nfield to\nLoadBalancer\nprovisions a load balancer for your Service.\nThe actual creation of the load balancer happens asynchronously, and\ninformation about the provisioned balancer is published in the Service's\n.status.loadBalancer\nfield.\nFor example:\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nmy-service\nspec\n:\nselector\n:\napp.kubernetes.io/name\n:\nMyApp\nports\n:\n-\nprotocol\n:\nTCP\nport\n:\n80\ntargetPort\n:\n9376\nclusterIP\n:\n10.0.171.239\ntype\n:\nLoadBalancer\nstatus\n:\nloadBalancer\n:\ningress\n:\n-\nip\n:\n192.0.2.127\nTraffic from the external load balancer is directed at the backend Pods. The cloud\nprovider decides how it is load balanced.\nTo implement a Service of\ntype: LoadBalancer\n, Kubernetes typically starts off\nby making the changes that are equivalent to you requesting a Service of\ntype: NodePort\n. The cloud-controller-manager component then configures the external\nload balancer to forward traffic to that assigned node port.\nYou can configure a load balanced Service to\nomit\nassigning a node port, provided that the\ncloud provider implementation supports this.\nSome cloud providers allow you to specify the\nloadBalancerIP\n. In those cases, the load-balancer is created\nwith the user-specified\nloadBalancerIP\n. If the\nloadBalancerIP\nfield is not specified,\nthe load balancer is set up with an ephemeral IP address. If you specify a\nloadBalancerIP\nbut your cloud provider does not support the feature, the\nloadbalancerIP\nfield that you\nset is ignored.\nNote:\nThe\n.spec.loadBalancerIP\nfield for a Service was deprecated in Kubernetes v1.24.\nThis field was under-specified and its meaning varies across implementations.\nIt also cannot support dual-stack networking. This field may be removed in a future API version.\nIf you're integrating with a provider that supports specifying the load balancer IP address(es)\nfor a Service via a (provider specific) annotation, you should switch to doing that.\nIf you are writing code for a load balancer integration with Kubernetes, avoid using this field.\nYou can integrate with\nGateway\nrather than Service, or you\ncan define your own (provider specific) annotations on the Service that specify the equivalent detail.\nNode liveness impact on load balancer traffic\nLoad balancer health checks are critical to modern applications. They are used to\ndetermine which server (virtual machine, or IP address) the load balancer should\ndispatch traffic to. The Kubernetes APIs do not define how health checks have to be\nimplemented for Kubernetes managed load balancers, instead it's the cloud providers\n(and the people implementing integration code) who decide on the behavior. Load\nbalancer health checks are extensively used within the context of supporting the\nexternalTrafficPolicy\nfield for Services.\nLoad balancers with mixed protocol types\nFEATURE STATE:\nKubernetes v1.26 [stable]\n(enabled by default)\nBy default, for LoadBalancer type of Services, when there is more than one port defined, all\nports must have the same protocol, and the protocol must be one which is supported\nby the cloud provider.\nThe feature gate\nMixedProtocolLBService\n(enabled by default for the kube-apiserver as of v1.24) allows the use of\ndifferent protocols for LoadBalancer type of Services, when there is more than one port defined.\nNote:\nThe set of protocols that can be used for load balanced Services is defined by your\ncloud provider; they may impose restrictions beyond what the Kubernetes API enforces.\nDisabling load balancer NodePort allocation\nFEATURE STATE:\nKubernetes v1.24 [stable]\nYou can optionally disable node port allocation for a Service of\ntype: LoadBalancer\n, by setting\nthe field\nspec.allocateLoadBalancerNodePorts\nto\nfalse\n. This should only be used for load balancer implementations\nthat route traffic directly to pods as opposed to using node ports. By default,\nspec.allocateLoadBalancerNodePorts\nis\ntrue\nand type LoadBalancer Services will continue to allocate node ports. If\nspec.allocateLoadBalancerNodePorts\nis set to\nfalse\non an existing Service with allocated node ports, those node ports will\nnot\nbe de-allocated automatically.\nYou must explicitly remove the\nnodePorts\nentry in every Service port to de-allocate those node ports.\nSpecifying class of load balancer implementation\nFEATURE STATE:\nKubernetes v1.24 [stable]\nFor a Service with\ntype\nset to\nLoadBalancer\n, the\n.spec.loadBalancerClass\nfield\nenables you to use a load balancer implementation other than the cloud provider default.\nBy default,\n.spec.loadBalancerClass\nis not set and a\nLoadBalancer\ntype of Service uses the cloud provider's default load balancer implementation if the\ncluster is configured with a cloud provider using the\n--cloud-provider\ncomponent\nflag.\nIf you specify\n.spec.loadBalancerClass\n, it is assumed that a load balancer\nimplementation that matches the specified class is watching for Services.\nAny default load balancer implementation (for example, the one provided by\nthe cloud provider) will ignore Services that have this field set.\nspec.loadBalancerClass\ncan be set on a Service of type\nLoadBalancer\nonly.\nOnce set, it cannot be changed.\nThe value of\nspec.loadBalancerClass\nmust be a label-style identifier,\nwith an optional prefix such as \"\ninternal-vip\n\" or \"\nexample.com/internal-vip\n\".\nUnprefixed names are reserved for end-users.\nLoad balancer IP address mode\nFEATURE STATE:\nKubernetes v1.32 [stable]\n(enabled by default)\nFor a Service of\ntype: LoadBalancer\n, a controller can set\n.status.loadBalancer.ingress.ipMode\n.\nThe\n.status.loadBalancer.ingress.ipMode\nspecifies how the load-balancer IP behaves.\nIt may be specified only when the\n.status.loadBalancer.ingress.ip\nfield is also specified.\nThere are two possible values for\n.status.loadBalancer.ingress.ipMode\n: \"VIP\" and \"Proxy\".\nThe default value is \"VIP\" meaning that traffic is delivered to the node\nwith the destination set to the load-balancer's IP and port.\nThere are two cases when setting this to \"Proxy\", depending on how the load-balancer\nfrom the cloud provider delivers the traffics:\nIf the traffic is delivered to the node then DNATed to the pod, the destination would be set to the node's IP and node port;\nIf the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port.\nService implementations may use this information to adjust traffic routing.\nInternal load balancer\nIn a mixed environment it is sometimes necessary to route traffic from Services inside the same\n(virtual) network address block.\nIn a split-horizon DNS environment you would need two Services to be able to route both external\nand internal traffic to your endpoints.\nTo set an internal load balancer, add one of the following annotations to your Service\ndepending on the cloud service provider you're using:\nDefault\nGCP\nAWS\nAzure\nIBM Cloud\nOpenStack\nBaidu Cloud\nTencent Cloud\nAlibaba Cloud\nOCI\nSelect one of the tabs.\nmetadata\n:\nname\n:\nmy-service\nannotations\n:\nnetworking.gke.io/load-balancer-type\n:\n\"Internal\"\nmetadata\n:\nname\n:\nmy-service\nannotations\n:\nservice.beta.kubernetes.io/aws-load-balancer-scheme\n:\n\"internal\"\nmetadata\n:\nname\n:\nmy-service\nannotations\n:\nservice.beta.kubernetes.io/azure-load-balancer-internal\n:\n\"true\"\nmetadata\n:\nname\n:\nmy-service\nannotations\n:\nservice.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type\n:\n\"private\"\nmetadata\n:\nname\n:\nmy-service\nannotations\n:\nservice.beta.kubernetes.io/openstack-internal-load-balancer\n:\n\"true\"\nmetadata\n:\nname\n:\nmy-service\nannotations\n:\nservice.beta.kubernetes.io/cce-load-balancer-internal-vpc\n:\n\"true\"\nmetadata\n:\nannotations\n:\nservice.kubernetes.io/qcloud-loadbalancer-internal-subnetid\n:\nsubnet-xxxxx\nmetadata\n:\nannotations\n:\nservice.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type\n:\n\"intranet\"\nmetadata\n:\nname\n:\nmy-service\nannotations\n:\nservice.beta.kubernetes.io/oci-load-balancer-internal\n:\ntrue\ntype: ExternalName\nServices of type ExternalName map a Service to a DNS name, not to a typical selector such as\nmy-service\nor\ncassandra\n. You specify these Services with the\nspec.externalName\nparameter.\nThis Service definition, for example, maps\nthe\nmy-service\nService in the\nprod\nnamespace to\nmy.database.example.com\n:\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nmy-service\nnamespace\n:\nprod\nspec\n:\ntype\n:\nExternalName\nexternalName\n:\nmy.database.example.com\nNote:\nA Service of\ntype: ExternalName\naccepts an IPv4 address string,\nbut treats that string as a DNS name comprised of digits,\nnot as an IP address (the internet does not however allow such names in DNS).\nServices with external names that resemble IPv4\naddresses are not resolved by DNS servers.\nIf you want to map a Service directly to a specific IP address, consider using\nheadless Services\n.\nWhen looking up the host\nmy-service.prod.svc.cluster.local\n, the cluster DNS Service\nreturns a\nCNAME\nrecord with the value\nmy.database.example.com\n. Accessing\nmy-service\nworks in the same way as other Services but with the crucial\ndifference that redirection happens at the DNS level rather than via proxying or\nforwarding. Should you later decide to move your database into your cluster, you\ncan start its Pods, add appropriate selectors or endpoints, and change the\nService's\ntype\n.\nCaution:\nYou may have trouble using ExternalName for some common protocols, including HTTP and HTTPS.\nIf you use ExternalName then the hostname used by clients inside your cluster is different from\nthe name that the ExternalName references.\nFor protocols that use hostnames this difference may lead to errors or unexpected responses.\nHTTP requests will have a\nHost:\nheader that the origin server does not recognize;\nTLS servers will not be able to provide a certificate matching the hostname that the client connected to.\nHeadless Services\nSometimes you don't need load-balancing and a single Service IP. In\nthis case, you can create what are termed\nheadless Services\n, by explicitly\nspecifying\n\"None\"\nfor the cluster IP address (\n.spec.clusterIP\n).\nYou can use a headless Service to interface with other service discovery mechanisms,\nwithout being tied to Kubernetes' implementation.\nFor headless Services, a cluster IP is not allocated, kube-proxy does not handle\nthese Services, and there is no load balancing or proxying done by the platform for them.\nA headless Service allows a client to connect to whichever Pod it prefers, directly. Services that are headless don't\nconfigure routes and packet forwarding using\nvirtual IP addresses and proxies\n; instead, headless Services report the\nendpoint IP addresses of the individual pods via internal DNS records, served through the cluster's\nDNS service\n.\nTo define a headless Service, you make a Service with\n.spec.type\nset to ClusterIP (which is also the default for\ntype\n),\nand you additionally set\n.spec.clusterIP\nto None.\nThe string value None is a special case and is not the same as leaving the\n.spec.clusterIP\nfield unset.\nHow DNS is automatically configured depends on whether the Service has selectors defined:\nWith selectors\nFor headless Services that define selectors, the endpoints controller creates\nEndpointSlices in the Kubernetes API, and modifies the DNS configuration to return\nA or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing the Service.\nWithout selectors\nFor headless Services that do not define selectors, the control plane does\nnot create EndpointSlice objects. However, the DNS system looks for and configures\neither:\nDNS CNAME records for\ntype: ExternalName\nServices.\nDNS A / AAAA records for all IP addresses of the Service's ready endpoints,\nfor all Service types other than\nExternalName\n.\nFor IPv4 endpoints, the DNS system creates A records.\nFor IPv6 endpoints, the DNS system creates AAAA records.\nWhen you define a headless Service without a selector, the\nport\nmust\nmatch the\ntargetPort\n.\nDiscovering services\nFor clients running inside your cluster, Kubernetes supports two primary modes of\nfinding a Service: environment variables and DNS.\nEnvironment variables\nWhen a Pod is run on a Node, the kubelet adds a set of environment variables\nfor each active Service. It adds\n{SVCNAME}_SERVICE_HOST\nand\n{SVCNAME}_SERVICE_PORT\nvariables,\nwhere the Service name is upper-cased and dashes are converted to underscores.\nFor example, the Service\nredis-primary\nwhich exposes TCP port 6379 and has been\nallocated cluster IP address 10.0.0.11, produces the following environment\nvariables:\nREDIS_PRIMARY_SERVICE_HOST\n=\n10.0.0.11\nREDIS_PRIMARY_SERVICE_PORT\n=\n6379\nREDIS_PRIMARY_PORT\n=\ntcp://10.0.0.11:6379\nREDIS_PRIMARY_PORT_6379_TCP\n=\ntcp://10.0.0.11:6379\nREDIS_PRIMARY_PORT_6379_TCP_PROTO\n=\ntcp\nREDIS_PRIMARY_PORT_6379_TCP_PORT\n=\n6379\nREDIS_PRIMARY_PORT_6379_TCP_ADDR\n=\n10.0.0.11\nNote:\nWhen you have a Pod that needs to access a Service, and you are using\nthe environment variable method to publish the port and cluster IP to the client\nPods, you must create the Service\nbefore\nthe client Pods come into existence.\nOtherwise, those client Pods won't have their environment variables populated.\nIf you only use DNS to discover the cluster IP for a Service, you don't need to\nworry about this ordering issue.\nKubernetes also supports and provides variables that are compatible with Docker\nEngine's \"\nlegacy container links\n\" feature.\nYou can read\nmakeLinkVariables\nto see how this is implemented in Kubernetes.\nDNS\nYou can (and almost always should) set up a DNS service for your Kubernetes\ncluster using an\nadd-on\n.\nA cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new\nServices and creates a set of DNS records for each one. If DNS has been enabled\nthroughout your cluster then all Pods should automatically be able to resolve\nServices by their DNS name.\nFor example, if you have a Service called\nmy-service\nin a Kubernetes\nnamespace\nmy-ns\n, the control plane and the DNS Service acting together\ncreate a DNS record for\nmy-service.my-ns\n. Pods in the\nmy-ns\nnamespace\nshould be able to find the service by doing a name lookup for\nmy-service\n(\nmy-service.my-ns\nwould also work).\nPods in other namespaces must qualify the name as\nmy-service.my-ns\n. These names\nwill resolve to the cluster IP assigned for the Service.\nKubernetes also supports DNS SRV (Service) records for named ports. If the\nmy-service.my-ns\nService has a port named\nhttp\nwith the protocol set to\nTCP\n, you can do a DNS SRV query for\n_http._tcp.my-service.my-ns\nto discover\nthe port number for\nhttp\n, as well as the IP address.\nThe Kubernetes DNS server is the only way to access\nExternalName\nServices.\nYou can find more information about\nExternalName\nresolution in\nDNS for Services and Pods\n.\nVirtual IP addressing mechanism\nRead\nVirtual IPs and Service Proxies\nexplains the\nmechanism Kubernetes provides to expose a Service with a virtual IP address.\nTraffic policies\nYou can set the\n.spec.internalTrafficPolicy\nand\n.spec.externalTrafficPolicy\nfields\nto control how Kubernetes routes traffic to healthy (ready) backends.\nSee\nTraffic Policies\nfor more details.\nTraffic distribution\nFEATURE STATE:\nKubernetes v1.33 [stable]\n(enabled by default)\nThe\n.spec.trafficDistribution\nfield provides another way to influence traffic\nrouting within a Kubernetes Service. While traffic policies focus on strict\nsemantic guarantees, traffic distribution allows you to express\npreferences\n(such as routing to topologically closer endpoints). This can help optimize for\nperformance, cost, or reliability. In Kubernetes 1.34, the\nfollowing field value is supported:\nPreferClose\nIndicates a preference for routing traffic to endpoints that are in the same\nzone as the client.\nFEATURE STATE:\nKubernetes v1.34 [beta]\n(enabled by default)\nIn Kubernetes 1.34, two additional values are\navailable (unless the\nPreferSameTrafficDistribution\nfeature\ngate\nis\ndisabled):\nPreferSameZone\nThis is an alias for\nPreferClose\nthat is clearer about the intended semantics.\nPreferSameNode\nIndicates a preference for routing traffic to endpoints that are on the same\nnode as the client.\nIf the field is not set, the implementation will apply its default routing strategy.\nSee\nTraffic\nDistribution\nfor\nmore details\nSession stickiness\nIf you want to make sure that connections from a particular client are passed to\nthe same Pod each time, you can configure session affinity based on the client's\nIP address. Read\nsession affinity\nto learn more.\nExternal IPs\nIf there are external IPs that route to one or more cluster nodes, Kubernetes Services\ncan be exposed on those\nexternalIPs\n. When network traffic arrives into the cluster, with\nthe external IP (as destination IP) and the port matching that Service, rules and routes\nthat Kubernetes has configured ensure that the traffic is routed to one of the endpoints\nfor that Service.\nWhen you define a Service, you can specify\nexternalIPs\nfor any\nservice type\n.\nIn the example below, the Service named\n\"my-service\"\ncan be accessed by clients using TCP,\non\n\"198.51.100.32:80\"\n(calculated from\n.spec.externalIPs[]\nand\n.spec.ports[].port\n).\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nmy-service\nspec\n:\nselector\n:\napp.kubernetes.io/name\n:\nMyApp\nports\n:\n-\nname\n:\nhttp\nprotocol\n:\nTCP\nport\n:\n80\ntargetPort\n:\n49152\nexternalIPs\n:\n-\n198.51.100.32\nNote:\nKubernetes does not manage allocation of\nexternalIPs\n; these are the responsibility\nof the cluster administrator.\nAPI Object\nService is a top-level resource in the Kubernetes REST API. You can find more details\nabout the\nService API object\n.\nWhat's next\nLearn more about Services and how they fit into Kubernetes:\nFollow the\nConnecting Applications with Services\ntutorial.\nRead about\nIngress\n, which\nexposes HTTP and HTTPS routes from outside the cluster to Services within\nyour cluster.\nRead about\nGateway\n, an extension to\nKubernetes that provides more flexibility than Ingress.\nFor more context, read the following:\nVirtual IPs and Service Proxies\nEndpointSlices\nService API reference\nEndpointSlice API reference\nEndpoint API reference (legacy)\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified September 28, 2025 at 5:22 AM PST:\nupdated deprecated sample code on service/#internal-load-balancer (c1dd9eb799)\nService API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/services-networking/service/"}}
{"text": "Volumes | Kubernetes\nVolumes\nKubernetes\nvolumes\nprovide a way for containers in a\npod\nto access and share data via the filesystem. There are different kinds of volume that you can use for different purposes,\nsuch as:\npopulating a configuration file based on a\nConfigMap\nor a\nSecret\nproviding some temporary scratch space for a pod\nsharing a filesystem between two different containers in the same pod\nsharing a filesystem between two different pods (even if those Pods run on different nodes)\ndurably storing data so that it stays available even if the Pod restarts or is replaced\npassing configuration information to an app running in a container, based on details of the Pod\nthe container is in\n(for example: telling a\nsidecar container\nwhat namespace the Pod is running in)\nproviding read-only access to data in a different container image\nData sharing can be between different local processes within a container, or between different containers,\nor between Pods.\nWhy volumes are important\nData persistence:\nOn-disk files in a container are ephemeral, which presents some problems for\nnon-trivial applications when running in containers. One problem occurs when\na container crashes or is stopped, the container state is not saved so all of the\nfiles that were created or modified during the lifetime of the container are lost.\nAfter a crash, kubelet restarts the container with a clean state.\nShared storage:\nAnother problem occurs when multiple containers are running in a\nPod\nand\nneed to share files. It can be challenging to set up\nand access a shared filesystem across all of the containers.\nThe Kubernetes\nvolume\nabstraction\ncan help you to solve both of these problems.\nBefore you learn about volumes, PersistentVolumes and PersistentVolumeClaims, you should read up\nabout\nPods\nand make sure that you understand how\nKubernetes uses Pods to run containers.\nHow volumes work\nKubernetes supports many types of volumes. A\nPod\ncan use any number of volume types simultaneously.\nEphemeral volume\ntypes have a lifetime linked to a specific Pod,\nbut\npersistent volumes\nexist beyond\nthe lifetime of any individual pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes;\nhowever, Kubernetes does not destroy persistent volumes.\nFor any kind of volume in a given pod, data is preserved across container restarts.\nAt its core, a volume is a directory, possibly with some data in it, which\nis accessible to the containers in a pod. How that directory comes to be, the\nmedium that backs it, and the contents of it are determined by the particular\nvolume type used.\nTo use a volume, specify the volumes to provide for the Pod in\n.spec.volumes\nand declare where to mount those volumes into containers in\n.spec.containers[*].volumeMounts\n.\nWhen a pod is launched, a process in the container sees a filesystem view composed from the initial contents of\nthe\ncontainer image\n, plus volumes\n(if defined) mounted inside the container.\nThe process sees a root filesystem that initially matches the contents of the container image.\nAny writes to within that filesystem hierarchy, if allowed, affect what that process views\nwhen it performs a subsequent filesystem access.\nVolumes are mounted at\nspecified paths\nwithin the container filesystem.\nFor each container defined within a Pod, you must independently specify where\nto mount each volume that the container uses.\nVolumes cannot mount within other volumes (but see\nUsing subPath\nfor a related mechanism). Also, a volume cannot contain a hard link to anything in\na different volume.\nTypes of volumes\nKubernetes supports several types of volumes.\nawsElasticBlockStore (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree\nawsElasticBlockStore\ntype\nare redirected to the\nebs.csi.aws.com\nCSI\ndriver.\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release\nand then removed entirely in the v1.27 release.\nThe Kubernetes project suggests that you use the\nAWS EBS\nthird party storage driver instead.\nazureDisk (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree\nazureDisk\ntype\nare redirected to the\ndisk.csi.azure.com\nCSI\ndriver.\nThe AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release\nand then removed entirely in the v1.27 release.\nThe Kubernetes project suggests that you use the\nAzure Disk\nthird party storage driver instead.\nazureFile (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree\nazureFile\ntype\nare redirected to the\nfile.csi.azure.com\nCSI\ndriver.\nThe AzureFile in-tree storage driver was deprecated in the Kubernetes v1.21 release\nand then removed entirely in the v1.30 release.\nThe Kubernetes project suggests that you use the\nAzure File\nthird party storage driver instead.\ncephfs (removed)\nKubernetes 1.34 does not include a\ncephfs\nvolume type.\nThe\ncephfs\nin-tree storage driver was deprecated in the Kubernetes v1.28\nrelease and then removed entirely in the v1.31 release.\ncinder (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree\ncinder\ntype\nare redirected to the\ncinder.csi.openstack.org\nCSI\ndriver.\nThe OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release\nand then removed entirely in the v1.26 release.\nThe Kubernetes project suggests that you use the\nOpenStack Cinder\nthird party storage driver instead.\nconfigMap\nA\nConfigMap\nprovides a way to inject configuration data into pods.\nThe data stored in a ConfigMap can be referenced in a volume of type\nconfigMap\nand then consumed by containerized applications running in a pod.\nWhen referencing a ConfigMap, you provide the name of the ConfigMap in the\nvolume. You can customize the path to use for a specific\nentry in the ConfigMap. The following configuration shows how to mount\nthe\nlog-config\nConfigMap onto a Pod called\nconfigmap-pod\n:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nconfigmap-pod\nspec\n:\ncontainers\n:\n-\nname\n:\ntest\nimage\n:\nbusybox:1.28\ncommand\n:\n[\n'sh'\n,\n'-c'\n,\n'echo \"The app is running!\" && tail -f /dev/null'\n]\nvolumeMounts\n:\n-\nname\n:\nconfig-vol\nmountPath\n:\n/etc/config\nvolumes\n:\n-\nname\n:\nconfig-vol\nconfigMap\n:\nname\n:\nlog-config\nitems\n:\n-\nkey\n:\nlog_level\npath\n:\nlog_level.conf\nThe\nlog-config\nConfigMap is mounted as a volume, and all contents stored in\nits\nlog_level\nentry are mounted into the Pod at path\n/etc/config/log_level.conf\n.\nNote that this path is derived from the volume's\nmountPath\nand the\npath\nkeyed with\nlog_level\n.\nNote:\nYou must\ncreate a ConfigMap\nbefore you can use it.\nA ConfigMap is always mounted as\nreadOnly\n.\nA container using a ConfigMap as a\nsubPath\nvolume mount will not\nreceive updates when the ConfigMap changes.\nText data is exposed as files using the UTF-8 character encoding.\nFor other character encodings, use\nbinaryData\n.\ndownwardAPI\nA\ndownwardAPI\nvolume makes\ndownward API\ndata available to applications. Within the volume, you can find the exposed\ndata as read-only files in plain text format.\nNote:\nA container using the downward API as a\nsubPath\nvolume mount does not\nreceive updates when field values change.\nSee\nExpose Pod Information to Containers Through Files\nto learn more.\nemptyDir\nFor a Pod that defines an\nemptyDir\nvolume, the volume is created when the Pod is assigned to a node.\nAs the name says, the\nemptyDir\nvolume is initially empty. All containers in the Pod can read and write the same\nfiles in the\nemptyDir\nvolume, though that volume can be mounted at the same\nor different paths in each container. When a Pod is removed from a node for\nany reason, the data in the\nemptyDir\nis deleted permanently.\nNote:\nA container crashing does\nnot\nremove a Pod from a node. The data in an\nemptyDir\nvolume\nis safe across container crashes.\nSome uses for an\nemptyDir\nare:\nscratch space, such as for a disk-based merge sort\ncheckpointing a long computation for recovery from crashes\nholding files that a content-manager container fetches while a webserver\ncontainer serves the data\nThe\nemptyDir.medium\nfield controls where\nemptyDir\nvolumes are stored. By\ndefault\nemptyDir\nvolumes are stored on whatever medium that backs the node\nsuch as disk, SSD, or network storage, depending on your environment. If you set\nthe\nemptyDir.medium\nfield to\n\"Memory\"\n, Kubernetes mounts a tmpfs (RAM-backed\nfilesystem) for you instead. While tmpfs is very fast, be aware that, unlike\ndisks, files you write count against the memory limit of the container that wrote them.\nA size limit can be specified for the default medium, which limits the capacity\nof the\nemptyDir\nvolume. The storage is allocated from\nnode ephemeral storage\n.\nIf that is filled up from another source (for example, log files or image overlays),\nthe\nemptyDir\nmay run out of capacity before this limit.\nIf no size is specified, memory-backed volumes are sized to node allocatable memory.\nCaution:\nPlease check\nhere\nfor points to note in terms of resource management when using memory-backed\nemptyDir\n.\nemptyDir configuration example\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\ntest-pd\nspec\n:\ncontainers\n:\n-\nimage\n:\nregistry.k8s.io/test-webserver\nname\n:\ntest-container\nvolumeMounts\n:\n-\nmountPath\n:\n/cache\nname\n:\ncache-volume\nvolumes\n:\n-\nname\n:\ncache-volume\nemptyDir\n:\nsizeLimit\n:\n500Mi\nemptyDir memory configuration example\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\ntest-pd\nspec\n:\ncontainers\n:\n-\nimage\n:\nregistry.k8s.io/test-webserver\nname\n:\ntest-container\nvolumeMounts\n:\n-\nmountPath\n:\n/cache\nname\n:\ncache-volume\nvolumes\n:\n-\nname\n:\ncache-volume\nemptyDir\n:\nsizeLimit\n:\n500Mi\nmedium\n:\nMemory\nfc (fibre channel)\nAn\nfc\nvolume type allows an existing fibre channel block storage volume\nto be mounted in a Pod. You can specify single or multiple target world wide names (WWNs)\nusing the parameter\ntargetWWNs\nin your Volume configuration. If multiple WWNs are specified,\ntargetWWNs expect that those WWNs are from multi-path connections.\nNote:\nYou must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs\nbeforehand so that Kubernetes hosts can access them.\ngcePersistentDisk (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree\ngcePersistentDisk\ntype\nare redirected to the\npd.csi.storage.gke.io\nCSI\ndriver.\nThe\ngcePersistentDisk\nin-tree storage driver was deprecated in the Kubernetes v1.17 release\nand then removed entirely in the v1.28 release.\nThe Kubernetes project suggests that you use the\nGoogle Compute Engine Persistent Disk CSI\nthird party storage driver instead.\ngitRepo (deprecated)\nWarning:\nThe\ngitRepo\nvolume plugin is deprecated and is disabled by default.\nTo provision a Pod that has a Git repository mounted, you can mount an\nemptyDir\nvolume into an\ninit container\nthat clones the repo using Git, then mount the\nEmptyDir\ninto the Pod's container.\nYou can restrict the use of\ngitRepo\nvolumes in your cluster using\npolicies\n, such as\nValidatingAdmissionPolicy\n.\nYou can use the following Common Expression Language (CEL) expression as\npart of a policy to reject use of\ngitRepo\nvolumes:\n!has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))\nYou can use this deprecated storage plugin in your cluster if you explicitly\nenable the\nGitRepoVolumeDriver\nfeature gate\n.\nA\ngitRepo\nvolume is an example of a volume plugin. This plugin\nmounts an empty directory and clones a git repository into this directory\nfor your Pod to use.\nHere is an example of a\ngitRepo\nvolume:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nserver\nspec\n:\ncontainers\n:\n-\nimage\n:\nnginx\nname\n:\nnginx\nvolumeMounts\n:\n-\nmountPath\n:\n/mypath\nname\n:\ngit-volume\nvolumes\n:\n-\nname\n:\ngit-volume\ngitRepo\n:\nrepository\n:\n\"git@somewhere:me/my-git-repository.git\"\nrevision\n:\n\"22f1d8406d464b0c0874075539c1f2e96c253775\"\nglusterfs (removed)\nKubernetes 1.34 does not include a\nglusterfs\nvolume type.\nThe GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release\nand then removed entirely in the v1.26 release.\nhostPath\nA\nhostPath\nvolume mounts a file or directory from the host node's filesystem\ninto your Pod. This is not something that most Pods will need, but it offers a\npowerful escape hatch for some applications.\nWarning:\nUsing the\nhostPath\nvolume type presents many security risks.\nIf you can avoid using a\nhostPath\nvolume, you should. For example,\ndefine a\nlocal\nPersistentVolume\n, and use that instead.\nIf you are restricting access to specific directories on the node using\nadmission-time validation, that restriction is only effective when you\nadditionally require that any mounts of that\nhostPath\nvolume are\nread only\n. If you allow a read-write mount of any host path by an\nuntrusted Pod, the containers in that Pod may be able to subvert the\nread-write host mount.\nTake care when using\nhostPath\nvolumes, whether these are mounted as read-only\nor as read-write, because:\nAccess to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs\n(such as the container runtime socket) that can be used for container escape or to attack other\nparts of the cluster.\nPods with identical configuration (such as created from a PodTemplate) may\nbehave differently on different nodes due to different files on the nodes.\nhostPath\nvolume usage is not treated as ephemeral storage usage.\nYou need to monitor the disk usage by yourself because excessive\nhostPath\ndisk\nusage will lead to disk pressure on the node.\nSome uses for a\nhostPath\nare:\nrunning a container that needs access to node-level system components\n(such as a container that transfers system logs to a central location,\naccessing those logs using a read-only mount of\n/var/log\n)\nmaking a configuration file stored on the host system available read-only\nto a\nstatic pod\n;\nunlike normal Pods, static Pods cannot access ConfigMaps\nhostPath\nvolume types\nIn addition to the required\npath\nproperty, you can optionally specify a\ntype\nfor a\nhostPath\nvolume.\nThe available values for\ntype\nare:\nValue\nBehavior\n\"\"\nEmpty string (default) is for backward compatibility, which means that no checks will be performed before mounting the\nhostPath\nvolume.\nDirectoryOrCreate\nIf nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.\nDirectory\nA directory must exist at the given path\nFileOrCreate\nIf nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.\nFile\nA file must exist at the given path\nSocket\nA UNIX socket must exist at the given path\nCharDevice\n(Linux nodes only)\nA character device must exist at the given path\nBlockDevice\n(Linux nodes only)\nA block device must exist at the given path\nCaution:\nThe\nFileOrCreate\nmode does\nnot\ncreate the parent directory of the file. If the parent directory\nof the mounted file does not exist, the pod fails to start. To ensure that this mode works,\nyou can try to mount directories and files separately, as shown in the\nFileOrCreate\nexample\nfor\nhostPath\n.\nSome files or directories created on the underlying hosts might only be\naccessible by root. You then either need to run your process as root in a\nprivileged container\nor modify the file permissions on the host to read from or write to a\nhostPath\nvolume.\nhostPath configuration example\nLinux node\nWindows node\n---\n# This manifest mounts /data/foo on the host as /foo inside the\n# single container that runs within the hostpath-example-linux Pod.\n#\n# The mount into the container is read-only.\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nhostpath-example-linux\nspec\n:\nos\n:\n{\nname\n:\nlinux }\nnodeSelector\n:\nkubernetes.io/os\n:\nlinux\ncontainers\n:\n-\nname\n:\nexample-container\nimage\n:\nregistry.k8s.io/test-webserver\nvolumeMounts\n:\n-\nmountPath\n:\n/foo\nname\n:\nexample-volume\nreadOnly\n:\ntrue\nvolumes\n:\n-\nname\n:\nexample-volume\n# mount /data/foo, but only if that directory already exists\nhostPath\n:\npath\n:\n/data/foo\n# directory location on host\ntype\n:\nDirectory\n# this field is optional\n---\n# This manifest mounts C:\\Data\\foo on the host as C:\\foo, inside the\n# single container that runs within the hostpath-example-windows Pod.\n#\n# The mount into the container is read-only.\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nhostpath-example-windows\nspec\n:\nos\n:\n{\nname\n:\nwindows }\nnodeSelector\n:\nkubernetes.io/os\n:\nwindows\ncontainers\n:\n-\nname\n:\nexample-container\nimage\n:\nmicrosoft/windowsservercore:1709\nvolumeMounts\n:\n-\nname\n:\nexample-volume\nmountPath\n:\n\"C:\\\\foo\"\nreadOnly\n:\ntrue\nvolumes\n:\n# mount C:\\Data\\foo from the host, but only if that directory already exists\n-\nname\n:\nexample-volume\nhostPath\n:\npath\n:\n\"C:\\\\Data\\\\foo\"\n# directory location on host\ntype\n:\nDirectory\n# this field is optional\nhostPath FileOrCreate configuration example\nThe following manifest defines a Pod that mounts\n/var/local/aaa\ninside the single container in the Pod. If the node does not\nalready have a path\n/var/local/aaa\n, the kubelet creates\nit as a directory and then mounts it into the Pod.\nIf\n/var/local/aaa\nalready exists but is not a directory,\nthe Pod fails. Additionally, the kubelet attempts to make\na file named\n/var/local/aaa/1.txt\ninside that directory\n(as seen from the host); if something already exists at\nthat path and isn't a regular file, the Pod fails.\nHere's the example manifest:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\ntest-webserver\nspec\n:\nos\n:\n{\nname\n:\nlinux }\nnodeSelector\n:\nkubernetes.io/os\n:\nlinux\ncontainers\n:\n-\nname\n:\ntest-webserver\nimage\n:\nregistry.k8s.io/test-webserver:latest\nvolumeMounts\n:\n-\nmountPath\n:\n/var/local/aaa\nname\n:\nmydir\n-\nmountPath\n:\n/var/local/aaa/1.txt\nname\n:\nmyfile\nvolumes\n:\n-\nname\n:\nmydir\nhostPath\n:\n# Ensure the file directory is created.\npath\n:\n/var/local/aaa\ntype\n:\nDirectoryOrCreate\n-\nname\n:\nmyfile\nhostPath\n:\npath\n:\n/var/local/aaa/1.txt\ntype\n:\nFileOrCreate\nimage\nFEATURE STATE:\nKubernetes v1.33 [beta]\n(disabled by default)\nAn\nimage\nvolume source represents an OCI object (a container image or\nartifact) which is available on the kubelet's host machine.\nAn example of using the\nimage\nvolume source is:\npods/image-volumes.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nimage-volume\nspec\n:\ncontainers\n:\n-\nname\n:\nshell\ncommand\n:\n[\n\"sleep\"\n,\n\"infinity\"\n]\nimage\n:\ndebian\nvolumeMounts\n:\n-\nname\n:\nvolume\nmountPath\n:\n/volume\nvolumes\n:\n-\nname\n:\nvolume\nimage\n:\nreference\n:\nquay.io/crio/artifact:v2\npullPolicy\n:\nIfNotPresent\nThe volume is resolved at pod startup depending on which\npullPolicy\nvalue is\nprovided:\nAlways\nthe kubelet always attempts to pull the reference. If the pull fails,\nthe kubelet sets the Pod to\nFailed\n.\nNever\nthe kubelet never pulls the reference and only uses a local image or artifact.\nThe Pod becomes\nFailed\nif any layers of the image aren't already present locally,\nor if the manifest for that image isn't already cached.\nIfNotPresent\nthe kubelet pulls if the reference isn't already present on disk. The Pod becomes\nFailed\nif the reference isn't present and the pull fails.\nThe volume gets re-resolved if the pod gets deleted and recreated, which means\nthat new remote content will become available on pod recreation. A failure to\nresolve or pull the image during pod startup will block containers from starting\nand may add significant latency. Failures will be retried using normal volume\nbackoff and will be reported on the pod reason and message.\nThe types of objects that may be mounted by this volume are defined by the\ncontainer runtime implementation on a host machine. At a minimum, they must include\nall valid types supported by the container image field. The OCI object gets\nmounted in a single directory (\nspec.containers[*].volumeMounts.mountPath\n)\nand will be mounted read-only. On Linux, the container runtime typically also mounts the\nvolume with file execution blocked (\nnoexec\n).\nBesides that:\nsubPath\nor\nsubPathExpr\nmounts for containers (\nspec.containers[*].volumeMounts.[subPath,subPathExpr]\n)\nare only supported from Kubernetes v1.33.\nThe field\nspec.securityContext.fsGroupChangePolicy\nhas no effect on this\nvolume type.\nThe\nAlwaysPullImages\nAdmission Controller\ndoes also work for this volume source like for container images.\nThe following fields are available for the\nimage\ntype:\nreference\nArtifact reference to be used. For example, you could specify\nregistry.k8s.io/conformance:v1.34.0\nto load the\nfiles from the Kubernetes conformance test image. Behaves in the same way as\npod.spec.containers[*].image\n. Pull secrets will be assembled in the same way\nas for the container image by looking up node credentials, service account image\npull secrets, and pod spec image pull secrets. This field is optional to allow\nhigher level config management to default or override container images in\nworkload controllers like Deployments and StatefulSets.\nMore info about container images\npullPolicy\nPolicy for pulling OCI objects. Possible values are:\nAlways\n,\nNever\nor\nIfNotPresent\n. Defaults to\nAlways\nif\n:latest\ntag is specified, or\nIfNotPresent\notherwise.\nSee the\nUse an Image Volume With a Pod\nexample for more details on how to use the volume source.\niscsi\nAn\niscsi\nvolume allows an existing iSCSI (SCSI over IP) volume to be mounted\ninto your Pod. Unlike\nemptyDir\n, which is erased when a Pod is removed, the\ncontents of an\niscsi\nvolume are preserved and the volume is merely\nunmounted. This means that an iscsi volume can be pre-populated with data, and\nthat data can be shared between pods.\nNote:\nYou must have your own iSCSI server running with the volume created before you can use it.\nA feature of iSCSI is that it can be mounted as read-only by multiple consumers\nsimultaneously. This means that you can pre-populate a volume with your dataset\nand then serve it in parallel from as many Pods as you need. Unfortunately,\niSCSI volumes can only be mounted by a single consumer in read-write mode.\nSimultaneous writers are not allowed.\nlocal\nA\nlocal\nvolume represents a mounted local storage device such as a disk,\npartition or directory.\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic\nprovisioning is not supported.\nCompared to\nhostPath\nvolumes,\nlocal\nvolumes are used in a durable and\nportable manner without manually scheduling pods to nodes. The system is aware\nof the volume's node constraints by looking at the node affinity on the PersistentVolume.\nHowever,\nlocal\nvolumes are subject to the availability of the underlying\nnode and are not suitable for all applications. If a node becomes unhealthy,\nthen the\nlocal\nvolume becomes inaccessible to the pod. The pod using this volume\nis unable to run. Applications using\nlocal\nvolumes must be able to tolerate this\nreduced availability, as well as potential data loss, depending on the\ndurability characteristics of the underlying disk.\nThe following example shows a PersistentVolume using a\nlocal\nvolume and\nnodeAffinity\n:\napiVersion\n:\nv1\nkind\n:\nPersistentVolume\nmetadata\n:\nname\n:\nexample-pv\nspec\n:\ncapacity\n:\nstorage\n:\n100Gi\nvolumeMode\n:\nFilesystem\naccessModes\n:\n- ReadWriteOnce\npersistentVolumeReclaimPolicy\n:\nDelete\nstorageClassName\n:\nlocal-storage\nlocal\n:\npath\n:\n/mnt/disks/ssd1\nnodeAffinity\n:\nrequired\n:\nnodeSelectorTerms\n:\n-\nmatchExpressions\n:\n-\nkey\n:\nkubernetes.io/hostname\noperator\n:\nIn\nvalues\n:\n- example-node\nYou must set a PersistentVolume\nnodeAffinity\nwhen using\nlocal\nvolumes.\nThe Kubernetes scheduler uses the PersistentVolume\nnodeAffinity\nto schedule\nthese Pods to the correct node.\nPersistentVolume\nvolumeMode\ncan be set to \"Block\" (instead of the default\nvalue \"Filesystem\") to expose the local volume as a raw block device.\nWhen using local volumes, it is recommended to create a StorageClass with\nvolumeBindingMode\nset to\nWaitForFirstConsumer\n. For more details, see the\nlocal\nStorageClass\nexample.\nDelaying volume binding ensures that the PersistentVolumeClaim binding decision\nwill also be evaluated with any other node constraints the Pod may have,\nsuch as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.\nAn external static provisioner can be run separately for improved management of\nthe local volume lifecycle. Note that this provisioner does not support dynamic\nprovisioning yet. For an example on how to run an external local provisioner, see the\nlocal volume provisioner user guide\n.\nNote:\nThe local PersistentVolume requires manual cleanup and deletion by the\nuser if the external static provisioner is not used to manage the volume\nlifecycle.\nnfs\nAn\nnfs\nvolume allows an existing NFS (Network File System) share to be\nmounted into a Pod. Unlike\nemptyDir\n, which is erased when a Pod is\nremoved, the contents of an\nnfs\nvolume are preserved and the volume is merely\nunmounted. This means that an NFS volume can be pre-populated with data, and\nthat data can be shared between pods. NFS can be mounted by multiple\nwriters simultaneously.\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\ntest-pd\nspec\n:\ncontainers\n:\n-\nimage\n:\nregistry.k8s.io/test-webserver\nname\n:\ntest-container\nvolumeMounts\n:\n-\nmountPath\n:\n/my-nfs-data\nname\n:\ntest-volume\nvolumes\n:\n-\nname\n:\ntest-volume\nnfs\n:\nserver\n:\nmy-nfs-server.example.com\npath\n:\n/my-nfs-volume\nreadOnly\n:\ntrue\nNote:\nYou must have your own NFS server running with the share exported before you can use it.\nAlso note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or\nuse\n/etc/nfsmount.conf\n.\nYou can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.\npersistentVolumeClaim\nA\npersistentVolumeClaim\nvolume is used to mount a\nPersistentVolume\ninto a Pod. PersistentVolumeClaims\nare a way for users to \"claim\" durable storage (such as an iSCSI volume)\nwithout knowing the details of the particular cloud environment.\nSee the information about\nPersistentVolumes\nfor more\ndetails.\nportworxVolume (deprecated)\nFEATURE STATE:\nKubernetes v1.25 [deprecated]\nA\nportworxVolume\nis an elastic block storage layer that runs hyperconverged with\nKubernetes.\nPortworx\nfingerprints storage\nin a server, tiers based on capabilities, and aggregates capacity across multiple servers.\nPortworx runs in-guest in virtual machines or on bare metal Linux nodes.\nA\nportworxVolume\ncan be dynamically created through Kubernetes or it can also\nbe pre-provisioned and referenced inside a Pod.\nHere is an example Pod referencing a pre-provisioned Portworx volume:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\ntest-portworx-volume-pod\nspec\n:\ncontainers\n:\n-\nimage\n:\nregistry.k8s.io/test-webserver\nname\n:\ntest-container\nvolumeMounts\n:\n-\nmountPath\n:\n/mnt\nname\n:\npxvol\nvolumes\n:\n-\nname\n:\npxvol\n# This Portworx volume must already exist.\nportworxVolume\n:\nvolumeID\n:\n\"pxvol\"\nfsType\n:\n\"<fs-type>\"\nNote:\nMake sure you have an existing PortworxVolume with name\npxvol\nbefore using it in the Pod.\nPortworx CSI migration\nFEATURE STATE:\nKubernetes v1.33 [stable]\n(enabled by default)\nIn Kubernetes 1.34, all operations for the in-tree\nPortworx volumes are redirected to the\npxd.portworx.com\nContainer Storage Interface (CSI) Driver by default.\nPortworx CSI Driver\nmust be installed on the cluster.\nprojected\nA projected volume maps several existing volume sources into the same\ndirectory. For more details, see\nprojected volumes\n.\nrbd (removed)\nKubernetes 1.34 does not include a\nrbd\nvolume type.\nThe\nRados Block Device\n(RBD) in-tree storage driver\nand its csi migration support were deprecated in the Kubernetes v1.28 release\nand then removed entirely in the v1.31 release.\nsecret\nA\nsecret\nvolume is used to pass sensitive information, such as passwords, to\nPods. You can store secrets in the Kubernetes API and mount them as files for\nuse by pods without coupling to Kubernetes directly.\nsecret\nvolumes are\nbacked by tmpfs (a RAM-backed filesystem) so they are never written to\nnon-volatile storage.\nNote:\nYou must create a Secret in the Kubernetes API before you can use it.\nA Secret is always mounted as\nreadOnly\n.\nA container using a Secret as a\nsubPath\nvolume mount will not\nreceive Secret updates.\nFor more details, see\nConfiguring Secrets\n.\nvsphereVolume (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree\nvsphereVolume\ntype\nare redirected to the\ncsi.vsphere.vmware.com\nCSI\ndriver.\nThe\nvsphereVolume\nin-tree storage driver was deprecated in the Kubernetes v1.19 release\nand then removed entirely in the v1.30 release.\nThe Kubernetes project suggests that you use the\nvSphere CSI\nthird party storage driver instead.\nUsing subPath\nSometimes, it is useful to share one volume for multiple uses in a single pod.\nThe\nvolumeMounts[*].subPath\nproperty specifies a sub-path inside the referenced volume\ninstead of its root.\nThe following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP)\nusing a single, shared volume. This sample\nsubPath\nconfiguration is not recommended\nfor production use.\nThe PHP application's code and assets map to the volume's\nhtml\nfolder and\nthe MySQL database is stored in the volume's\nmysql\nfolder. For example:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nmy-lamp-site\nspec\n:\ncontainers\n:\n-\nname\n:\nmysql\nimage\n:\nmysql\nenv\n:\n-\nname\n:\nMYSQL_ROOT_PASSWORD\nvalue\n:\n\"rootpasswd\"\nvolumeMounts\n:\n-\nmountPath\n:\n/var/lib/mysql\nname\n:\nsite-data\nsubPath\n:\nmysql\n-\nname\n:\nphp\nimage\n:\nphp:7.0-apache\nvolumeMounts\n:\n-\nmountPath\n:\n/var/www/html\nname\n:\nsite-data\nsubPath\n:\nhtml\nvolumes\n:\n-\nname\n:\nsite-data\npersistentVolumeClaim\n:\nclaimName\n:\nmy-lamp-site-data\nUsing subPath with expanded environment variables\nFEATURE STATE:\nKubernetes v1.17 [stable]\nUse the\nsubPathExpr\nfield to construct\nsubPath\ndirectory names from\ndownward API environment variables.\nThe\nsubPath\nand\nsubPathExpr\nproperties are mutually exclusive.\nIn this example, a\nPod\nuses\nsubPathExpr\nto create a directory\npod1\nwithin\nthe\nhostPath\nvolume\n/var/log/pods\n.\nThe\nhostPath\nvolume takes the\nPod\nname from the\ndownwardAPI\n.\nThe host directory\n/var/log/pods/pod1\nis mounted at\n/logs\nin the container.\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\npod1\nspec\n:\ncontainers\n:\n-\nname\n:\ncontainer1\nenv\n:\n-\nname\n:\nPOD_NAME\nvalueFrom\n:\nfieldRef\n:\napiVersion\n:\nv1\nfieldPath\n:\nmetadata.name\nimage\n:\nbusybox:1.28\ncommand\n:\n[\n\"sh\"\n,\n\"-c\"\n,\n\"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt\"\n]\nvolumeMounts\n:\n-\nname\n:\nworkdir1\nmountPath\n:\n/logs\n# The variable expansion uses round brackets (not curly brackets).\nsubPathExpr\n:\n$(POD_NAME)\nrestartPolicy\n:\nNever\nvolumes\n:\n-\nname\n:\nworkdir1\nhostPath\n:\npath\n:\n/var/log/pods\nResources\nThe storage medium (such as Disk or SSD) of an\nemptyDir\nvolume is determined by the\nmedium of the filesystem holding the kubelet root dir (typically\n/var/lib/kubelet\n). There is no limit on how much space an\nemptyDir\nor\nhostPath\nvolume can consume, and no isolation between containers or\npods.\nTo learn about requesting space using a resource specification, see\nhow to manage resources\n.\nOut-of-tree volume plugins\nThe out-of-tree volume plugins include\nContainer Storage Interface\n(CSI), and also\nFlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage plugins\nwithout adding their plugin source code to the Kubernetes repository.\nPreviously, all volume plugins were \"in-tree\". The \"in-tree\" plugins were built, linked, compiled,\nand shipped with the core Kubernetes binaries. This meant that adding a new storage system to\nKubernetes (a volume plugin) required checking code into the core Kubernetes code repository.\nBoth CSI and FlexVolume allow volume plugins to be developed independently of\nthe Kubernetes code base, and deployed (installed) on Kubernetes clusters as\nextensions.\nFor storage vendors looking to create an out-of-tree volume plugin, please refer\nto the\nvolume plugin FAQ\n.\ncsi\nContainer Storage Interface\n(CSI) defines a standard interface for container orchestration systems (like\nKubernetes) to expose arbitrary storage systems to their container workloads.\nPlease read the\nCSI design proposal\nfor more information.\nNote:\nSupport for CSI spec versions 0.2 and 0.3 is deprecated in Kubernetes\nv1.13 and will be removed in a future release.\nNote:\nCSI drivers may not be compatible across all Kubernetes releases.\nPlease check the specific CSI driver's documentation for supported\ndeployments steps for each Kubernetes release and a compatibility matrix.\nOnce a CSI-compatible volume driver is deployed on a Kubernetes cluster, users\nmay use the\ncsi\nvolume type to attach or mount the volumes exposed by the\nCSI driver.\nA\ncsi\nvolume can be used in a Pod in three different ways:\nthrough a reference to a\nPersistentVolumeClaim\nwith a\ngeneric ephemeral volume\nwith a\nCSI ephemeral volume\nif the driver supports that\nThe following fields are available to storage administrators to configure a CSI\npersistent volume:\ndriver\n: A string value that specifies the name of the volume driver to use.\nThis value must correspond to the value returned in the\nGetPluginInfoResponse\nby the CSI driver as defined in the\nCSI spec\n.\nIt is used by Kubernetes to identify which CSI driver to call out to, and by\nCSI driver components to identify which PV objects belong to the CSI driver.\nvolumeHandle\n: A string value that uniquely identifies the volume. This value\nmust correspond to the value returned in the\nvolume.id\nfield of the\nCreateVolumeResponse\nby the CSI driver as defined in the\nCSI spec\n.\nThe value is passed as\nvolume_id\nin all calls to the CSI volume driver when\nreferencing the volume.\nreadOnly\n: An optional boolean value indicating whether the volume is to be\n\"ControllerPublished\" (attached) as read only. Default is false. This value is passed\nto the CSI driver via the\nreadonly\nfield in the\nControllerPublishVolumeRequest\n.\nfsType\n: If the PV's\nVolumeMode\nis\nFilesystem\n, then this field may be used\nto specify the filesystem that should be used to mount the volume. If the\nvolume has not been formatted and formatting is supported, this value will be\nused to format the volume.\nThis value is passed to the CSI driver via the\nVolumeCapability\nfield of\nControllerPublishVolumeRequest\n,\nNodeStageVolumeRequest\n, and\nNodePublishVolumeRequest\n.\nvolumeAttributes\n: A map of string to string that specifies static properties\nof a volume. This map must correspond to the map returned in the\nvolume.attributes\nfield of the\nCreateVolumeResponse\nby the CSI driver as\ndefined in the\nCSI spec\n.\nThe map is passed to the CSI driver via the\nvolume_context\nfield in the\nControllerPublishVolumeRequest\n,\nNodeStageVolumeRequest\n, and\nNodePublishVolumeRequest\n.\ncontrollerPublishSecretRef\n: A reference to the secret object containing\nsensitive information to pass to the CSI driver to complete the CSI\nControllerPublishVolume\nand\nControllerUnpublishVolume\ncalls. This field is\noptional, and may be empty if no secret is required. If the Secret\ncontains more than one secret, all secrets are passed.\nnodeExpandSecretRef\n: A reference to the secret containing sensitive\ninformation to pass to the CSI driver to complete the CSI\nNodeExpandVolume\ncall. This field is optional and may be empty if no\nsecret is required. If the object contains more than one secret, all\nsecrets are passed. When you have configured secret data for node-initiated\nvolume expansion, the kubelet passes that data via the\nNodeExpandVolume()\ncall to the CSI driver. All supported versions of Kubernetes offer the\nnodeExpandSecretRef\nfield, and have it available by default. Kubernetes releases\nprior to v1.25 did not include this support.\nEnable the\nfeature gate\nnamed\nCSINodeExpandSecret\nfor each kube-apiserver and for the kubelet on every\nnode. Since Kubernetes version 1.27, this feature has been enabled by default\nand no explicit enablement of the feature gate is required.\nYou must also be using a CSI driver that supports or requires secret data during\nnode-initiated storage resize operations.\nnodePublishSecretRef\n: A reference to the secret object containing\nsensitive information to pass to the CSI driver to complete the CSI\nNodePublishVolume\ncall. This field is optional and may be empty if no\nsecret is required. If the secret object contains more than one secret, all\nsecrets are passed.\nnodeStageSecretRef\n: A reference to the secret object containing\nsensitive information to pass to the CSI driver to complete the CSI\nNodeStageVolume\ncall. This field is optional and may be empty if no secret\nis required. If the Secret contains more than one secret, all secrets\nare passed.\nCSI raw block volume support\nFEATURE STATE:\nKubernetes v1.18 [stable]\nVendors with external CSI drivers can implement raw block volume support\nin Kubernetes workloads.\nYou can set up your\nPersistentVolume/PersistentVolumeClaim with raw block volume support\nas usual, without any CSI-specific changes.\nCSI ephemeral volumes\nFEATURE STATE:\nKubernetes v1.25 [stable]\nYou can directly configure CSI volumes within the Pod\nspecification. Volumes specified in this way are ephemeral and do not\npersist across pod restarts. See\nEphemeral Volumes\nfor more information.\nFor more information on how to develop a CSI driver, refer to the\nkubernetes-csi documentation\nWindows CSI proxy\nFEATURE STATE:\nKubernetes v1.22 [stable]\nCSI node plugins need to perform various privileged\noperations like scanning of disk devices and mounting of file systems. These operations\ndiffer for each host operating system. For Linux worker nodes, containerized CSI node\nplugins are typically deployed as privileged containers. For Windows worker nodes,\nprivileged operations for containerized CSI node plugins is supported using\ncsi-proxy\n, a community-managed,\nstand-alone binary that needs to be pre-installed on each Windows node.\nFor more details, refer to the deployment guide of the CSI plugin you wish to deploy.\nMigrating to CSI drivers from in-tree plugins\nFEATURE STATE:\nKubernetes v1.25 [stable]\nThe\nCSIMigration\nfeature directs operations against existing in-tree\nplugins to corresponding CSI plugins (which are expected to be installed and configured).\nAs a result, operators do not have to make any\nconfiguration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims\n(referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.\nNote:\nExisting PVs created by an in-tree volume plugin can still be used in the future without any configuration\nchanges, even after the migration to CSI is completed for that volume type, and even after you upgrade to a\nversion of Kubernetes that doesn't have compiled-in support for that kind of storage.\nAs part of that migration, you - or another cluster administrator -\nmust\nhave installed and configured\nthe appropriate CSI driver for that storage. The core of Kubernetes does not install that software for you.\nAfter that migration, you can also define new PVCs and PVs that refer to the legacy, built-in\nstorage integrations.\nProvided you have the appropriate CSI driver installed and configured, the PV creation continues\nto work, even for brand new volumes. The actual storage management now happens through\nthe CSI driver.\nThe operations and features that are supported include:\nprovisioning/delete, attach/detach, mount/unmount and resizing of volumes.\nIn-tree plugins that support\nCSIMigration\nand have a corresponding CSI driver implemented\nare listed in\nTypes of Volumes\n.\nflexVolume (deprecated)\nFEATURE STATE:\nKubernetes v1.23 [deprecated]\nFlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface\nwith storage drivers. The FlexVolume driver binaries must be installed in a pre-defined\nvolume plugin path on each node and in some cases the control plane nodes as well.\nPods interact with FlexVolume drivers through the\nflexVolume\nin-tree volume plugin.\nThe following FlexVolume\nplugins\n,\ndeployed as PowerShell scripts on the host, support Windows nodes:\nSMB\niSCSI\nNote:\nFlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.\nMaintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI.\nUsers of FlexVolume should move their workloads to use the equivalent CSI Driver.\nMount propagation\nCaution:\nMount propagation is a low-level feature that does not work consistently on all\nvolume types. The Kubernetes project recommends only using mount propagation with\nhostPath\nor memory-backed\nemptyDir\nvolumes. See\nKubernetes issue #95049\nfor more context.\nMount propagation allows for sharing volumes mounted by a container to\nother containers in the same pod, or even to other pods on the same node.\nMount propagation of a volume is controlled by the\nmountPropagation\nfield\nin\ncontainers[*].volumeMounts\n. Its values are:\nNone\n- This volume mount will not receive any subsequent mounts\nthat are mounted to this volume or any of its subdirectories by the host.\nIn similar fashion, no mounts created by the container will be visible on\nthe host. This is the default mode.\nThis mode is equal to\nrprivate\nmount propagation as described in\nmount(8)\nHowever, the CRI runtime may choose\nrslave\nmount propagation (i.e.,\nHostToContainer\n) instead, when\nrprivate\npropagation is not applicable.\ncri-dockerd (Docker) is known to choose\nrslave\nmount propagation when the\nmount source contains the Docker daemon's root directory (\n/var/lib/docker\n).\nHostToContainer\n- This volume mount will receive all subsequent mounts\nthat are mounted to this volume or any of its subdirectories.\nIn other words, if the host mounts anything inside the volume mount, the\ncontainer will see it mounted there.\nSimilarly, if any Pod with\nBidirectional\nmount propagation to the same\nvolume mounts anything there, the container with\nHostToContainer\nmount\npropagation will see it.\nThis mode is equal to\nrslave\nmount propagation as described in the\nmount(8)\nBidirectional\n- This volume mount behaves the same the\nHostToContainer\nmount.\nIn addition, all volume mounts created by the container will be propagated\nback to the host and to all containers of all pods that use the same volume.\nA typical use case for this mode is a Pod with a FlexVolume or CSI driver or\na Pod that needs to mount something on the host using a\nhostPath\nvolume.\nThis mode is equal to\nrshared\nmount propagation as described in the\nmount(8)\nWarning:\nBidirectional\nmount propagation can be dangerous. It can damage\nthe host operating system and therefore it is allowed only in privileged\ncontainers. Familiarity with Linux kernel behavior is strongly recommended.\nIn addition, any volume mounts created by containers in pods must be destroyed\n(unmounted) by the containers on termination.\nRead-only mounts\nA mount can be made read-only by setting the\n.spec.containers[].volumeMounts[].readOnly\nfield to\ntrue\n.\nThis does not make the volume itself read-only, but that specific container will\nnot be able to write to it.\nOther containers in the Pod may mount the same volume as read-write.\nOn Linux, read-only mounts are not recursively read-only by default.\nFor example, consider a Pod which mounts the hosts\n/mnt\nas a\nhostPath\nvolume. If\nthere is another filesystem mounted read-write on\n/mnt/<SUBMOUNT>\n(such as tmpfs,\nNFS, or USB storage), the volume mounted into the container(s) will also have a writeable\n/mnt/<SUBMOUNT>\n, even if the mount itself was specified as read-only.\nRecursive read-only mounts\nFEATURE STATE:\nKubernetes v1.33 [stable]\n(enabled by default)\nRecursive read-only mounts can be enabled by setting the\nRecursiveReadOnlyMounts\nfeature gate\nfor kubelet and kube-apiserver, and setting the\n.spec.containers[].volumeMounts[].recursiveReadOnly\nfield for a pod.\nThe allowed values are:\nDisabled\n(default): no effect.\nEnabled\n: makes the mount recursively read-only.\nNeeds all the following requirements to be satisfied:\nreadOnly\nis set to\ntrue\nmountPropagation\nis unset, or, set to\nNone\nThe host is running with Linux kernel v5.12 or later\nThe\nCRI-level\ncontainer runtime supports recursive read-only mounts\nThe OCI-level container runtime supports recursive read-only mounts.\nIt will fail if any of these is not true.\nIfPossible\n: attempts to apply\nEnabled\n, and falls back to\nDisabled\nif the feature is not supported by the kernel or the runtime class.\nExample:\nstorage/rro.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nrro\nspec\n:\nvolumes\n:\n-\nname\n:\nmnt\nhostPath\n:\n# tmpfs is mounted on /mnt/tmpfs\npath\n:\n/mnt\ncontainers\n:\n-\nname\n:\nbusybox\nimage\n:\nbusybox\nargs\n:\n[\n\"sleep\"\n,\n\"infinity\"\n]\nvolumeMounts\n:\n# /mnt-rro/tmpfs is not writable\n-\nname\n:\nmnt\nmountPath\n:\n/mnt-rro\nreadOnly\n:\ntrue\nmountPropagation\n:\nNone\nrecursiveReadOnly\n:\nEnabled\n# /mnt-ro/tmpfs is writable\n-\nname\n:\nmnt\nmountPath\n:\n/mnt-ro\nreadOnly\n:\ntrue\n# /mnt-rw/tmpfs is writable\n-\nname\n:\nmnt\nmountPath\n:\n/mnt-rw\nWhen this property is recognized by kubelet and kube-apiserver,\nthe\n.status.containerStatuses[].volumeMounts[].recursiveReadOnly\nfield is set to either\nEnabled\nor\nDisabled\n.\nImplementations\nNote:\nThis section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the\ncontent guide\nbefore submitting a change.\nMore information.\nThe following container runtimes are known to support recursive read-only mounts.\nCRI-level:\ncontainerd\n, since v2.0\nCRI-O\n, since v1.30\nOCI-level:\nrunc\n, since v1.1\ncrun\n, since v1.8.6\nWhat's next\nFollow an example of\ndeploying WordPress and MySQL with Persistent Volumes\n.\nItems on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the\nCNCF website guidelines\nfor more details.\nYou should read the\ncontent guide\nbefore proposing a change that adds an extra third-party link.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 17, 2025 at 3:46 AM PST:\nRemove Portworx volume example as per KEP 1326 (9fbaa42877)\nVolume API reference\nEdit this page\nThird party content advice\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/storage/volumes/"}}
{"text": "Install Tools | Kubernetes\nInstall Tools\nSet up Kubernetes tools on your computer.\nkubectl\nThe Kubernetes command-line tool,\nkubectl\n, allows\nyou to run commands against Kubernetes clusters.\nYou can use kubectl to deploy applications, inspect and manage cluster resources,\nand view logs. For more information including a complete list of kubectl operations, see the\nkubectl\nreference documentation\n.\nkubectl is installable on a variety of Linux platforms, macOS and Windows.\nFind your preferred operating system below.\nInstall kubectl on Linux\nInstall kubectl on macOS\nInstall kubectl on Windows\nkind\nkind\nlets you run Kubernetes on\nyour local computer. This tool requires that you have either\nDocker\nor\nPodman\ninstalled.\nThe kind\nQuick Start\npage\nshows you what you need to do to get up and running with kind.\nView kind Quick Start Guide\nminikube\nLike\nkind\n,\nminikube\nis a tool that lets you run Kubernetes\nlocally.\nminikube\nruns an all-in-one or a multi-node local Kubernetes cluster on your personal\ncomputer (including Windows, macOS and Linux PCs) so that you can try out\nKubernetes, or for daily development work.\nYou can follow the official\nGet Started!\nguide if your focus is\non getting the tool installed.\nView minikube Get Started! Guide\nOnce you have\nminikube\nworking, you can use it to\nrun a sample application\n.\nkubeadm\nYou can use the\nkubeadm\ntool to create and manage Kubernetes clusters.\nIt performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.\nInstalling kubeadm\nshows you how to install kubeadm.\nOnce installed, you can use it to\ncreate a cluster\n.\nView kubeadm Install Guide\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified July 12, 2023 at 1:25 AM PST:\nRevise docs home page (9520b96a61)\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/tasks/tools/"}}
{"text": "Access Applications in a Cluster | Kubernetes\nAccess Applications in a Cluster\nConfigure load balancing, port forwarding, or setup firewall or DNS configurations to access applications in a cluster.\nDeploy and Access the Kubernetes Dashboard\nDeploy the web UI (Kubernetes Dashboard) and access it.\nAccessing Clusters\nConfigure Access to Multiple Clusters\nUse Port Forwarding to Access Applications in a Cluster\nUse a Service to Access an Application in a Cluster\nConnect a Frontend to a Backend Using Services\nCreate an External Load Balancer\nList All Container Images Running in a Cluster\nCommunicate Between Containers in the Same Pod Using a Shared Volume\nConfigure DNS for a Cluster\nAccess Services Running on Clusters\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified January 13, 2023 at 11:05 AM PST:\nUpdate page weights in /tasks/access-application-cluster, /configure-pod-container, /configmap-secret (97693ff044)\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/tasks/access-application-cluster/"}}
{"text": "Monitoring, Logging, and Debugging | Kubernetes\nMonitoring, Logging, and Debugging\nSet up monitoring and logging to troubleshoot a cluster, or debug a containerized application.\nSometimes things go wrong. This guide helps you gather the relevant information and resolve issues. It has four sections:\nDebugging your application\n- Useful\nfor users who are deploying code into Kubernetes and wondering why it is not working.\nDebugging your cluster\n- Useful\nfor cluster administrators and operators troubleshooting issues with the Kubernetes cluster itself.\nLogging in Kubernetes\n- Useful\nfor cluster administrators who want to set up and manage logging in Kubernetes.\nMonitoring in Kubernetes\n- Useful\nfor cluster administrators who want to enable monitoring in a Kubernetes cluster.\nYou should also check the known issues for the\nrelease\nyou're using.\nGetting help\nIf your problem isn't answered by any of the guides above, there are variety of\nways for you to get help from the Kubernetes community.\nQuestions\nThe documentation on this site has been structured to provide answers to a wide\nrange of questions.\nConcepts\nexplain the Kubernetes\narchitecture and how each component works, while\nSetup\nprovides\npractical instructions for getting started.\nTasks\nshow how to\naccomplish commonly used tasks, and\nTutorials\nare more\ncomprehensive walkthroughs of real-world, industry-specific, or end-to-end\ndevelopment scenarios. The\nReference\nsection provides\ndetailed documentation on the\nKubernetes API\nand command-line interfaces (CLIs), such as\nkubectl\n.\nHelp! My question isn't covered! I need help now!\nStack Exchange, Stack Overflow, or Server Fault\nIf you have questions related to\nsoftware development\nfor your containerized app,\nyou can ask those on\nStack Overflow\n.\nIf you have Kubernetes questions related to\ncluster management\nor\nconfiguration\n,\nyou can ask those on\nServer Fault\n.\nThere are also several more specific Stack Exchange network sites which might\nbe the right place to ask Kubernetes questions in areas such as\nDevOps\n,\nSoftware Engineering\n,\nor\nInfoSec\n.\nSomeone else from the community may have already asked a similar question or\nmay be able to help with your problem.\nThe Kubernetes team will also monitor\nposts tagged Kubernetes\n.\nIf there aren't any existing questions that help,\nplease ensure that your question\nis\non-topic on Stack Overflow\n,\nServer Fault\n, or the Stack Exchange\nNetwork site you're asking on\n, and read through the guidance on\nhow to ask a new question\n,\nbefore asking a new one!\nSlack\nMany people from the Kubernetes community hang out on Kubernetes Slack in the\n#kubernetes-users\nchannel.\nSlack requires registration; you can\nrequest an invitation\n,\nand registration is open to everyone). Feel free to come and ask any and all questions.\nOnce registered, access the\nKubernetes organisation in Slack\nvia your web browser or via Slack's own dedicated app.\nOnce you are registered, browse the growing list of channels for various subjects of\ninterest. For example, people new to Kubernetes may also want to join the\n#kubernetes-novice\nchannel. As another example, developers should join the\n#kubernetes-contributors\nchannel.\nThere are also many country specific / local language channels. Feel free to join\nthese channels for localized support and info:\nCountry / language specific Slack channels\nCountry\nChannels\nChina\n#cn-users\n,\n#cn-events\nFinland\n#fi-users\nFrance\n#fr-users\n,\n#fr-events\nGermany\n#de-users\n,\n#de-events\nIndia\n#in-users\n,\n#in-events\nItaly\n#it-users\n,\n#it-events\nJapan\n#jp-users\n,\n#jp-events\nKorea\n#kr-users\nNetherlands\n#nl-users\nNorway\n#norw-users\nPoland\n#pl-users\nRussia\n#ru-users\nSpain\n#es-users\nSweden\n#se-users\nTurkey\n#tr-users\n,\n#tr-events\nForum\nYou're welcome to join the official Kubernetes Forum:\ndiscuss.kubernetes.io\n.\nBugs and feature requests\nIf you have what looks like a bug, or you would like to make a feature request,\nplease use the\nGitHub issue tracking system\n.\nBefore you file an issue, please search existing issues to see if your issue is\nalready covered.\nIf filing a bug, please include detailed information about how to reproduce the\nproblem, such as:\nKubernetes version:\nkubectl version\nCloud provider, OS distro, network configuration, and container runtime version\nSteps to reproduce the problem\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified August 09, 2025 at 2:51 PM PST:\nUpdating doc with Logging and Monitoring information (f5d051a432)\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/tasks/debug/"}}
{"text": "ConfigMaps | Kubernetes\nConfigMaps\nA ConfigMap is an API object used to store non-confidential data in key-value pairs.\nPods\ncan consume ConfigMaps as\nenvironment variables, command-line arguments, or as configuration files in a\nvolume\n.\nA ConfigMap allows you to decouple environment-specific configuration from your\ncontainer images\n, so that your applications are easily portable.\nCaution:\nConfigMap does not provide secrecy or encryption.\nIf the data you want to store are confidential, use a\nSecret\nrather than a ConfigMap,\nor use additional (third party) tools to keep your data private.\nMotivation\nUse a ConfigMap for setting configuration data separately from application code.\nFor example, imagine that you are developing an application that you can run on your\nown computer (for development) and in the cloud (to handle real traffic).\nYou write the code to look in an environment variable named\nDATABASE_HOST\n.\nLocally, you set that variable to\nlocalhost\n. In the cloud, you set it to\nrefer to a Kubernetes\nService\nthat exposes the database component to your cluster.\nThis lets you fetch a container image running in the cloud and\ndebug the exact same code locally if needed.\nNote:\nA ConfigMap is not designed to hold large chunks of data. The data stored in a\nConfigMap cannot exceed 1 MiB. If you need to store settings that are\nlarger than this limit, you may want to consider mounting a volume or use a\nseparate database or file service.\nConfigMap object\nA ConfigMap is an\nAPI object\nthat lets you store configuration for other objects to use. Unlike most\nKubernetes objects that have a\nspec\n, a ConfigMap has\ndata\nand\nbinaryData\nfields. These fields accept key-value pairs as their values. Both the\ndata\nfield and the\nbinaryData\nare optional. The\ndata\nfield is designed to\ncontain UTF-8 strings while the\nbinaryData\nfield is designed to\ncontain binary data as base64-encoded strings.\nThe name of a ConfigMap must be a valid\nDNS subdomain name\n.\nEach key under the\ndata\nor the\nbinaryData\nfield must consist of\nalphanumeric characters,\n-\n,\n_\nor\n.\n. The keys stored in\ndata\nmust not\noverlap with the keys in the\nbinaryData\nfield.\nStarting from v1.19, you can add an\nimmutable\nfield to a ConfigMap\ndefinition to create an\nimmutable ConfigMap\n.\nConfigMaps and Pods\nYou can write a Pod\nspec\nthat refers to a ConfigMap and configures the container(s)\nin that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in\nthe same\nnamespace\n.\nNote:\nThe\nspec\nof a\nstatic Pod\ncannot refer to a ConfigMap\nor any other API objects.\nHere's an example ConfigMap that has some keys with single values,\nand other keys where the value looks like a fragment of a configuration\nformat.\napiVersion\n:\nv1\nkind\n:\nConfigMap\nmetadata\n:\nname\n:\ngame-demo\ndata\n:\n# property-like keys; each key maps to a simple value\nplayer_initial_lives\n:\n\"3\"\nui_properties_file_name\n:\n\"user-interface.properties\"\n# file-like keys\ngame.properties\n:\n|\nenemy.types=aliens,monsters\nplayer.maximum-lives=5\nuser-interface.properties\n:\n|\ncolor.good=purple\ncolor.bad=yellow\nallow.textmode=true\nThere are four different ways that you can use a ConfigMap to configure\na container inside a Pod:\nInside a container command and args\nEnvironment variables for a container\nAdd a file in read-only volume, for the application to read\nWrite code to run inside the Pod that uses the Kubernetes API to read a ConfigMap\nThese different methods lend themselves to different ways of modeling\nthe data being consumed.\nFor the first three methods, the\nkubelet\nuses the data from\nthe ConfigMap when it launches container(s) for a Pod.\nThe fourth method means you have to write code to read the ConfigMap and its data.\nHowever, because you're using the Kubernetes API directly, your application can\nsubscribe to get updates whenever the ConfigMap changes, and react\nwhen that happens. By accessing the Kubernetes API directly, this\ntechnique also lets you access a ConfigMap in a different namespace.\nHere's an example Pod that uses values from\ngame-demo\nto configure a Pod:\nconfigmap/configure-pod.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nconfigmap-demo-pod\nspec\n:\ncontainers\n:\n-\nname\n:\ndemo\nimage\n:\nalpine\ncommand\n:\n[\n\"sleep\"\n,\n\"3600\"\n]\nenv\n:\n# Define the environment variable\n-\nname\n:\nPLAYER_INITIAL_LIVES\n# Notice that the case is different here\n# from the key name in the ConfigMap.\nvalueFrom\n:\nconfigMapKeyRef\n:\nname\n:\ngame-demo\n# The ConfigMap this value comes from.\nkey\n:\nplayer_initial_lives\n# The key to fetch.\n-\nname\n:\nUI_PROPERTIES_FILE_NAME\nvalueFrom\n:\nconfigMapKeyRef\n:\nname\n:\ngame-demo\nkey\n:\nui_properties_file_name\nvolumeMounts\n:\n-\nname\n:\nconfig\nmountPath\n:\n\"/config\"\nreadOnly\n:\ntrue\nvolumes\n:\n# You set volumes at the Pod level, then mount them into containers inside that Pod\n-\nname\n:\nconfig\nconfigMap\n:\n# Provide the name of the ConfigMap you want to mount.\nname\n:\ngame-demo\n# An array of keys from the ConfigMap to create as files\nitems\n:\n-\nkey\n:\n\"game.properties\"\npath\n:\n\"game.properties\"\n-\nkey\n:\n\"user-interface.properties\"\npath\n:\n\"user-interface.properties\"\nA ConfigMap doesn't differentiate between single line property values and\nmulti-line file-like values.\nWhat matters is how Pods and other objects consume those values.\nFor this example, defining a volume and mounting it inside the\ndemo\ncontainer as\n/config\ncreates two files,\n/config/game.properties\nand\n/config/user-interface.properties\n,\neven though there are four keys in the ConfigMap. This is because the Pod\ndefinition specifies an\nitems\narray in the\nvolumes\nsection.\nIf you omit the\nitems\narray entirely, every key in the ConfigMap becomes\na file with the same name as the key, and you get 4 files.\nUsing ConfigMaps\nConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other\nparts of the system, without being directly exposed to the Pod. For example,\nConfigMaps can hold data that other parts of the system should use for configuration.\nThe most common way to use ConfigMaps is to configure settings for\ncontainers running in a Pod in the same namespace. You can also use a\nConfigMap separately.\nFor example, you\nmight encounter\naddons\nor\noperators\nthat\nadjust their behavior based on a ConfigMap.\nUsing ConfigMaps as files from a Pod\nTo consume a ConfigMap in a volume in a Pod:\nCreate a ConfigMap or use an existing one. Multiple Pods can reference the\nsame ConfigMap.\nModify your Pod definition to add a volume under\n.spec.volumes[]\n. Name\nthe volume anything, and have a\n.spec.volumes[].configMap.name\nfield set\nto reference your ConfigMap object.\nAdd a\n.spec.containers[].volumeMounts[]\nto each container that needs the\nConfigMap. Specify\n.spec.containers[].volumeMounts[].readOnly = true\nand\n.spec.containers[].volumeMounts[].mountPath\nto an unused directory name\nwhere you would like the ConfigMap to appear.\nModify your image or command line so that the program looks for files in\nthat directory. Each key in the ConfigMap\ndata\nmap becomes the filename\nunder\nmountPath\n.\nThis is an example of a Pod that mounts a ConfigMap in a volume:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nmypod\nspec\n:\ncontainers\n:\n-\nname\n:\nmypod\nimage\n:\nredis\nvolumeMounts\n:\n-\nname\n:\nfoo\nmountPath\n:\n\"/etc/foo\"\nreadOnly\n:\ntrue\nvolumes\n:\n-\nname\n:\nfoo\nconfigMap\n:\nname\n:\nmyconfigmap\nEach ConfigMap you want to use needs to be referred to in\n.spec.volumes\n.\nIf there are multiple containers in the Pod, then each container needs its\nown\nvolumeMounts\nblock, but only one\n.spec.volumes\nis needed per ConfigMap.\nMounted ConfigMaps are updated automatically\nWhen a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.\nThe kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.\nHowever, the kubelet uses its local cache for getting the current value of the ConfigMap.\nThe type of the cache is configurable using the\nconfigMapAndSecretChangeDetectionStrategy\nfield in\nthe\nKubeletConfiguration struct\n.\nA ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting\nall requests directly to the API server.\nAs a result, the total delay from the moment when the ConfigMap is updated to the moment\nwhen new keys are projected to the Pod can be as long as the kubelet sync period + cache\npropagation delay, where the cache propagation delay depends on the chosen cache type\n(it equals to watch propagation delay, ttl of cache, or zero correspondingly).\nConfigMaps consumed as environment variables are not updated automatically and require a pod restart.\nNote:\nA container using a ConfigMap as a\nsubPath\nvolume mount will not receive ConfigMap updates.\nUsing Configmaps as environment variables\nTo use a Configmap in an\nenvironment variable\nin a Pod:\nFor each container in your Pod specification, add an environment variable\nfor each Configmap key that you want to use to the\nenv[].valueFrom.configMapKeyRef\nfield.\nModify your image and/or command line so that the program looks for values\nin the specified environment variables.\nThis is an example of defining a ConfigMap as a pod environment variable:\nThe following ConfigMap (myconfigmap.yaml) stores two properties: username and access_level:\napiVersion\n:\nv1\nkind\n:\nConfigMap\nmetadata\n:\nname\n:\nmyconfigmap\ndata\n:\nusername\n:\nk8s-admin\naccess_level\n:\n\"1\"\nThe following command will create the ConfigMap object:\nkubectl apply -f myconfigmap.yaml\nThe following Pod consumes the content of the ConfigMap as environment variables:\nconfigmap/env-configmap.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nenv-configmap\nspec\n:\ncontainers\n:\n-\nname\n:\napp\ncommand\n:\n[\n\"/bin/sh\"\n,\n\"-c\"\n,\n\"printenv\"\n]\nimage\n:\nbusybox:latest\nenvFrom\n:\n-\nconfigMapRef\n:\nname\n:\nmyconfigmap\nThe\nenvFrom\nfield instructs Kubernetes to create environment variables from the sources nested within it.\nThe inner\nconfigMapRef\nrefers to a ConfigMap by its name and selects all its key-value pairs.\nAdd the Pod to your cluster, then retrieve its logs to see the output from the printenv command.\nThis should confirm that the two key-value pairs from the ConfigMap have been set as environment variables:\nkubectl apply -f env-configmap.yaml\nkubectl logs pod/env-configmap\nThe output is similar to this:\n...\nusername: \"k8s-admin\"\naccess_level: \"1\"\n...\nSometimes a Pod won't require access to all the values in a ConfigMap.\nFor example, you could have another Pod which only uses the username value from the ConfigMap.\nFor this use case, you can use the\nenv.valueFrom\nsyntax instead, which lets you select individual keys in\na ConfigMap. The name of the environment variable can also be different from the key within the ConfigMap.\nFor example:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nenv-configmap\nspec\n:\ncontainers\n:\n-\nname\n:\nenvars-test-container\nimage\n:\nnginx\nenv\n:\n-\nname\n:\nCONFIGMAP_USERNAME\nvalueFrom\n:\nconfigMapKeyRef\n:\nname\n:\nmyconfigmap\nkey\n:\nusername\nIn the Pod created from this manifest, you will see that the environment variable\nCONFIGMAP_USERNAME\nis set to the value of the\nusername\nvalue from the ConfigMap.\nOther keys from the ConfigMap data are not copied into the environment.\nIt's important to note that the range of characters allowed for environment\nvariable names in pods is\nrestricted\n.\nIf any keys do not meet the rules, those keys are not made available to your container, though\nthe Pod is allowed to start.\nImmutable ConfigMaps\nFEATURE STATE:\nKubernetes v1.21 [stable]\nThe Kubernetes feature\nImmutable Secrets and ConfigMaps\nprovides an option to set\nindividual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps\n(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their\ndata has the following advantages:\nprotects you from accidental (or unwanted) updates that could cause applications outages\nimproves performance of your cluster by significantly reducing load on kube-apiserver, by\nclosing watches for ConfigMaps marked as immutable.\nYou can create an immutable ConfigMap by setting the\nimmutable\nfield to\ntrue\n.\nFor example:\napiVersion\n:\nv1\nkind\n:\nConfigMap\nmetadata\n:\n...\ndata\n:\n...\nimmutable\n:\ntrue\nOnce a ConfigMap is marked as immutable, it is\nnot\npossible to revert this change\nnor to mutate the contents of the\ndata\nor the\nbinaryData\nfield. You can\nonly delete and recreate the ConfigMap. Because existing Pods maintain a mount point\nto the deleted ConfigMap, it is recommended to recreate these pods.\nWhat's next\nRead about\nSecrets\n.\nRead\nConfigure a Pod to Use a ConfigMap\n.\nRead about\nchanging a ConfigMap (or any other Kubernetes object)\nRead\nThe Twelve-Factor App\nto understand the motivation for\nseparating code from configuration.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified November 21, 2025 at 2:18 PM PST:\nFix formatting of kubectl logs command (69fb346f79)\nConfigMap API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/configuration/configmap/"}}
{"text": "Secrets | Kubernetes\nSecrets\nA Secret is an object that contains a small amount of sensitive data such as\na password, a token, or a key. Such information might otherwise be put in a\nPod\nspecification or in a\ncontainer image\n. Using a\nSecret means that you don't need to include confidential data in your\napplication code.\nBecause Secrets can be created independently of the Pods that use them, there\nis less risk of the Secret (and its data) being exposed during the workflow of\ncreating, viewing, and editing Pods. Kubernetes, and applications that run in\nyour cluster, can also take additional precautions with Secrets, such as avoiding\nwriting sensitive data to nonvolatile storage.\nSecrets are similar to\nConfigMaps\nbut are specifically intended to hold confidential data.\nCaution:\nKubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store\n(etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd.\nAdditionally, anyone who is authorized to create a Pod in a namespace can use that access to read\nany Secret in that namespace; this includes indirect access such as the ability to create a\nDeployment.\nIn order to safely use Secrets, take at least the following steps:\nEnable Encryption at Rest\nfor Secrets.\nEnable or configure RBAC rules\nwith\nleast-privilege access to Secrets.\nRestrict Secret access to specific containers.\nConsider using external Secret store providers\n.\nFor more guidelines to manage and improve the security of your Secrets, refer to\nGood practices for Kubernetes Secrets\n.\nSee\nInformation security for Secrets\nfor more details.\nUses for Secrets\nYou can use Secrets for purposes such as the following:\nSet environment variables for a container\n.\nProvide credentials such as SSH keys or passwords to Pods\n.\nAllow the kubelet to pull container images from private registries\n.\nThe Kubernetes control plane also uses Secrets; for example,\nbootstrap token Secrets\nare a mechanism to\nhelp automate node registration.\nUse case: dotfiles in a secret volume\nYou can make your data \"hidden\" by defining a key that begins with a dot.\nThis key represents a dotfile or \"hidden\" file. For example, when the following Secret\nis mounted into a volume,\nsecret-volume\n, the volume will contain a single file,\ncalled\n.secret-file\n, and the\ndotfile-test-container\nwill have this file\npresent at the path\n/etc/secret-volume/.secret-file\n.\nNote:\nFiles beginning with dot characters are hidden from the output of\nls -l\n;\nyou must use\nls -la\nto see them when listing directory contents.\nsecret/dotfile-secret.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\ndotfile-secret\ndata\n:\n.secret-file\n:\ndmFsdWUtMg0KDQo=\n---\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nsecret-dotfiles-pod\nspec\n:\nvolumes\n:\n-\nname\n:\nsecret-volume\nsecret\n:\nsecretName\n:\ndotfile-secret\ncontainers\n:\n-\nname\n:\ndotfile-test-container\nimage\n:\nregistry.k8s.io/busybox\ncommand\n:\n- ls\n-\n\"-l\"\n-\n\"/etc/secret-volume\"\nvolumeMounts\n:\n-\nname\n:\nsecret-volume\nreadOnly\n:\ntrue\nmountPath\n:\n\"/etc/secret-volume\"\nUse case: Secret visible to one container in a Pod\nConsider a program that needs to handle HTTP requests, do some complex business\nlogic, and then sign some messages with an HMAC. Because it has complex\napplication logic, there might be an unnoticed remote file reading exploit in\nthe server, which could expose the private key to an attacker.\nThis could be divided into two processes in two containers: a frontend container\nwhich handles user interaction and business logic, but which cannot see the\nprivate key; and a signer container that can see the private key, and responds\nto simple signing requests from the frontend (for example, over localhost networking).\nWith this partitioned approach, an attacker now has to trick the application\nserver into doing something rather arbitrary, which may be harder than getting\nit to read a file.\nAlternatives to Secrets\nRather than using a Secret to protect confidential data, you can pick from alternatives.\nHere are some of your options:\nIf your cloud-native component needs to authenticate to another application that you\nknow is running within the same Kubernetes cluster, you can use a\nServiceAccount\nand its tokens to identify your client.\nThere are third-party tools that you can run, either within or outside your cluster,\nthat manage sensitive data. For example, a service that Pods access over HTTPS,\nthat reveals a Secret if the client correctly authenticates (for example, with a ServiceAccount\ntoken).\nFor authentication, you can implement a custom signer for X.509 certificates, and use\nCertificateSigningRequests\nto let that custom signer issue certificates to Pods that need them.\nYou can use a\ndevice plugin\nto expose node-local encryption hardware to a specific Pod. For example, you can schedule\ntrusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.\nYou can also combine two or more of those options, including the option to use Secret objects themselves.\nFor example: implement (or deploy) an\noperator\nthat fetches short-lived session tokens from an external service, and then creates Secrets based\non those short-lived session tokens. Pods running in your cluster can make use of the session tokens,\nand operator ensures they are valid. This separation means that you can run Pods that are unaware of\nthe exact mechanisms for issuing and refreshing those session tokens.\nTypes of Secret\nWhen creating a Secret, you can specify its type using the\ntype\nfield of\nthe\nSecret\nresource, or certain equivalent\nkubectl\ncommand line flags (if available).\nThe Secret type is used to facilitate programmatic handling of the Secret data.\nKubernetes provides several built-in types for some common usage scenarios.\nThese types vary in terms of the validations performed and the constraints\nKubernetes imposes on them.\nBuilt-in Type\nUsage\nOpaque\narbitrary user-defined data\nkubernetes.io/service-account-token\nServiceAccount token\nkubernetes.io/dockercfg\nserialized\n~/.dockercfg\nfile\nkubernetes.io/dockerconfigjson\nserialized\n~/.docker/config.json\nfile\nkubernetes.io/basic-auth\ncredentials for basic authentication\nkubernetes.io/ssh-auth\ncredentials for SSH authentication\nkubernetes.io/tls\ndata for a TLS client or server\nbootstrap.kubernetes.io/token\nbootstrap token data\nYou can define and use your own Secret type by assigning a non-empty string as the\ntype\nvalue for a Secret object (an empty string is treated as an\nOpaque\ntype).\nKubernetes doesn't impose any constraints on the type name. However, if you\nare using one of the built-in types, you must meet all the requirements defined\nfor that type.\nIf you are defining a type of Secret that's for public use, follow the convention\nand structure the Secret type to have your domain name before the name, separated\nby a\n/\n. For example:\ncloud-hosting.example.net/cloud-api-credentials\n.\nOpaque Secrets\nOpaque\nis the default Secret type if you don't explicitly specify a type in\na Secret manifest. When you create a Secret using\nkubectl\n, you must use the\ngeneric\nsubcommand to indicate an\nOpaque\nSecret type. For example, the\nfollowing command creates an empty Secret of type\nOpaque\n:\nkubectl create secret generic empty-secret\nkubectl get secret empty-secret\nThe output looks like:\nNAME           TYPE     DATA   AGE\nempty-secret   Opaque   0      2m6s\nThe\nDATA\ncolumn shows the number of data items stored in the Secret.\nIn this case,\n0\nmeans you have created an empty Secret.\nServiceAccount token Secrets\nA\nkubernetes.io/service-account-token\ntype of Secret is used to store a\ntoken credential that identifies a\nServiceAccount\n. This\nis a legacy mechanism that provides long-lived ServiceAccount credentials to\nPods.\nIn Kubernetes v1.22 and later, the recommended approach is to obtain a\nshort-lived, automatically rotating ServiceAccount token by using the\nTokenRequest\nAPI instead. You can get these short-lived tokens using the following methods:\nCall the\nTokenRequest\nAPI either directly or by using an API client like\nkubectl\n. For example, you can use the\nkubectl create token\ncommand.\nRequest a mounted token in a\nprojected volume\nin your Pod manifest. Kubernetes creates the token and mounts it in the Pod.\nThe token is automatically invalidated when the Pod that it's mounted in is\ndeleted. For details, see\nLaunch a Pod using service account token projection\n.\nNote:\nYou should only create a ServiceAccount token Secret\nif you can't use the\nTokenRequest\nAPI to obtain a token,\nand the security exposure of persisting a non-expiring token credential\nin a readable API object is acceptable to you. For instructions, see\nManually create a long-lived API token for a ServiceAccount\n.\nWhen using this Secret type, you need to ensure that the\nkubernetes.io/service-account.name\nannotation is set to an existing\nServiceAccount name. If you are creating both the ServiceAccount and\nthe Secret objects, you should create the ServiceAccount object first.\nAfter the Secret is created, a Kubernetes\ncontroller\nfills in some other fields such as the\nkubernetes.io/service-account.uid\nannotation, and the\ntoken\nkey in the\ndata\nfield, which is populated with an authentication token.\nThe following example configuration declares a ServiceAccount token Secret:\nsecret/serviceaccount-token-secret.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nsecret-sa-sample\nannotations\n:\nkubernetes.io/service-account.name\n:\n\"sa-name\"\ntype\n:\nkubernetes.io/service-account-token\ndata\n:\nextra\n:\nYmFyCg==\nAfter creating the Secret, wait for Kubernetes to populate the\ntoken\nkey in the\ndata\nfield.\nSee the\nServiceAccount\ndocumentation for more information on how ServiceAccounts work.\nYou can also check the\nautomountServiceAccountToken\nfield and the\nserviceAccountName\nfield of the\nPod\nfor information on referencing ServiceAccount credentials from within Pods.\nDocker config Secrets\nIf you are creating a Secret to store credentials for accessing a container image registry,\nyou must use one of the following\ntype\nvalues for that Secret:\nkubernetes.io/dockercfg\n: store a serialized\n~/.dockercfg\nwhich is the\nlegacy format for configuring Docker command line. The Secret\ndata\nfield contains a\n.dockercfg\nkey whose value is the content of a\nbase64 encoded\n~/.dockercfg\nfile.\nkubernetes.io/dockerconfigjson\n: store a serialized JSON that follows the\nsame format rules as the\n~/.docker/config.json\nfile, which is a new format\nfor\n~/.dockercfg\n. The Secret\ndata\nfield must contain a\n.dockerconfigjson\nkey for which the value is the content of a base64\nencoded\n~/.docker/config.json\nfile.\nBelow is an example for a\nkubernetes.io/dockercfg\ntype of Secret:\nsecret/dockercfg-secret.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nsecret-dockercfg\ntype\n:\nkubernetes.io/dockercfg\ndata\n:\n.dockercfg\n:\n|\neyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=\nNote:\nIf you do not want to perform the base64 encoding, you can choose to use the\nstringData\nfield instead.\nWhen you create Docker config Secrets using a manifest, the API\nserver checks whether the expected key exists in the\ndata\nfield, and\nit verifies if the value provided can be parsed as a valid JSON. The API\nserver doesn't validate if the JSON actually is a Docker config file.\nYou can also use\nkubectl\nto create a Secret for accessing a container\nregistry, such as when you don't have a Docker configuration file:\nkubectl create secret docker-registry secret-tiger-docker\n\\\n--docker-email\n=\ntiger@acme.example\n\\\n--docker-username\n=\ntiger\n\\\n--docker-password\n=\npass1234\n\\\n--docker-server\n=\nmy-registry.example:5000\nThis command creates a Secret of type\nkubernetes.io/dockerconfigjson\n.\nRetrieve the\n.data.dockerconfigjson\nfield from that new Secret and decode the\ndata:\nkubectl get secret secret-tiger-docker -o\njsonpath\n=\n'{.data.*}'\n| base64 -d\nThe output is equivalent to the following JSON document (which is also a valid\nDocker configuration file):\n{\n\"auths\"\n: {\n\"my-registry.example:5000\"\n: {\n\"username\"\n:\n\"tiger\"\n,\n\"password\"\n:\n\"pass1234\"\n,\n\"email\"\n:\n\"tiger@acme.example\"\n,\n\"auth\"\n:\n\"dGlnZXI6cGFzczEyMzQ=\"\n}\n}\n}\nCaution:\nThe\nauth\nvalue there is base64 encoded; it is obscured but not secret.\nAnyone who can read that Secret can learn the registry access bearer token.\nIt is suggested to use\ncredential providers\nto dynamically and securely provide pull secrets on-demand.\nBasic authentication Secret\nThe\nkubernetes.io/basic-auth\ntype is provided for storing credentials needed\nfor basic authentication. When using this Secret type, the\ndata\nfield of the\nSecret must contain one of the following two keys:\nusername\n: the user name for authentication\npassword\n: the password or token for authentication\nBoth values for the above two keys are base64 encoded strings. You can\nalternatively provide the clear text content using the\nstringData\nfield in the\nSecret manifest.\nThe following manifest is an example of a basic authentication Secret:\nsecret/basicauth-secret.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nsecret-basic-auth\ntype\n:\nkubernetes.io/basic-auth\nstringData\n:\nusername\n:\nadmin\n# required field for kubernetes.io/basic-auth\npassword\n:\nt0p-Secret\n# required field for kubernetes.io/basic-auth\nNote:\nThe\nstringData\nfield for a Secret does not work well with server-side apply.\nThe basic authentication Secret type is provided only for convenience.\nYou can create an\nOpaque\ntype for credentials used for basic authentication.\nHowever, using the defined and public Secret type (\nkubernetes.io/basic-auth\n) helps other\npeople to understand the purpose of your Secret, and sets a convention for what key names\nto expect.\nSSH authentication Secrets\nThe builtin type\nkubernetes.io/ssh-auth\nis provided for storing data used in\nSSH authentication. When using this Secret type, you will have to specify a\nssh-privatekey\nkey-value pair in the\ndata\n(or\nstringData\n) field\nas the SSH credential to use.\nThe following manifest is an example of a Secret used for SSH public/private\nkey authentication:\nsecret/ssh-auth-secret.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nsecret-ssh-auth\ntype\n:\nkubernetes.io/ssh-auth\ndata\n:\n# the data is abbreviated in this example\nssh-privatekey\n:\n|\nUG91cmluZzYlRW1vdGljb24lU2N1YmE=\nThe SSH authentication Secret type is provided only for convenience.\nYou can create an\nOpaque\ntype for credentials used for SSH authentication.\nHowever, using the defined and public Secret type (\nkubernetes.io/ssh-auth\n) helps other\npeople to understand the purpose of your Secret, and sets a convention for what key names\nto expect.\nThe Kubernetes API verifies that the required keys are set for a Secret of this type.\nCaution:\nSSH private keys do not establish trusted communication between an SSH client and\nhost server on their own. A secondary means of establishing trust is needed to\nmitigate \"man in the middle\" attacks, such as a\nknown_hosts\nfile added to a ConfigMap.\nTLS Secrets\nThe\nkubernetes.io/tls\nSecret type is for storing\na certificate and its associated key that are typically used for TLS.\nOne common use for TLS Secrets is to configure encryption in transit for\nan\nIngress\n, but you can also use it\nwith other resources or directly in your workload.\nWhen using this type of Secret, the\ntls.key\nand the\ntls.crt\nkey must be provided\nin the\ndata\n(or\nstringData\n) field of the Secret configuration, although the API\nserver doesn't actually validate the values for each key.\nAs an alternative to using\nstringData\n, you can use the\ndata\nfield to provide\nthe base64 encoded certificate and private key. For details, see\nConstraints on Secret names and data\n.\nThe following YAML contains an example config for a TLS Secret:\nsecret/tls-auth-secret.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nsecret-tls\ntype\n:\nkubernetes.io/tls\ndata\n:\n# values are base64 encoded, which obscures them but does NOT provide\n# any useful level of confidentiality\n# Replace the following values with your own base64-encoded certificate and key.\ntls.crt\n:\n\"REPLACE_WITH_BASE64_CERT\"\ntls.key\n:\n\"REPLACE_WITH_BASE64_KEY\"\nThe TLS Secret type is provided only for convenience.\nYou can create an\nOpaque\ntype for credentials used for TLS authentication.\nHowever, using the defined and public Secret type (\nkubernetes.io/tls\n)\nhelps ensure the consistency of Secret format in your project. The API server\nverifies if the required keys are set for a Secret of this type.\nTo create a TLS Secret using\nkubectl\n, use the\ntls\nsubcommand:\nkubectl create secret tls my-tls-secret\n\\\n--cert\n=\npath/to/cert/file\n\\\n--key\n=\npath/to/key/file\nThe public/private key pair must exist before hand. The public key certificate for\n--cert\nmust be .PEM encoded\nand must match the given private key for\n--key\n.\nBootstrap token Secrets\nThe\nbootstrap.kubernetes.io/token\nSecret type is for\ntokens used during the node bootstrap process. It stores tokens used to sign\nwell-known ConfigMaps.\nA bootstrap token Secret is usually created in the\nkube-system\nnamespace and\nnamed in the form\nbootstrap-token-<token-id>\nwhere\n<token-id>\nis a 6 character\nstring of the token ID.\nAs a Kubernetes manifest, a bootstrap token Secret might look like the\nfollowing:\nsecret/bootstrap-token-secret-base64.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nbootstrap-token-5emitj\nnamespace\n:\nkube-system\ntype\n:\nbootstrap.kubernetes.io/token\ndata\n:\nauth-extra-groups\n:\nc3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=\nexpiration\n:\nMjAyMC0wOS0xM1QwNDozOToxMFo=\ntoken-id\n:\nNWVtaXRq\ntoken-secret\n:\na3E0Z2lodnN6emduMXAwcg==\nusage-bootstrap-authentication\n:\ndHJ1ZQ==\nusage-bootstrap-signing\n:\ndHJ1ZQ==\nA bootstrap token Secret has the following keys specified under\ndata\n:\ntoken-id\n: A random 6 character string as the token identifier. Required.\ntoken-secret\n: A random 16 character string as the actual token Secret. Required.\ndescription\n: A human-readable string that describes what the token is\nused for. Optional.\nexpiration\n: An absolute UTC time using\nRFC3339\nspecifying when the token\nshould be expired. Optional.\nusage-bootstrap-<usage>\n: A boolean flag indicating additional usage for\nthe bootstrap token.\nauth-extra-groups\n: A comma-separated list of group names that will be\nauthenticated as in addition to the\nsystem:bootstrappers\ngroup.\nYou can alternatively provide the values in the\nstringData\nfield of the Secret\nwithout base64 encoding them:\nsecret/bootstrap-token-secret-literal.yaml\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\n# Note how the Secret is named\nname\n:\nbootstrap-token-5emitj\n# A bootstrap token Secret usually resides in the kube-system namespace\nnamespace\n:\nkube-system\ntype\n:\nbootstrap.kubernetes.io/token\nstringData\n:\nauth-extra-groups\n:\n\"system:bootstrappers:kubeadm:default-node-token\"\nexpiration\n:\n\"2020-09-13T04:39:10Z\"\n# This token ID is used in the name\ntoken-id\n:\n\"5emitj\"\ntoken-secret\n:\n\"kq4gihvszzgn1p0r\"\n# This token can be used for authentication\nusage-bootstrap-authentication\n:\n\"true\"\n# and it can be used for signing\nusage-bootstrap-signing\n:\n\"true\"\nNote:\nThe\nstringData\nfield for a Secret does not work well with server-side apply.\nWorking with Secrets\nCreating a Secret\nThere are several options to create a Secret:\nUse\nkubectl\nUse a configuration file\nUse the Kustomize tool\nConstraints on Secret names and data\nThe name of a Secret object must be a valid\nDNS subdomain name\n.\nYou can specify the\ndata\nand/or the\nstringData\nfield when creating a\nconfiguration file for a Secret. The\ndata\nand the\nstringData\nfields are optional.\nThe values for all keys in the\ndata\nfield have to be base64-encoded strings.\nIf the conversion to base64 string is not desirable, you can choose to specify\nthe\nstringData\nfield instead, which accepts arbitrary strings as values.\nThe keys of\ndata\nand\nstringData\nmust consist of alphanumeric characters,\n-\n,\n_\nor\n.\n. All key-value pairs in the\nstringData\nfield are internally\nmerged into the\ndata\nfield. If a key appears in both the\ndata\nand the\nstringData\nfield, the value specified in the\nstringData\nfield takes\nprecedence.\nSize limit\nIndividual Secrets are limited to 1MiB in size. This is to discourage creation\nof very large Secrets that could exhaust the API server and kubelet memory.\nHowever, creation of many smaller Secrets could also exhaust memory. You can\nuse a\nresource quota\nto limit the\nnumber of Secrets (or other resources) in a namespace.\nEditing a Secret\nYou can edit an existing Secret unless it is\nimmutable\n. To\nedit a Secret, use one of the following methods:\nUse\nkubectl\nUse a configuration file\nYou can also edit the data in a Secret using the\nKustomize tool\n. However, this\nmethod creates a new\nSecret\nobject with the edited data.\nDepending on how you created the Secret, as well as how the Secret is used in\nyour Pods, updates to existing\nSecret\nobjects are propagated automatically to\nPods that use the data. For more information, refer to\nUsing Secrets as files from a Pod\nsection.\nUsing a Secret\nSecrets can be mounted as data volumes or exposed as\nenvironment variables\nto be used by a container in a Pod. Secrets can also be used by other parts of the\nsystem, without being directly exposed to the Pod. For example, Secrets can hold\ncredentials that other parts of the system should use to interact with external\nsystems on your behalf.\nSecret volume sources are validated to ensure that the specified object\nreference actually points to an object of type Secret. Therefore, a Secret\nneeds to be created before any Pods that depend on it.\nIf the Secret cannot be fetched (perhaps because it does not exist, or\ndue to a temporary lack of connection to the API server) the kubelet\nperiodically retries running that Pod. The kubelet also reports an Event\nfor that Pod, including details of the problem fetching the Secret.\nOptional Secrets\nWhen you reference a Secret in a Pod, you can mark the Secret as\noptional\n,\nsuch as in the following example. If an optional Secret doesn't exist,\nKubernetes ignores it.\nsecret/optional-secret.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nmypod\nspec\n:\ncontainers\n:\n-\nname\n:\nmypod\nimage\n:\nredis\nvolumeMounts\n:\n-\nname\n:\nfoo\nmountPath\n:\n\"/etc/foo\"\nreadOnly\n:\ntrue\nvolumes\n:\n-\nname\n:\nfoo\nsecret\n:\nsecretName\n:\nmysecret\noptional\n:\ntrue\nBy default, Secrets are required. None of a Pod's containers will start until\nall non-optional Secrets are available.\nIf a Pod references a specific key in a non-optional Secret and that Secret\ndoes exist, but is missing the named key, the Pod fails during startup.\nUsing Secrets as files from a Pod\nIf you want to access data from a Secret in a Pod, one way to do that is to\nhave Kubernetes make the value of that Secret be available as a file inside\nthe filesystem of one or more of the Pod's containers.\nFor instructions, refer to\nCreate a Pod that has access to the secret data through a Volume\n.\nWhen a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks\nthis and updates the data in the volume, using an eventually-consistent approach.\nNote:\nA container using a Secret as a\nsubPath\nvolume mount does not receive\nautomated Secret updates.\nThe kubelet keeps a cache of the current keys and values for the Secrets that are used in\nvolumes for pods on that node.\nYou can configure the way that the kubelet detects changes from the cached values. The\nconfigMapAndSecretChangeDetectionStrategy\nfield in the\nkubelet configuration\ncontrols\nwhich strategy the kubelet uses. The default strategy is\nWatch\n.\nUpdates to Secrets can be either propagated by an API watch mechanism (the default), based on\na cache with a defined time-to-live, or polled from the cluster API server on each kubelet\nsynchronisation loop.\nAs a result, the total delay from the moment when the Secret is updated to the moment\nwhen new keys are projected to the Pod can be as long as the kubelet sync period + cache\npropagation delay, where the cache propagation delay depends on the chosen cache type\n(following the same order listed in the previous paragraph, these are:\nwatch propagation delay, the configured cache TTL, or zero for direct polling).\nUsing Secrets as environment variables\nTo use a Secret in an\nenvironment variable\nin a Pod:\nFor each container in your Pod specification, add an environment variable\nfor each Secret key that you want to use to the\nenv[].valueFrom.secretKeyRef\nfield.\nModify your image and/or command line so that the program looks for values\nin the specified environment variables.\nFor instructions, refer to\nDefine container environment variables using Secret data\n.\nIt's important to note that the range of characters allowed for environment variable\nnames in pods is\nrestricted\n.\nIf any keys do not meet the rules, those keys are not made available to your container, though\nthe Pod is allowed to start.\nContainer image pull Secrets\nIf you want to fetch container images from a private repository, you need a way for\nthe kubelet on each node to authenticate to that repository. You can configure\nimage pull Secrets\nto make this possible. These Secrets are configured at the Pod\nlevel.\nUsing imagePullSecrets\nThe\nimagePullSecrets\nfield is a list of references to Secrets in the same namespace.\nYou can use an\nimagePullSecrets\nto pass a Secret that contains a Docker (or other) image registry\npassword to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.\nSee the\nPodSpec API\nfor more information about the\nimagePullSecrets\nfield.\nManually specifying an imagePullSecret\nYou can learn how to specify\nimagePullSecrets\nfrom the\ncontainer images\ndocumentation.\nArranging for imagePullSecrets to be automatically attached\nYou can manually create\nimagePullSecrets\n, and reference these from a ServiceAccount. Any Pods\ncreated with that ServiceAccount or created with that ServiceAccount by default, will get their\nimagePullSecrets\nfield set to that of the service account.\nSee\nAdd ImagePullSecrets to a service account\nfor a detailed explanation of that process.\nUsing Secrets with static Pods\nYou cannot use ConfigMaps or Secrets with\nstatic Pods\n.\nImmutable Secrets\nFEATURE STATE:\nKubernetes v1.21 [stable]\nKubernetes lets you mark specific Secrets (and ConfigMaps) as\nimmutable\n.\nPreventing changes to the data of an existing Secret has the following benefits:\nprotects you from accidental (or unwanted) updates that could cause applications outages\n(for clusters that extensively use Secrets - at least tens of thousands of unique Secret\nto Pod mounts), switching to immutable Secrets improves the performance of your cluster\nby significantly reducing load on kube-apiserver. The kubelet does not need to maintain\na [watch] on any Secrets that are marked as immutable.\nMarking a Secret as immutable\nYou can create an immutable Secret by setting the\nimmutable\nfield to\ntrue\n. For example,\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\n...\ndata\n:\n...\nimmutable\n:\ntrue\nYou can also update any existing mutable Secret to make it immutable.\nNote:\nOnce a Secret or ConfigMap is marked as immutable, it is\nnot\npossible to revert this change\nnor to mutate the contents of the\ndata\nfield. You can only delete and recreate the Secret.\nExisting Pods maintain a mount point to the deleted Secret - it is recommended to recreate\nthese pods.\nInformation security for Secrets\nAlthough ConfigMap and Secret work similarly, Kubernetes applies some additional\nprotection for Secret objects.\nSecrets often hold values that span a spectrum of importance, many of which can\ncause escalations within Kubernetes (e.g. service account tokens) and to\nexternal systems. Even if an individual app can reason about the power of the\nSecrets it expects to interact with, other apps within the same namespace can\nrender those assumptions invalid.\nA Secret is only sent to a node if a Pod on that node requires it.\nFor mounting Secrets into Pods, the kubelet stores a copy of the data into a\ntmpfs\nso that the confidential data is not written to durable storage.\nOnce the Pod that depends on the Secret is deleted, the kubelet deletes its local copy\nof the confidential data from the Secret.\nThere may be several containers in a Pod. By default, containers you define\nonly have access to the default ServiceAccount and its related Secret.\nYou must explicitly define environment variables or map a volume into a\ncontainer in order to provide access to any other Secret.\nThere may be Secrets for several Pods on the same node. However, only the\nSecrets that a Pod requests are potentially visible within its containers.\nTherefore, one Pod does not have access to the Secrets of another Pod.\nConfigure least-privilege access to Secrets\nTo enhance the security measures around Secrets, use separate namespaces to isolate access to mounted secrets.\nWarning:\nAny containers that run with\nprivileged: true\non a node can access all\nSecrets used on that node.\nWhat's next\nFor guidelines to manage and improve the security of your Secrets, refer to\nGood practices for Kubernetes Secrets\n.\nLearn how to\nmanage Secrets using\nkubectl\nLearn how to\nmanage Secrets using config file\nLearn how to\nmanage Secrets using kustomize\nRead the\nAPI reference\nfor\nSecret\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified November 19, 2024 at 10:53 PM PST:\nAddress comments (3b8c927a3b)\nSecret API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/configuration/secret/"}}
