{"text": "Getting Started - pip documentation v25.3\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\npip documentation v25.3\npip documentation v25.3\nGetting Started\nInstallation\nUser Guide\nTopic Guides\nAuthentication\nCaching\nConfiguration\nDependency Resolution\nMore on Dependency Resolution\nHTTPS Certificates\nLocal project installs\nRepeatable Installs\nSecure installs\nVCS Support\nManaging a different Python interpreter\nPip is not a workflow management tool\nReference\nBuild System Interface\nRequirement Specifiers\nRequirements File Format\nInstallation Report\npip\ninspect\nJSON output specification\nCommands\npip\npip install\npip uninstall\npip inspect\npip list\npip show\npip freeze\npip check\npip lock\npip download\npip wheel\npip hash\npip search\npip index\npip cache\npip config\npip debug\nProject\nDevelopment\nGetting Started\nContributing\nContinuous Integration\nIssue Triage\nArchitecture of pip’s internals\nBroad functionality overview\nRepository anatomy & directory structure\nConfiguration File Handling\nFinding and choosing files (\nindex\nand\nPackageFinder\n)\nCommand Line Interface\nOptions that control the installation process\nRelease process\nVendoring Policy\nUX Research & Design\nHow to Contribute\nUX Guidance\nUX Research Results\nAbout pip’s Users\nHow Users Understand pip\nHow pip users think about security\nHow pip is used in interactive environments (i.e. CI, CD)\npip Personas\nPrioritizing pip Features\nProviding an override to install packages with conflicting dependencies\npip --force-reinstall\npip search\npip Upgrade Conflict\nImproving pip’s Documentation\nChangelog\nCode of Conduct\nGitHub\nBack to top\nView this page\nGetting Started\n¶\nTo get started with using pip, you should\ninstall Python\non your system.\nEnsure you have a working pip\n¶\nAs a first step, you should check that you have a working Python with pip\ninstalled. This can be done by running the following commands and making\nsure that the output looks similar.\nLinux\n$\npython\n--version\nPython 3.N.N\n$\npython\n-m\npip\n--version\npip X.Y.Z from ... (python 3.N.N)\nMacOS\n$\npython\n--version\nPython 3.N.N\n$\npython\n-m\npip\n--version\npip X.Y.Z from ... (python 3.N.N)\nWindows\nC:>\npy --version\nPython 3.N.N\nC:>\npy -m pip --version\npip X.Y.Z from ... (python 3.N.N)\nIf that worked, congratulations! You have a working pip in your environment.\nIf you got output that does not look like the sample above, please read\nthe\nInstallation\npage. It provides guidance on how to install pip\nwithin a Python environment that doesn’t have it.\nCommon tasks\n¶\nInstall a package\n¶\nLinux\n$\npython\n-m\npip\ninstall\nsampleproject\n[...]\nSuccessfully installed sampleproject\nMacOS\n$\npython\n-m\npip\ninstall\nsampleproject\n[...]\nSuccessfully installed sampleproject\nWindows\nC:>\npy -m pip install sampleproject\n[...]\nSuccessfully installed sampleproject\nBy default, pip will fetch packages from\nPython Package Index\n, a\nrepository of software for the Python programming language where anyone can\nupload packages.\nInstall a package from GitHub\n¶\nLinux\n$\npython\n-m\npip\ninstall\ngit+https://github.com/pypa/sampleproject.git@main\n[...]\nSuccessfully installed sampleproject\nMacOS\n$\npython\n-m\npip\ninstall\ngit+https://github.com/pypa/sampleproject.git@main\n[...]\nSuccessfully installed sampleproject\nWindows\nC:>\npy -m pip install git+https://github.com/pypa/sampleproject.git@main\n[...]\nSuccessfully installed sampleproject\nSee\nVCS Support\nfor more information about this syntax.\nInstall a package from a distribution file\n¶\npip can install directly from distribution files as well. They come in 2 forms:\nsource distribution\n(usually shortened to “sdist”)\nwheel distribution\n(usually shortened to “wheel”)\nLinux\n$\npython\n-m\npip\ninstall\nsampleproject-1.0.tar.gz\n[...]\nSuccessfully installed sampleproject\n$\npython\n-m\npip\ninstall\nsampleproject-1.0-py3-none-any.whl\n[...]\nSuccessfully installed sampleproject\nMacOS\n$\npython\n-m\npip\ninstall\nsampleproject-1.0.tar.gz\n[...]\nSuccessfully installed sampleproject\n$\npython\n-m\npip\ninstall\nsampleproject-1.0-py3-none-any.whl\n[...]\nSuccessfully installed sampleproject\nWindows\nC:>\npy -m pip install sampleproject-1.0.tar.gz\n[...]\nSuccessfully installed sampleproject\nC:>\npy -m pip install sampleproject-1.0-py3-none-any.whl\n[...]\nSuccessfully installed sampleproject\nInstall multiple packages using a requirements file\n¶\nMany Python projects use\nrequirements.txt\nfiles, to specify the\nlist of packages that need to be installed for the project to run. To install\nthe packages listed in that file, you can run:\nLinux\n$\npython\n-m\npip\ninstall\n-r\nrequirements.txt\n[...]\nSuccessfully installed sampleproject\nMacOS\n$\npython\n-m\npip\ninstall\n-r\nrequirements.txt\n[...]\nSuccessfully installed sampleproject\nWindows\nC:>\npy -m pip install -r requirements.txt\n[...]\nSuccessfully installed sampleproject\nUpgrade a package\n¶\nLinux\n$\npython\n-m\npip\ninstall\n--upgrade\nsampleproject\n[...]\nSuccessfully installed sampleproject\nMacOS\n$\npython\n-m\npip\ninstall\n--upgrade\nsampleproject\n[...]\nSuccessfully installed sampleproject\nWindows\nC:>\npy -m pip install --upgrade sampleproject\n[...]\nSuccessfully installed sampleproject\nUninstall a package\n¶\nLinux\n$\npython\n-m\npip\nuninstall\nsampleproject\nUninstalling sampleproject:\n[...]\nProceed (Y/n)? y\nSuccessfully uninstalled sampleproject\nMacOS\n$\npython\n-m\npip\nuninstall\nsampleproject\nUninstalling sampleproject:\n[...]\nProceed (Y/n)? y\nSuccessfully uninstalled sampleproject\nWindows\nC:>\npy -m pip uninstall sampleproject\nUninstalling sampleproject:\n[...]\nProceed (Y/n)? y\nSuccessfully uninstalled sampleproject\nNext Steps\n¶\nIt is recommended to learn about what virtual environments are and how to use\nthem. This is covered in the\nInstalling Packages\ntutorial on packaging.python.org.\nOn this page\nGetting Started\nEnsure you have a working pip\nCommon tasks\nInstall a package\nInstall a package from GitHub\nInstall a package from a distribution file\nInstall multiple packages using a requirements file\nUpgrade a package\nUninstall a package\nNext Steps", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://pip.pypa.io/en/stable/getting-started/"}}
{"text": "pip install - pip documentation v25.3\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\npip documentation v25.3\npip documentation v25.3\nGetting Started\nInstallation\nUser Guide\nTopic Guides\nAuthentication\nCaching\nConfiguration\nDependency Resolution\nMore on Dependency Resolution\nHTTPS Certificates\nLocal project installs\nRepeatable Installs\nSecure installs\nVCS Support\nManaging a different Python interpreter\nPip is not a workflow management tool\nReference\nBuild System Interface\nRequirement Specifiers\nRequirements File Format\nInstallation Report\npip\ninspect\nJSON output specification\nCommands\npip\npip install\npip uninstall\npip inspect\npip list\npip show\npip freeze\npip check\npip lock\npip download\npip wheel\npip hash\npip search\npip index\npip cache\npip config\npip debug\nProject\nDevelopment\nGetting Started\nContributing\nContinuous Integration\nIssue Triage\nArchitecture of pip’s internals\nBroad functionality overview\nRepository anatomy & directory structure\nConfiguration File Handling\nFinding and choosing files (\nindex\nand\nPackageFinder\n)\nCommand Line Interface\nOptions that control the installation process\nRelease process\nVendoring Policy\nUX Research & Design\nHow to Contribute\nUX Guidance\nUX Research Results\nAbout pip’s Users\nHow Users Understand pip\nHow pip users think about security\nHow pip is used in interactive environments (i.e. CI, CD)\npip Personas\nPrioritizing pip Features\nProviding an override to install packages with conflicting dependencies\npip --force-reinstall\npip search\npip Upgrade Conflict\nImproving pip’s Documentation\nChangelog\nCode of Conduct\nGitHub\nBack to top\nView this page\npip install\n¶\nUsage\n¶\nUnix/macOS\npython\n-\nm\npip\ninstall\n[\noptions\n]\n<\nrequirement\nspecifier\n>\n[\npackage\n-\nindex\n-\noptions\n]\n...\npython\n-\nm\npip\ninstall\n[\noptions\n]\n-\nr\n<\nrequirements\nfile\n>\n[\npackage\n-\nindex\n-\noptions\n]\n...\npython\n-\nm\npip\ninstall\n[\noptions\n]\n[\n-\ne\n]\n<\nvcs\nproject\nurl\n>\n...\npython\n-\nm\npip\ninstall\n[\noptions\n]\n[\n-\ne\n]\n<\nlocal\nproject\npath\n>\n...\npython\n-\nm\npip\ninstall\n[\noptions\n]\n<\narchive\nurl\n/\npath\n>\n...\nWindows\npy\n-\nm\npip\ninstall\n[\noptions\n]\n<\nrequirement\nspecifier\n>\n[\npackage\n-\nindex\n-\noptions\n]\n...\npy\n-\nm\npip\ninstall\n[\noptions\n]\n-\nr\n<\nrequirements\nfile\n>\n[\npackage\n-\nindex\n-\noptions\n]\n...\npy\n-\nm\npip\ninstall\n[\noptions\n]\n[\n-\ne\n]\n<\nvcs\nproject\nurl\n>\n...\npy\n-\nm\npip\ninstall\n[\noptions\n]\n[\n-\ne\n]\n<\nlocal\nproject\npath\n>\n...\npy\n-\nm\npip\ninstall\n[\noptions\n]\n<\narchive\nurl\n/\npath\n>\n...\nDescription\n¶\nInstall packages from:\nPyPI (and other indexes) using requirement specifiers.\nVCS project urls.\nLocal project directories.\nLocal or remote source archives.\npip also supports installing from “requirements files”, which provide\nan easy way to specify a whole environment to be installed.\nOverview\n¶\npip install has several stages:\nIdentify the base requirements. The user supplied arguments are processed\nhere.\nResolve dependencies. What will be installed is determined here.\nBuild wheels. All the dependencies that can be are built into wheels.\nInstall the packages (and uninstall anything being upgraded/replaced).\nNote that\npip\ninstall\nprefers to leave the installed version as-is\nunless\n--upgrade\nis specified.\nArgument Handling\n¶\nWhen looking at the items to be installed, pip checks what type of item\neach is, in the following order:\nProject or archive URL.\nLocal directory (which must contain a\npyproject.toml\nor\nsetup.py\n,\notherwise pip will report an error).\nLocal file (a sdist or wheel format archive, following the naming\nconventions for those formats).\nA\nversion specifier\n.\nEach item identified is added to the set of requirements to be satisfied by\nthe install.\nWorking Out the Name and Version\n¶\nFor each candidate item, pip needs to know the project name and version. For\nwheels (identified by the\n.whl\nfile extension) this can be obtained from\nthe filename, as per the Wheel spec. For local directories, or explicitly\nspecified sdist files, the\nsetup.py\negg_info\ncommand is used to determine\nthe project metadata. For sdists located via an index, the filename is parsed\nfor the name and project version (this is in theory slightly less reliable\nthan using the\negg_info\ncommand, but avoids downloading and processing\nunnecessary numbers of files).\nThe\nDirect URL requirement syntax\ncan be used\nto explicitly state the project name (see\nVCS Support\n).\nSatisfying Requirements\n¶\nOnce pip has the set of requirements to satisfy, it chooses which version of\neach requirement to install using the simple rule that the latest version that\nsatisfies the given constraints will be installed (but see\nhere\nfor an exception regarding pre-release versions). Where more than one source of\nthe chosen version is available, it is assumed that any source is acceptable\n(as otherwise the versions would differ).\nObtaining information about what was installed\n¶\nThe install command has a\n--report\noption that will generate a JSON report of what\npip has installed. In combination with the\n--dry-run\nand\n--ignore-installed\nit\ncan be used to\nresolve\na set of requirements without actually installing them.\nThe report can be written to a file, or to standard output (using\n--report\n-\nin\ncombination with\n--quiet\n).\nThe format of the JSON report is described in\nInstallation Report\n.\nInstallation Order\n¶\nNote\nThis section is only about installation order of runtime dependencies, and\ndoes not apply to build dependencies (those are specified using the\n[build-system] table\n).\nAs of v6.1.0, pip installs dependencies before their dependents, i.e. in\n“topological order.”  This is the only commitment pip currently makes related\nto order.  While it may be coincidentally true that pip will install things in\nthe order of the install arguments or in the order of the items in a\nrequirements file, this is not a promise.\nIn the event of a dependency cycle (aka “circular dependency”), the current\nimplementation (which might possibly change later) has it such that the first\nencountered member of the cycle is installed last.\nFor instance, if quux depends on foo which depends on bar which depends on baz,\nwhich depends on foo:\nUnix/macOS\n$\npython\n-m\npip\ninstall\nquux\n...\nInstalling collected packages baz, bar, foo, quux\n$\npython\n-m\npip\ninstall\nbar\n...\nInstalling collected packages foo, baz, bar\nWindows\nC:\\> py -m pip install quux\n...\nInstalling collected packages baz, bar, foo, quux\nC:\\> py -m pip install bar\n...\nInstalling collected packages foo, baz, bar\nPrior to v6.1.0, pip made no commitments about install order.\nThe decision to install topologically is based on the principle that\ninstallations should proceed in a way that leaves the environment usable at each\nstep. This has two main practical benefits:\nConcurrent use of the environment during the install is more likely to work.\nA failed install is less likely to leave a broken environment.  Although pip\nwould like to support failure rollbacks eventually, in the mean time, this is\nan improvement.\nAlthough the new install order is not intended to replace (and does not replace)\nthe use of\nsetup_requires\nto declare build dependencies, it may help certain\nprojects install from sdist (that might previously fail) that fit the following\nprofile:\nThey have build dependencies that are also declared as install dependencies\nusing\ninstall_requires\n.\npython\nsetup.py\negg_info\nworks without their build dependencies being\ninstalled.\nFor whatever reason, they don’t or won’t declare their build dependencies using\nsetup_requires\n.\nRequirements File Format\nThis section has been moved to\nRequirements File Format\n.\nRequirement Specifiers\nThis section has been moved to\nRequirement Specifiers\n.\nPer-requirement Overrides\nThis is now covered in\nRequirements File Format\n.\nPre-release Versions\n¶\nStarting with v1.4, pip will only install stable versions as specified by\npre-releases\nby default. If a version cannot be parsed as a\ncompliant\nversion then it is assumed to be\na pre-release.\nIf a Requirement specifier includes a pre-release or development version\n(e.g.\n>=0.0.dev0\n) then pip will allow pre-release and development versions\nfor that requirement. This does not include the != flag.\nThe\npip\ninstall\ncommand also supports a\n--pre\nflag\nthat enables installation of pre-releases and development releases.\nVCS Support\nThis is now covered in\nVCS Support\n.\nFinding Packages\n¶\npip searches for packages on\nPyPI\nusing the\nHTTP simple interface\n,\nwhich is documented\nhere\nand\nthere\n.\npip offers a number of package index options for modifying how packages are\nfound.\npip looks for packages in a number of places: on PyPI (or the index given as\n--index-url\n, if not disabled via\n--no-index\n), in the local filesystem,\nand in any additional repositories specified via\n--find-links\nor\n--extra-index-url\n. There is no priority in the locations that are searched.\nRather they are all checked, and the “best” match for the requirements (in\nterms of version number - see the\nspecification\nfor details) is selected.\nSee the\npip install Examples\n.\nSSL Certificate Verification\nThis is now covered in\nHTTPS Certificates\n.\nCaching\nThis is now covered in\nCaching\n.\nWheel Cache\nThis is now covered in\nCaching\n.\nHash checking mode\nThis is now covered in\nSecure installs\n.\nLocal Project Installs\nThis is now covered in\nLocal project installs\n.\nEditable installs\nThis is now covered in\nLocal project installs\n.\nBuild System Interface\nThis is now covered in\nBuild System Interface\n.\nOptions\n¶\n-r\n,\n--requirement\n<file>\n¶\nInstall from the given requirements file. This option can be used multiple times.\n(environment variable:\nPIP_REQUIREMENT\n)\n-c\n,\n--constraint\n<file>\n¶\nConstrain versions using the given constraints file. This option can be used multiple times.\n(environment variable:\nPIP_CONSTRAINT\n)\n--build-constraint\n<file>\n¶\nConstrain build dependencies using the given constraints file. This option can be used multiple times.\n(environment variable:\nPIP_BUILD_CONSTRAINT\n)\n--no-deps\n¶\nDon’t install package dependencies.\n(environment variable:\nPIP_NO_DEPS\n,\nPIP_NO_DEPENDENCIES\n)\n--pre\n¶\nInclude pre-release and development versions. By default, pip only finds stable versions.\n(environment variable:\nPIP_PRE\n)\n-e\n,\n--editable\n<path/url>\n¶\nInstall a project in editable mode (i.e. setuptools “develop mode”) from a local project path or a VCS url.\n(environment variable:\nPIP_EDITABLE\n)\n--dry-run\n¶\nDon’t actually install anything, just print what would be. Can be used in combination with --ignore-installed to ‘resolve’ the requirements.\n(environment variable:\nPIP_DRY_RUN\n)\n-t\n,\n--target\n<dir>\n¶\nInstall packages into <dir>. By default this will not replace existing files/folders in <dir>. Use --upgrade to replace existing packages in <dir> with new versions.\n(environment variable:\nPIP_TARGET\n)\n--platform\n<platform>\n¶\nOnly use wheels compatible with <platform>. Defaults to the platform of the running system. Use this option multiple times to specify multiple platforms supported by the target interpreter.\n(environment variable:\nPIP_PLATFORM\n)\n--python-version\n<python_version>\n¶\nThe Python interpreter version to use for wheel and “Requires-Python”\ncompatibility checks. Defaults to a version derived from the running\ninterpreter. The version can be specified using up to three dot-separated\nintegers (e.g. “3” for 3.0.0, “3.7” for 3.7.0, or “3.7.3”). A major-minor\nversion can also be given as a string without dots (e.g. “37” for 3.7.0).\n(environment variable:\nPIP_PYTHON_VERSION\n)\n--implementation\n<implementation>\n¶\nOnly use wheels compatible with Python implementation <implementation>, e.g. ‘pp’, ‘jy’, ‘cp’,  or ‘ip’. If not specified, then the current interpreter implementation is used.  Use ‘py’ to force implementation-agnostic wheels.\n(environment variable:\nPIP_IMPLEMENTATION\n)\n--abi\n<abi>\n¶\nOnly use wheels compatible with Python abi <abi>, e.g. ‘pypy_41’. If not specified, then the current interpreter abi tag is used. Use this option multiple times to specify multiple abis supported by the target interpreter. Generally you will need to specify --implementation, --platform, and --python-version when using this option.\n(environment variable:\nPIP_ABI\n)\n--user\n¶\nInstall to the Python user install directory for your platform. Typically ~/.local/, or %APPDATA%Python on Windows. (See the Python documentation for site.USER_BASE for full details.)\n(environment variable:\nPIP_USER\n)\n--root\n<dir>\n¶\nInstall everything relative to this alternate root directory.\n(environment variable:\nPIP_ROOT\n)\n--prefix\n<dir>\n¶\nInstallation prefix where lib, bin and other top-level folders are placed. Note that the resulting installation may contain scripts and other resources which reference the Python interpreter of pip, and not that of\n--prefix\n. See also the\n--python\noption if the intention is to install packages into another (possibly pip-free) environment.\n(environment variable:\nPIP_PREFIX\n)\n--src\n<dir>\n¶\nDirectory to check out editable projects into. The default in a virtualenv is “<venv path>/src”. The default for global installs is “<current dir>/src”.\n(environment variable:\nPIP_SRC\n,\nPIP_SOURCE\n,\nPIP_SOURCE_DIR\n,\nPIP_SOURCE_DIRECTORY\n)\n-U\n,\n--upgrade\n¶\nUpgrade all specified packages to the newest available version. The handling of dependencies depends on the upgrade-strategy used.\n(environment variable:\nPIP_UPGRADE\n)\n--upgrade-strategy\n<upgrade_strategy>\n¶\nDetermines how dependency upgrading should be handled [default: only-if-needed]. “eager” - dependencies are upgraded regardless of whether the currently installed version satisfies the requirements of the upgraded package(s). “only-if-needed” -  are upgraded only when they do not satisfy the requirements of the upgraded package(s).\n(environment variable:\nPIP_UPGRADE_STRATEGY\n)\n--force-reinstall\n¶\nReinstall all packages even if they are already up-to-date.\n(environment variable:\nPIP_FORCE_REINSTALL\n)\n-I\n,\n--ignore-installed\n¶\nIgnore the installed packages, overwriting them. This can break your system if the existing package is of a different version or was installed with a different package manager!\n(environment variable:\nPIP_IGNORE_INSTALLED\n)\n--ignore-requires-python\n¶\nIgnore the Requires-Python information.\n(environment variable:\nPIP_IGNORE_REQUIRES_PYTHON\n)\n--no-build-isolation\n¶\nDisable isolation when building a modern source distribution. Build dependencies specified by PEP 518 must be already installed if this option is used.\n(environment variable:\nPIP_NO_BUILD_ISOLATION\n)\n--check-build-dependencies\n¶\nCheck the build dependencies.\n(environment variable:\nPIP_CHECK_BUILD_DEPENDENCIES\n)\n--break-system-packages\n¶\nAllow pip to modify an EXTERNALLY-MANAGED Python installation\n(environment variable:\nPIP_BREAK_SYSTEM_PACKAGES\n)\n-C\n,\n--config-settings\n<settings>\n¶\nConfiguration settings to be passed to the build backend. Settings take the form KEY=VALUE. Use multiple --config-settings options to pass multiple keys to the backend.\n(environment variable:\nPIP_CONFIG_SETTINGS\n)\n--compile\n¶\nCompile Python source files to bytecode\n(environment variable:\nPIP_COMPILE\n)\n--no-compile\n¶\nDo not compile Python source files to bytecode\n(environment variable:\nPIP_NO_COMPILE\n)\n--no-warn-script-location\n¶\nDo not warn when installing scripts outside PATH\n(environment variable:\nPIP_NO_WARN_SCRIPT_LOCATION\n)\n--no-warn-conflicts\n¶\nDo not warn about broken dependencies\n(environment variable:\nPIP_NO_WARN_CONFLICTS\n)\n--no-binary\n<format_control>\n¶\nDo not use binary packages. Can be supplied multiple times, and each time adds to the existing value. Accepts either “:all:” to disable all binary packages, “:none:” to empty the set (notice the colons), or one or more package names with commas between them (no colons). Note that some packages are tricky to compile and may fail to install when this option is used on them.\n(environment variable:\nPIP_NO_BINARY\n)\n--only-binary\n<format_control>\n¶\nDo not use source packages. Can be supplied multiple times, and each time adds to the existing value. Accepts either “:all:” to disable all source packages, “:none:” to empty the set, or one or more package names with commas between them. Packages without binary distributions will fail to install when this option is used on them.\n(environment variable:\nPIP_ONLY_BINARY\n)\n--prefer-binary\n¶\nPrefer binary packages over source packages, even if the source packages are newer.\n(environment variable:\nPIP_PREFER_BINARY\n)\n--require-hashes\n¶\nRequire a hash to check each requirement against, for repeatable installs. This option is implied when any package in a requirements file has a --hash option.\n(environment variable:\nPIP_REQUIRE_HASHES\n)\n--progress-bar\n<progress_bar>\n¶\nSpecify whether the progress bar should be used. In ‘auto’ mode, --quiet will suppress all progress bars. [auto, on, off, raw] (default: auto)\n(environment variable:\nPIP_PROGRESS_BAR\n)\n--root-user-action\n<root_user_action>\n¶\nAction if pip is run as a root user [warn, ignore] (default: warn)\n(environment variable:\nPIP_ROOT_USER_ACTION\n)\n--report\n<file>\n¶\nGenerate a JSON file describing what pip did to install the provided requirements. Can be used in combination with --dry-run and --ignore-installed to ‘resolve’ the requirements. When - is used as file name it writes to stdout. When writing to stdout, please combine with the --quiet option to avoid mixing pip logging output with JSON output.\n(environment variable:\nPIP_REPORT\n)\n--group\n<[path:]group>\n¶\nInstall a named dependency-group from a “pyproject.toml” file. If a path is given, the name of the file must be “pyproject.toml”. Defaults to using “pyproject.toml” in the current directory.\n(environment variable:\nPIP_GROUP\n)\n--no-clean\n¶\nDon’t clean up build directories.\n(environment variable:\nPIP_NO_CLEAN\n)\n-i\n,\n--index-url\n<url>\n¶\nBase URL of the Python Package Index (default\nhttps://pypi.org/simple\n). This should point to a repository compliant with PEP 503 (the simple repository API) or a local directory laid out in the same format.\n(environment variable:\nPIP_INDEX_URL\n,\nPIP_PYPI_URL\n)\n--extra-index-url\n<url>\n¶\nExtra URLs of package indexes to use in addition to --index-url. Should follow the same rules as --index-url.\n(environment variable:\nPIP_EXTRA_INDEX_URL\n)\n--no-index\n¶\nIgnore package index (only looking at --find-links URLs instead).\n(environment variable:\nPIP_NO_INDEX\n)\n-f\n,\n--find-links\n<url>\n¶\nIf a URL or path to an html file, then parse for links to archives such as sdist (.tar.gz) or wheel (.whl) files. If a local path or\nfile://\nURL that’s a directory, then look for archives in the directory listing. Links to VCS project URLs are not supported.\n(environment variable:\nPIP_FIND_LINKS\n)\nExamples\n¶\nInstall\nSomePackage\nand its dependencies from\nPyPI\nusing\nRequirement Specifiers\nUnix/macOS\npython\n-m\npip\ninstall\nSomePackage\n# latest version\npython\n-m\npip\ninstall\n'SomePackage==1.0.4'\n# specific version\npython\n-m\npip\ninstall\n'SomePackage>=1.0.4'\n# minimum version\nWindows\npy\n-m\npip\ninstall\nSomePackage\n# latest version\npy\n-m\npip\ninstall\n\"SomePackage==1.0.4\"\n# specific version\npy\n-m\npip\ninstall\n\"SomePackage>=1.0.4\"\n# minimum version\nInstall a list of requirements specified in a file.  See the\nRequirements files\n.\nUnix/macOS\npython\n-m\npip\ninstall\n-r\nrequirements.txt\nWindows\npy\n-m\npip\ninstall\n-r\nrequirements.txt\nUpgrade an already installed\nSomePackage\nto the latest from PyPI.\nUnix/macOS\npython\n-m\npip\ninstall\n--upgrade\nSomePackage\nWindows\npy\n-m\npip\ninstall\n--upgrade\nSomePackage\nNote\nThis will guarantee an update to\nSomePackage\nas it is a direct\nrequirement, and possibly upgrade dependencies if their installed\nversions do not meet the minimum requirements of\nSomePackage\n.\nAny non-requisite updates of its dependencies (indirect requirements)\nwill be affected by the\n--upgrade-strategy\ncommand.\nInstall a local project in “editable” mode. See the section on\nEditable Installs\n.\nUnix/macOS\npython\n-m\npip\ninstall\n-e\n.\n# project in current directory\npython\n-m\npip\ninstall\n-e\npath/to/project\n# project in another directory\nWindows\npy\n-m\npip\ninstall\n-e\n.\n# project in current directory\npy\n-m\npip\ninstall\n-e\npath/to/project\n# project in another directory\nInstall a project from VCS\nUnix/macOS\npython\n-m\npip\ninstall\n'SomeProject@git+https://git.repo/some_pkg.git@1.3.1'\nWindows\npy\n-m\npip\ninstall\n\"SomeProject@git+https://git.repo/some_pkg.git@1.3.1\"\nInstall a project from VCS in “editable” mode. See the sections on\nVCS Support\nand\nEditable Installs\n.\nUnix/macOS\npython\n-m\npip\ninstall\n-e\n'SomePackage @ git+https://git.repo/some_pkg.git'\n# from git\npython\n-m\npip\ninstall\n-e\n'SomePackage @ hg+https://hg.repo/some_pkg.git'\n# from mercurial\npython\n-m\npip\ninstall\n-e\n'SomePakcage @ svn+svn://svn.repo/some_pkg/trunk/'\n# from svn\npython\n-m\npip\ninstall\n-e\n'SomePackage @ git+https://git.repo/some_pkg.git@feature'\n# from 'feature' branch\npython\n-m\npip\ninstall\n-e\n'SomePackage @ git+https://git.repo/some_repo.git#subdirectory=subdir_path'\n# install a python package from a repo subdirectory\nWindows\npy\n-m\npip\ninstall\n-e\n\"SomePackage @ git+https://git.repo/some_pkg.git\"\n# from git\npy\n-m\npip\ninstall\n-e\n\"SomePackage @ hg+https://hg.repo/some_pkg.git\"\n# from mercurial\npy\n-m\npip\ninstall\n-e\n\"SomePackage @ svn+svn://svn.repo/some_pkg/trunk/\"\n# from svn\npy\n-m\npip\ninstall\n-e\n\"SomePackage @ git+https://git.repo/some_pkg.git@feature\"\n# from 'feature' branch\npy\n-m\npip\ninstall\n-e\n\"SomePackage @ git+https://git.repo/some_repo.git#subdirectory=subdir_path\"\n# install a python package from a repo subdirectory\nInstall a package with extras, i.e., optional dependencies\n(\nspecification\n).\nUnix/macOS\npython\n-m\npip\ninstall\n'SomePackage[PDF]'\npython\n-m\npip\ninstall\n'SomePackage[PDF] @ git+https://git.repo/SomePackage@main#subdirectory=subdir_path'\npython\n-m\npip\ninstall\n'.[PDF]'\n# project in current directory\npython\n-m\npip\ninstall\n'SomePackage[PDF]==3.0'\npython\n-m\npip\ninstall\n'SomePackage[PDF,EPUB]'\n# multiple extras\nWindows\npy\n-m\npip\ninstall\n\"SomePackage[PDF]\"\npy\n-m\npip\ninstall\n\"SomePackage[PDF] @ git+https://git.repo/SomePackage@main#subdirectory=subdir_path\"\npy\n-m\npip\ninstall\n\".[PDF]\"\n# project in current directory\npy\n-m\npip\ninstall\n\"SomePackage[PDF]==3.0\"\npy\n-m\npip\ninstall\n\"SomePackage[PDF,EPUB]\"\n# multiple extras\nInstall a particular source archive file.\nUnix/macOS\npython\n-m\npip\ninstall\n'./downloads/SomePackage-1.0.4.tar.gz'\npython\n-m\npip\ninstall\n'http://my.package.repo/SomePackage-1.0.4.zip'\nWindows\npy\n-m\npip\ninstall\n\"./downloads/SomePackage-1.0.4.tar.gz\"\npy\n-m\npip\ninstall\n\"http://my.package.repo/SomePackage-1.0.4.zip\"\nInstall a particular source archive file following direct references\n(\nspecification\n).\nUnix/macOS\npython\n-m\npip\ninstall\n'SomeProject@http://my.package.repo/SomeProject-1.2.3-py33-none-any.whl'\npython\n-m\npip\ninstall\n'SomeProject @ http://my.package.repo/SomeProject-1.2.3-py33-none-any.whl'\npython\n-m\npip\ninstall\n'SomeProject@http://my.package.repo/1.2.3.tar.gz'\nWindows\npy\n-m\npip\ninstall\n\"SomeProject@http://my.package.repo/SomeProject-1.2.3-py33-none-any.whl\"\npy\n-m\npip\ninstall\n\"SomeProject @ http://my.package.repo/SomeProject-1.2.3-py33-none-any.whl\"\npy\n-m\npip\ninstall\n\"SomeProject@http://my.package.repo/1.2.3.tar.gz\"\nInstall from alternative package repositories.\nInstall from a different index, and not\nPyPI\nUnix/macOS\npython\n-m\npip\ninstall\n--index-url\nhttp://my.package.repo/simple/\nSomePackage\nWindows\npy\n-m\npip\ninstall\n--index-url\nhttp://my.package.repo/simple/\nSomePackage\nInstall from a local flat directory containing archives (and don’t scan indexes):\nUnix/macOS\npython\n-m\npip\ninstall\n--no-index\n--find-links\n=\nfile:///local/dir/\nSomePackage\npython\n-m\npip\ninstall\n--no-index\n--find-links\n=\n/local/dir/\nSomePackage\npython\n-m\npip\ninstall\n--no-index\n--find-links\n=\nrelative/dir/\nSomePackage\nWindows\npy\n-m\npip\ninstall\n--no-index\n--find-links\n=\nfile:///local/dir/\nSomePackage\npy\n-m\npip\ninstall\n--no-index\n--find-links\n=\n/local/dir/\nSomePackage\npy\n-m\npip\ninstall\n--no-index\n--find-links\n=\nrelative/dir/\nSomePackage\nSearch an additional index during install, in addition to\nPyPI\nWarning\nUsing the\n--extra-index-url\noption to search for packages which are\nnot in the main repository (for example, private packages) is unsafe.\nThis is a class of security issue known as\ndependency confusion\n: an\nattacker can publish a package with the same name to a public index,\nwhich may then be chosen instead of your private package.\nUnix/macOS\npython\n-m\npip\ninstall\n--extra-index-url\nhttp://my.package.repo/simple\nSomePackage\nWindows\npy\n-m\npip\ninstall\n--extra-index-url\nhttp://my.package.repo/simple\nSomePackage\nFind pre-release and development versions, in addition to stable versions.  By default, pip only finds stable versions.\nUnix/macOS\npython\n-m\npip\ninstall\n--pre\nSomePackage\nWindows\npy\n-m\npip\ninstall\n--pre\nSomePackage\nInstall packages from source.\nDo not use any binary packages\nUnix/macOS\npython\n-m\npip\ninstall\nSomePackage1\nSomePackage2\n--no-binary\n:all:\nWindows\npy\n-m\npip\ninstall\nSomePackage1\nSomePackage2\n--no-binary\n:all:\nSpecify\nSomePackage1\nto be installed from source:\nUnix/macOS\npython\n-m\npip\ninstall\nSomePackage1\nSomePackage2\n--no-binary\nSomePackage1\nWindows\npy\n-m\npip\ninstall\nSomePackage1\nSomePackage2\n--no-binary\nSomePackage1\nOn this page\npip install\nUsage\nDescription\nOverview\nArgument Handling\nWorking Out the Name and Version\nSatisfying Requirements\nObtaining information about what was installed\nInstallation Order\nPre-release Versions\nFinding Packages\nOptions\nExamples", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://pip.pypa.io/en/stable/cli/pip_install/"}}
{"text": "pip freeze - pip documentation v25.3\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\npip documentation v25.3\npip documentation v25.3\nGetting Started\nInstallation\nUser Guide\nTopic Guides\nAuthentication\nCaching\nConfiguration\nDependency Resolution\nMore on Dependency Resolution\nHTTPS Certificates\nLocal project installs\nRepeatable Installs\nSecure installs\nVCS Support\nManaging a different Python interpreter\nPip is not a workflow management tool\nReference\nBuild System Interface\nRequirement Specifiers\nRequirements File Format\nInstallation Report\npip\ninspect\nJSON output specification\nCommands\npip\npip install\npip uninstall\npip inspect\npip list\npip show\npip freeze\npip check\npip lock\npip download\npip wheel\npip hash\npip search\npip index\npip cache\npip config\npip debug\nProject\nDevelopment\nGetting Started\nContributing\nContinuous Integration\nIssue Triage\nArchitecture of pip’s internals\nBroad functionality overview\nRepository anatomy & directory structure\nConfiguration File Handling\nFinding and choosing files (\nindex\nand\nPackageFinder\n)\nCommand Line Interface\nOptions that control the installation process\nRelease process\nVendoring Policy\nUX Research & Design\nHow to Contribute\nUX Guidance\nUX Research Results\nAbout pip’s Users\nHow Users Understand pip\nHow pip users think about security\nHow pip is used in interactive environments (i.e. CI, CD)\npip Personas\nPrioritizing pip Features\nProviding an override to install packages with conflicting dependencies\npip --force-reinstall\npip search\npip Upgrade Conflict\nImproving pip’s Documentation\nChangelog\nCode of Conduct\nGitHub\nBack to top\nView this page\npip freeze\n¶\nUsage\n¶\nUnix/macOS\npython\n-\nm\npip\nfreeze\n[\noptions\n]\nWindows\npy\n-\nm\npip\nfreeze\n[\noptions\n]\nDescription\n¶\nOutput installed packages in requirements format.\npackages are listed in a case-insensitive sorted order.\nNote\nBy default,\npip\nfreeze\nomits bootstrap packaging tools so the output\nfocuses on your project’s dependencies. On Python\n3.11 and earlier\nthis excludes\npip\n,\nsetuptools\n,\nwheel\nand\ndistribute\n; on\nPython\n3.12 and later\nonly\npip\nis excluded. Use\n--all\nto\ninclude those packages when you need a complete environment snapshot.\npip\nfreeze\nreports what is installed; it does\nnot\ncompute a\nlockfile or a solver result.\nOptions\n¶\n-r\n,\n--requirement\n<file>\n¶\nUse the order in the given requirements file and its comments when generating output. This option can be used multiple times.\n(environment variable:\nPIP_REQUIREMENT\n)\n-l\n,\n--local\n¶\nIf in a virtualenv that has global access, do not output globally-installed packages.\n(environment variable:\nPIP_LOCAL\n)\n--user\n¶\nOnly output packages installed in user-site.\n(environment variable:\nPIP_USER\n)\n--path\n<path>\n¶\nRestrict to the specified installation path for listing packages (can be used multiple times).\n(environment variable:\nPIP_PATH\n)\n--all\n¶\nDo not skip these packages in the output: setuptools, pip, wheel, distribute\n(environment variable:\nPIP_ALL\n)\n--exclude-editable\n¶\nExclude editable package from output.\n(environment variable:\nPIP_EXCLUDE_EDITABLE\n)\n--exclude\n<package>\n¶\nExclude specified package from the output\n(environment variable:\nPIP_EXCLUDE\n)\nExamples\n¶\nGenerate output suitable for a requirements file.\nUnix/macOS\n$\npython\n-m\npip\nfreeze\ndocutils==0.11\nJinja2==2.7.2\nMarkupSafe==0.19\nPygments==1.6\nSphinx==1.2.2\nWindows\nC:\\> py -m pip freeze\ndocutils==0.11\nJinja2==2.7.2\nMarkupSafe==0.19\nPygments==1.6\nSphinx==1.2.2\nGenerate a requirements file and then install from it in another environment.\nUnix/macOS\nenv1/bin/python\n-m\npip\nfreeze\n>\nrequirements.txt\nenv2/bin/python\n-m\npip\ninstall\n-r\nrequirements.txt\nWindows\nenv1\n\\b\nin\n\\p\nython\n-m\npip\nfreeze\n>\nrequirements.txt\nenv2\n\\b\nin\n\\p\nython\n-m\npip\ninstall\n-r\nrequirements.txt\nFixing “Permission denied:” errors\n¶\nThe purpose of this section of documentation is to provide practical\nsuggestions to users seeing a\n“Permission denied” error\non\npip\nfreeze\n.\nThis error occurs, for instance, when the command is installed only for another\nuser, and the current user doesn’t have the permission to execute the other\nuser’s command.\nTo solve that issue, you can try one of the following:\nInstall the command for yourself (e.g. in your home directory).\nAsk the system admin to allow this command for all users.\nCheck and correct the PATH variable of your own environment.\nCheck the\nACL (Access-Control List)\nfor this command.\nOn this page\npip freeze\nUsage\nDescription\nOptions\nExamples\nFixing “Permission denied:” errors", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://pip.pypa.io/en/stable/cli/pip_freeze/"}}
{"text": "venv â Creation of virtual environments — Python 3.14.0 documentation\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.14.0 Documentation\n»\nThe Python Standard Library\n»\nSoftware Packaging and Distribution\n»\nvenv\nâ Creation of virtual environments\n|\nTheme\nAuto\nLight\nDark\n|\nvenv\nâ Creation of virtual environments\nÂ¶\nAdded in version 3.3.\nSource code:\nLib/venv/\nThe\nvenv\nmodule supports creating lightweight âvirtual environmentsâ,\neach with their own independent set of Python packages installed in\ntheir\nsite\ndirectories.\nA virtual environment is created on top of an existing\nPython installation, known as the virtual environmentâs âbaseâ Python, and by\ndefault is isolated from the packages in the base environment,\nso that only those explicitly installed in the virtual environment are\navailable. See\nVirtual Environments\nand\nsite\nâs\nvirtual environments documentation\nfor more information.\nWhen used from within a virtual environment, common installation tools such as\npip\nwill install Python packages into a virtual environment\nwithout needing to be told to do so explicitly.\nA virtual environment is (amongst other things):\nUsed to contain a specific Python interpreter and software libraries and\nbinaries which are needed to support a project (library or application). These\nare by default isolated from software in other virtual environments and Python\ninterpreters and libraries installed in the operating system.\nContained in a directory, conventionally named\n.venv\nor\nvenv\nin\nthe project directory, or under a container directory for lots of virtual\nenvironments, such as\n~/.virtualenvs\n.\nNot checked into source control systems such as Git.\nConsidered as disposable â it should be simple to delete and recreate it from\nscratch. You donât place any project code in the environment.\nNot considered as movable or copyable â you just recreate the same\nenvironment in the target location.\nSee\nPEP 405\nfor more background on Python virtual environments.\nSee also\nPython Packaging User Guide: Creating and using virtual environments\nAvailability\n: not Android, not iOS, not WASI.\nThis module is not supported on\nmobile platforms\nor\nWebAssembly platforms\n.\nCreating virtual environments\nÂ¶\nVirtual environments\nare created by executing the\nvenv\nmodule:\npython\n-m\nvenv\n/path/to/new/virtual/environment\nThis creates the target directory (including parent directories as needed)\nand places a\npyvenv.cfg\nfile in it with a\nhome\nkey\npointing to the Python installation from which the command was run.\nIt also creates a\nbin\n(or\nScripts\non Windows) subdirectory\ncontaining a copy or symlink of the Python executable\n(as appropriate for the platform or arguments used at environment creation time).\nIt also creates a\nlib/pythonX.Y/site-packages\nsubdirectory\n(on Windows, this is\nLib\\site-packages\n).\nIf an existing directory is specified, it will be re-used.\nChanged in version 3.5:\nThe use of\nvenv\nis now recommended for creating virtual environments.\nDeprecated since version 3.6, removed in version 3.8:\npyvenv\nwas the recommended tool for creating virtual environments\nfor Python 3.3 and 3.4, and replaced in 3.5 by executing\nvenv\ndirectly.\nOn Windows, invoke the\nvenv\ncommand as follows:\nPS>\npython\n-m\nvenv\nC\n:\\\npath\n\\\nto\n\\\nnew\n\\\nvirtual\n\\\nenvironment\nThe command, if run with\n-h\n, will show the available options:\nusage: venv [-h] [--system-site-packages] [--symlinks | --copies] [--clear]\n[--upgrade] [--without-pip] [--prompt PROMPT] [--upgrade-deps]\n[--without-scm-ignore-files]\nENV_DIR [ENV_DIR ...]\nCreates virtual Python environments in one or more target directories.\nOnce an environment has been created, you may wish to activate it, e.g. by\nsourcing an activate script in its bin directory.\nENV_DIR\nÂ¶\nA required argument specifying the directory to create the environment in.\n--system-site-packages\nÂ¶\nGive the virtual environment access to the system site-packages directory.\n--symlinks\nÂ¶\nTry to use symlinks rather than copies, when symlinks are not the default for the platform.\n--copies\nÂ¶\nTry to use copies rather than symlinks, even when symlinks are the default for the platform.\n--clear\nÂ¶\nDelete the contents of the environment directory if it already exists, before environment creation.\n--upgrade\nÂ¶\nUpgrade the environment directory to use this version of Python, assuming Python has been upgraded in-place.\n--without-pip\nÂ¶\nSkips installing or upgrading pip in the virtual environment (pip is bootstrapped by default).\n--prompt\n<PROMPT>\nÂ¶\nProvides an alternative prompt prefix for this environment.\n--upgrade-deps\nÂ¶\nUpgrade core dependencies (pip) to the latest version in PyPI.\n--without-scm-ignore-files\nÂ¶\nSkips adding SCM ignore files to the environment directory (Git is supported by default).\nChanged in version 3.4:\nInstalls pip by default, added the\n--without-pip\nand\n--copies\noptions.\nChanged in version 3.4:\nIn earlier versions, if the target directory already existed, an error was\nraised, unless the\n--clear\nor\n--upgrade\noption was provided.\nChanged in version 3.9:\nAdd\n--upgrade-deps\noption to upgrade pip + setuptools to the latest on PyPI.\nChanged in version 3.12:\nsetuptools\nis no longer a core venv dependency.\nChanged in version 3.13:\nAdded the\n--without-scm-ignore-files\noption.\nChanged in version 3.13:\nvenv\nnow creates a\n.gitignore\nfile for Git by default.\nNote\nWhile symlinks are supported on Windows, they are not recommended. Of\nparticular note is that double-clicking\npython.exe\nin File Explorer\nwill resolve the symlink eagerly and ignore the virtual environment.\nNote\nOn Microsoft Windows, it may be required to enable the\nActivate.ps1\nscript by setting the execution policy for the user. You can do this by\nissuing the following PowerShell command:\nPS\nC\n:\\>\nSet-ExecutionPolicy\n-ExecutionPolicy\nRemoteSigned\n-Scope\nCurrentUser\nSee\nAbout Execution Policies\nfor more information.\nThe created\npyvenv.cfg\nfile also includes the\ninclude-system-site-packages\nkey, set to\ntrue\nif\nvenv\nis\nrun with the\n--system-site-packages\noption,\nfalse\notherwise.\nUnless the\n--without-pip\noption is given,\nensurepip\nwill be\ninvoked to bootstrap\npip\ninto the virtual environment.\nMultiple paths can be given to\nvenv\n, in which case an identical virtual\nenvironment will be created, according to the given options, at each provided\npath.\nHow venvs work\nÂ¶\nWhen a Python interpreter is running from a virtual environment,\nsys.prefix\nand\nsys.exec_prefix\npoint to the directories of the virtual environment,\nwhereas\nsys.base_prefix\nand\nsys.base_exec_prefix\npoint to those of the base Python used to create the environment.\nIt is sufficient to check\nsys.prefix\n!=\nsys.base_prefix\nto determine if the current interpreter is\nrunning from a virtual environment.\nA virtual environment may be âactivatedâ using a script in its binary directory\n(\nbin\non POSIX;\nScripts\non Windows).\nThis will prepend that directory to your\nPATH\n, so that running\npython\nwill invoke the environmentâs Python interpreter\nand you can run installed scripts without having to use their full path.\nThe invocation of the activation script is platform-specific\n(\n<venv>\nmust be replaced by the path to the directory\ncontaining the virtual environment):\nPlatform\nShell\nCommand to activate virtual environment\nPOSIX\nbash/zsh\n$\nsource\n<venv>\n/bin/activate\nfish\n$\nsource\n<venv>\n/bin/activate.fish\ncsh/tcsh\n$\nsource\n<venv>\n/bin/activate.csh\npwsh\n$\n<venv>\n/bin/Activate.ps1\nWindows\ncmd.exe\nC:\\>\n<venv>\n\\Scripts\\activate.bat\nPowerShell\nPS\nC:\\>\n<venv>\n\\Scripts\\Activate.ps1\nAdded in version 3.4:\nfish\nand\ncsh\nactivation scripts.\nAdded in version 3.8:\nPowerShell activation scripts installed under POSIX for PowerShell Core\nsupport.\nYou donât specifically\nneed\nto activate a virtual environment,\nas you can just specify the full path to that environmentâs\nPython interpreter when invoking Python.\nFurthermore, all scripts installed in the environment\nshould be runnable without activating it.\nIn order to achieve this, scripts installed into virtual environments have\na âshebangâ line which points to the environmentâs Python interpreter,\n#!/\n<path-to-venv>\n/bin/python\n.\nThis means that the script will run with that interpreter regardless of the\nvalue of\nPATH\n. On Windows, âshebangâ line processing is supported if\nyou have the\nPython install manager\ninstalled. Thus, double-clicking an installed\nscript in a Windows Explorer window should run it with the correct interpreter\nwithout the environment needing to be activated or on the\nPATH\n.\nWhen a virtual environment has been activated, the\nVIRTUAL_ENV\nenvironment variable is set to the path of the environment.\nSince explicitly activating a virtual environment is not required to use it,\nVIRTUAL_ENV\ncannot be relied upon to determine\nwhether a virtual environment is being used.\nWarning\nBecause scripts installed in environments should not expect the\nenvironment to be activated, their shebang lines contain the absolute paths\nto their environmentâs interpreters. Because of this, environments are\ninherently non-portable, in the general case. You should always have a\nsimple means of recreating an environment (for example, if you have a\nrequirements file\nrequirements.txt\n, you can invoke\npip\ninstall\n-r\nrequirements.txt\nusing the environmentâs\npip\nto install all of the\npackages needed by the environment). If for any reason you need to move the\nenvironment to a new location, you should recreate it at the desired\nlocation and delete the one at the old location. If you move an environment\nbecause you moved a parent directory of it, you should recreate the\nenvironment in its new location. Otherwise, software installed into the\nenvironment may not work as expected.\nYou can deactivate a virtual environment by typing\ndeactivate\nin your shell.\nThe exact mechanism is platform-specific and is an internal implementation\ndetail (typically, a script or shell function will be used).\nAPI\nÂ¶\nThe high-level method described above makes use of a simple API which provides\nmechanisms for third-party virtual environment creators to customize environment\ncreation according to their needs, the\nEnvBuilder\nclass.\nclass\nvenv.\nEnvBuilder\n(\nsystem_site_packages\n=\nFalse\n,\nclear\n=\nFalse\n,\nsymlinks\n=\nFalse\n,\nupgrade\n=\nFalse\n,\nwith_pip\n=\nFalse\n,\nprompt\n=\nNone\n,\nupgrade_deps\n=\nFalse\n,\n*\n,\nscm_ignore_files\n=\nfrozenset()\n)\nÂ¶\nThe\nEnvBuilder\nclass accepts the following keyword arguments on\ninstantiation:\nsystem_site_packages\nâ a boolean value indicating that the system Python\nsite-packages should be available to the environment (defaults to\nFalse\n).\nclear\nâ a boolean value which, if true, will delete the contents of\nany existing target directory, before creating the environment.\nsymlinks\nâ a boolean value indicating whether to attempt to symlink the\nPython binary rather than copying.\nupgrade\nâ a boolean value which, if true, will upgrade an existing\nenvironment with the running Python - for use when that Python has been\nupgraded in-place (defaults to\nFalse\n).\nwith_pip\nâ a boolean value which, if true, ensures pip is\ninstalled in the virtual environment. This uses\nensurepip\nwith\nthe\n--default-pip\noption.\nprompt\nâ a string to be used after virtual environment is activated\n(defaults to\nNone\nwhich means directory name of the environment would\nbe used). If the special string\n\".\"\nis provided, the basename of the\ncurrent directory is used as the prompt.\nupgrade_deps\nâ Update the base venv modules to the latest on PyPI\nscm_ignore_files\nâ Create ignore files based for the specified source\ncontrol managers (SCM) in the iterable. Support is defined by having a\nmethod named\ncreate_{scm}_ignore_file\n. The only value supported by\ndefault is\n\"git\"\nvia\ncreate_git_ignore_file()\n.\nChanged in version 3.4:\nAdded the\nwith_pip\nparameter\nChanged in version 3.6:\nAdded the\nprompt\nparameter\nChanged in version 3.9:\nAdded the\nupgrade_deps\nparameter\nChanged in version 3.13:\nAdded the\nscm_ignore_files\nparameter\nEnvBuilder\nmay be used as a base class.\ncreate\n(\nenv_dir\n)\nÂ¶\nCreate a virtual environment by specifying the target directory\n(absolute or relative to the current directory) which is to contain the\nvirtual environment.  The\ncreate\nmethod will either create the\nenvironment in the specified directory, or raise an appropriate\nexception.\nThe\ncreate\nmethod of the\nEnvBuilder\nclass illustrates the\nhooks available for subclass customization:\ndef\ncreate\n(\nself\n,\nenv_dir\n):\n\"\"\"\nCreate a virtualized Python environment in a directory.\nenv_dir is the target directory to create an environment in.\n\"\"\"\nenv_dir\n=\nos\n.\npath\n.\nabspath\n(\nenv_dir\n)\ncontext\n=\nself\n.\nensure_directories\n(\nenv_dir\n)\nself\n.\ncreate_configuration\n(\ncontext\n)\nself\n.\nsetup_python\n(\ncontext\n)\nself\n.\nsetup_scripts\n(\ncontext\n)\nself\n.\npost_setup\n(\ncontext\n)\nEach of the methods\nensure_directories()\n,\ncreate_configuration()\n,\nsetup_python()\n,\nsetup_scripts()\nand\npost_setup()\ncan be overridden.\nensure_directories\n(\nenv_dir\n)\nÂ¶\nCreates the environment directory and all necessary subdirectories that\ndonât already exist, and returns a context object.  This context object\nis just a holder for attributes (such as paths) for use by the other\nmethods.  If the\nEnvBuilder\nis created with the arg\nclear=True\n, contents of the environment directory will be cleared\nand then all necessary subdirectories will be recreated.\nThe returned context object is a\ntypes.SimpleNamespace\nwith the\nfollowing attributes:\nenv_dir\n- The location of the virtual environment. Used for\n__VENV_DIR__\nin activation scripts (see\ninstall_scripts()\n).\nenv_name\n- The name of the virtual environment. Used for\n__VENV_NAME__\nin activation scripts (see\ninstall_scripts()\n).\nprompt\n- The prompt to be used by the activation scripts. Used for\n__VENV_PROMPT__\nin activation scripts (see\ninstall_scripts()\n).\nexecutable\n- The underlying Python executable used by the virtual\nenvironment. This takes into account the case where a virtual environment\nis created from another virtual environment.\ninc_path\n- The include path for the virtual environment.\nlib_path\n- The purelib path for the virtual environment.\nbin_path\n- The script path for the virtual environment.\nbin_name\n- The name of the script path relative to the virtual\nenvironment location. Used for\n__VENV_BIN_NAME__\nin activation\nscripts (see\ninstall_scripts()\n).\nenv_exe\n- The name of the Python interpreter in the virtual\nenvironment. Used for\n__VENV_PYTHON__\nin activation scripts\n(see\ninstall_scripts()\n).\nenv_exec_cmd\n- The name of the Python interpreter, taking into\naccount filesystem redirections. This can be used to run Python in\nthe virtual environment.\nChanged in version 3.11:\nThe\nvenv\nsysconfig installation scheme\nis used to construct the paths of the created directories.\nChanged in version 3.12:\nThe attribute\nlib_path\nwas added to the context, and the context\nobject was documented.\ncreate_configuration\n(\ncontext\n)\nÂ¶\nCreates the\npyvenv.cfg\nconfiguration file in the environment.\nsetup_python\n(\ncontext\n)\nÂ¶\nCreates a copy or symlink to the Python executable in the environment.\nOn POSIX systems, if a specific executable\npython3.x\nwas used,\nsymlinks to\npython\nand\npython3\nwill be created pointing to that\nexecutable, unless files with those names already exist.\nsetup_scripts\n(\ncontext\n)\nÂ¶\nInstalls activation scripts appropriate to the platform into the virtual\nenvironment.\nupgrade_dependencies\n(\ncontext\n)\nÂ¶\nUpgrades the core venv dependency packages (currently\npip\n)\nin the environment. This is done by shelling out to the\npip\nexecutable in the environment.\nAdded in version 3.9.\nChanged in version 3.12:\nsetuptools\nis no longer a core venv dependency.\npost_setup\n(\ncontext\n)\nÂ¶\nA placeholder method which can be overridden in third party\nimplementations to pre-install packages in the virtual environment or\nperform other post-creation steps.\ninstall_scripts\n(\ncontext\n,\npath\n)\nÂ¶\nThis method can be\ncalled from\nsetup_scripts()\nor\npost_setup()\nin subclasses to\nassist in installing custom scripts into the virtual environment.\npath\nis the path to a directory that should contain subdirectories\ncommon\n,\nposix\n,\nnt\n; each containing scripts destined for the\nbin\ndirectory in the environment.  The contents of\ncommon\nand the\ndirectory corresponding to\nos.name\nare copied after some text\nreplacement of placeholders:\n__VENV_DIR__\nis replaced with the absolute path of the environment\ndirectory.\n__VENV_NAME__\nis replaced with the environment name (final path\nsegment of environment directory).\n__VENV_PROMPT__\nis replaced with the prompt (the environment\nname surrounded by parentheses and with a following space)\n__VENV_BIN_NAME__\nis replaced with the name of the bin directory\n(either\nbin\nor\nScripts\n).\n__VENV_PYTHON__\nis replaced with the absolute path of the\nenvironmentâs executable.\nThe directories are allowed to exist (for when an existing environment\nis being upgraded).\ncreate_git_ignore_file\n(\ncontext\n)\nÂ¶\nCreates a\n.gitignore\nfile within the virtual environment that causes\nthe entire directory to be ignored by the Git source control manager.\nAdded in version 3.13.\nChanged in version 3.7.2:\nWindows now uses redirector scripts for\npython[w].exe\ninstead of\ncopying the actual binaries. In 3.7.2 only\nsetup_python()\ndoes\nnothing unless running from a build in the source tree.\nChanged in version 3.7.3:\nWindows copies the redirector scripts as part of\nsetup_python()\ninstead of\nsetup_scripts()\n. This was not the case in 3.7.2.\nWhen using symlinks, the original executables will be linked.\nThere is also a module-level convenience function:\nvenv.\ncreate\n(\nenv_dir\n,\nsystem_site_packages\n=\nFalse\n,\nclear\n=\nFalse\n,\nsymlinks\n=\nFalse\n,\nwith_pip\n=\nFalse\n,\nprompt\n=\nNone\n,\nupgrade_deps\n=\nFalse\n,\n*\n,\nscm_ignore_files\n=\nfrozenset()\n)\nÂ¶\nCreate an\nEnvBuilder\nwith the given keyword arguments, and call its\ncreate()\nmethod with the\nenv_dir\nargument.\nAdded in version 3.3.\nChanged in version 3.4:\nAdded the\nwith_pip\nparameter\nChanged in version 3.6:\nAdded the\nprompt\nparameter\nChanged in version 3.9:\nAdded the\nupgrade_deps\nparameter\nChanged in version 3.13:\nAdded the\nscm_ignore_files\nparameter\nAn example of extending\nEnvBuilder\nÂ¶\nThe following script shows how to extend\nEnvBuilder\nby implementing a\nsubclass which installs setuptools and pip into a created virtual environment:\nimport\nos\nimport\nos.path\nfrom\nsubprocess\nimport\nPopen\n,\nPIPE\nimport\nsys\nfrom\nthreading\nimport\nThread\nfrom\nurllib.parse\nimport\nurlparse\nfrom\nurllib.request\nimport\nurlretrieve\nimport\nvenv\nclass\nExtendedEnvBuilder\n(\nvenv\n.\nEnvBuilder\n):\n\"\"\"\nThis builder installs setuptools and pip so that you can pip or\neasy_install other packages into the created virtual environment.\n:param nodist: If true, setuptools and pip are not installed into the\ncreated virtual environment.\n:param nopip: If true, pip is not installed into the created\nvirtual environment.\n:param progress: If setuptools or pip are installed, the progress of the\ninstallation can be monitored by passing a progress\ncallable. If specified, it is called with two\narguments: a string indicating some progress, and a\ncontext indicating where the string is coming from.\nThe context argument can have one of three values:\n'main', indicating that it is called from virtualize()\nitself, and 'stdout' and 'stderr', which are obtained\nby reading lines from the output streams of a subprocess\nwhich is used to install the app.\nIf a callable is not specified, default progress\ninformation is output to sys.stderr.\n\"\"\"\ndef\n__init__\n(\nself\n,\n*\nargs\n,\n**\nkwargs\n):\nself\n.\nnodist\n=\nkwargs\n.\npop\n(\n'nodist'\n,\nFalse\n)\nself\n.\nnopip\n=\nkwargs\n.\npop\n(\n'nopip'\n,\nFalse\n)\nself\n.\nprogress\n=\nkwargs\n.\npop\n(\n'progress'\n,\nNone\n)\nself\n.\nverbose\n=\nkwargs\n.\npop\n(\n'verbose'\n,\nFalse\n)\nsuper\n()\n.\n__init__\n(\n*\nargs\n,\n**\nkwargs\n)\ndef\npost_setup\n(\nself\n,\ncontext\n):\n\"\"\"\nSet up any packages which need to be pre-installed into the\nvirtual environment being created.\n:param context: The information for the virtual environment\ncreation request being processed.\n\"\"\"\nos\n.\nenviron\n[\n'VIRTUAL_ENV'\n]\n=\ncontext\n.\nenv_dir\nif\nnot\nself\n.\nnodist\n:\nself\n.\ninstall_setuptools\n(\ncontext\n)\n# Can't install pip without setuptools\nif\nnot\nself\n.\nnopip\nand\nnot\nself\n.\nnodist\n:\nself\n.\ninstall_pip\n(\ncontext\n)\ndef\nreader\n(\nself\n,\nstream\n,\ncontext\n):\n\"\"\"\nRead lines from a subprocess' output stream and either pass to a progress\ncallable (if specified) or write progress information to sys.stderr.\n\"\"\"\nprogress\n=\nself\n.\nprogress\nwhile\nTrue\n:\ns\n=\nstream\n.\nreadline\n()\nif\nnot\ns\n:\nbreak\nif\nprogress\nis\nnot\nNone\n:\nprogress\n(\ns\n,\ncontext\n)\nelse\n:\nif\nnot\nself\n.\nverbose\n:\nsys\n.\nstderr\n.\nwrite\n(\n'.'\n)\nelse\n:\nsys\n.\nstderr\n.\nwrite\n(\ns\n.\ndecode\n(\n'utf-8'\n))\nsys\n.\nstderr\n.\nflush\n()\nstream\n.\nclose\n()\ndef\ninstall_script\n(\nself\n,\ncontext\n,\nname\n,\nurl\n):\n_\n,\n_\n,\npath\n,\n_\n,\n_\n,\n_\n=\nurlparse\n(\nurl\n)\nfn\n=\nos\n.\npath\n.\nsplit\n(\npath\n)[\n-\n1\n]\nbinpath\n=\ncontext\n.\nbin_path\ndistpath\n=\nos\n.\npath\n.\njoin\n(\nbinpath\n,\nfn\n)\n# Download script into the virtual environment's binaries folder\nurlretrieve\n(\nurl\n,\ndistpath\n)\nprogress\n=\nself\n.\nprogress\nif\nself\n.\nverbose\n:\nterm\n=\n'\n\\n\n'\nelse\n:\nterm\n=\n''\nif\nprogress\nis\nnot\nNone\n:\nprogress\n(\n'Installing\n%s\n...\n%s\n'\n%\n(\nname\n,\nterm\n),\n'main'\n)\nelse\n:\nsys\n.\nstderr\n.\nwrite\n(\n'Installing\n%s\n...\n%s\n'\n%\n(\nname\n,\nterm\n))\nsys\n.\nstderr\n.\nflush\n()\n# Install in the virtual environment\nargs\n=\n[\ncontext\n.\nenv_exe\n,\nfn\n]\np\n=\nPopen\n(\nargs\n,\nstdout\n=\nPIPE\n,\nstderr\n=\nPIPE\n,\ncwd\n=\nbinpath\n)\nt1\n=\nThread\n(\ntarget\n=\nself\n.\nreader\n,\nargs\n=\n(\np\n.\nstdout\n,\n'stdout'\n))\nt1\n.\nstart\n()\nt2\n=\nThread\n(\ntarget\n=\nself\n.\nreader\n,\nargs\n=\n(\np\n.\nstderr\n,\n'stderr'\n))\nt2\n.\nstart\n()\np\n.\nwait\n()\nt1\n.\njoin\n()\nt2\n.\njoin\n()\nif\nprogress\nis\nnot\nNone\n:\nprogress\n(\n'done.'\n,\n'main'\n)\nelse\n:\nsys\n.\nstderr\n.\nwrite\n(\n'done.\n\\n\n'\n)\n# Clean up - no longer needed\nos\n.\nunlink\n(\ndistpath\n)\ndef\ninstall_setuptools\n(\nself\n,\ncontext\n):\n\"\"\"\nInstall setuptools in the virtual environment.\n:param context: The information for the virtual environment\ncreation request being processed.\n\"\"\"\nurl\n=\n\"https://bootstrap.pypa.io/ez_setup.py\"\nself\n.\ninstall_script\n(\ncontext\n,\n'setuptools'\n,\nurl\n)\n# clear up the setuptools archive which gets downloaded\npred\n=\nlambda\no\n:\no\n.\nstartswith\n(\n'setuptools-'\n)\nand\no\n.\nendswith\n(\n'.tar.gz'\n)\nfiles\n=\nfilter\n(\npred\n,\nos\n.\nlistdir\n(\ncontext\n.\nbin_path\n))\nfor\nf\nin\nfiles\n:\nf\n=\nos\n.\npath\n.\njoin\n(\ncontext\n.\nbin_path\n,\nf\n)\nos\n.\nunlink\n(\nf\n)\ndef\ninstall_pip\n(\nself\n,\ncontext\n):\n\"\"\"\nInstall pip in the virtual environment.\n:param context: The information for the virtual environment\ncreation request being processed.\n\"\"\"\nurl\n=\n'https://bootstrap.pypa.io/get-pip.py'\nself\n.\ninstall_script\n(\ncontext\n,\n'pip'\n,\nurl\n)\ndef\nmain\n(\nargs\n=\nNone\n):\nimport\nargparse\nparser\n=\nargparse\n.\nArgumentParser\n(\nprog\n=\n__name__\n,\ndescription\n=\n'Creates virtual Python '\n'environments in one or '\n'more target '\n'directories.'\n)\nparser\n.\nadd_argument\n(\n'dirs'\n,\nmetavar\n=\n'ENV_DIR'\n,\nnargs\n=\n'+'\n,\nhelp\n=\n'A directory in which to create the '\n'virtual environment.'\n)\nparser\n.\nadd_argument\n(\n'--no-setuptools'\n,\ndefault\n=\nFalse\n,\naction\n=\n'store_true'\n,\ndest\n=\n'nodist'\n,\nhelp\n=\n\"Don't install setuptools or pip in the \"\n\"virtual environment.\"\n)\nparser\n.\nadd_argument\n(\n'--no-pip'\n,\ndefault\n=\nFalse\n,\naction\n=\n'store_true'\n,\ndest\n=\n'nopip'\n,\nhelp\n=\n\"Don't install pip in the virtual \"\n\"environment.\"\n)\nparser\n.\nadd_argument\n(\n'--system-site-packages'\n,\ndefault\n=\nFalse\n,\naction\n=\n'store_true'\n,\ndest\n=\n'system_site'\n,\nhelp\n=\n'Give the virtual environment access to the '\n'system site-packages dir.'\n)\nif\nos\n.\nname\n==\n'nt'\n:\nuse_symlinks\n=\nFalse\nelse\n:\nuse_symlinks\n=\nTrue\nparser\n.\nadd_argument\n(\n'--symlinks'\n,\ndefault\n=\nuse_symlinks\n,\naction\n=\n'store_true'\n,\ndest\n=\n'symlinks'\n,\nhelp\n=\n'Try to use symlinks rather than copies, '\n'when symlinks are not the default for '\n'the platform.'\n)\nparser\n.\nadd_argument\n(\n'--clear'\n,\ndefault\n=\nFalse\n,\naction\n=\n'store_true'\n,\ndest\n=\n'clear'\n,\nhelp\n=\n'Delete the contents of the '\n'virtual environment '\n'directory if it already '\n'exists, before virtual '\n'environment creation.'\n)\nparser\n.\nadd_argument\n(\n'--upgrade'\n,\ndefault\n=\nFalse\n,\naction\n=\n'store_true'\n,\ndest\n=\n'upgrade'\n,\nhelp\n=\n'Upgrade the virtual '\n'environment directory to '\n'use this version of '\n'Python, assuming Python '\n'has been upgraded '\n'in-place.'\n)\nparser\n.\nadd_argument\n(\n'--verbose'\n,\ndefault\n=\nFalse\n,\naction\n=\n'store_true'\n,\ndest\n=\n'verbose'\n,\nhelp\n=\n'Display the output '\n'from the scripts which '\n'install setuptools and pip.'\n)\noptions\n=\nparser\n.\nparse_args\n(\nargs\n)\nif\noptions\n.\nupgrade\nand\noptions\n.\nclear\n:\nraise\nValueError\n(\n'you cannot supply --upgrade and --clear together.'\n)\nbuilder\n=\nExtendedEnvBuilder\n(\nsystem_site_packages\n=\noptions\n.\nsystem_site\n,\nclear\n=\noptions\n.\nclear\n,\nsymlinks\n=\noptions\n.\nsymlinks\n,\nupgrade\n=\noptions\n.\nupgrade\n,\nnodist\n=\noptions\n.\nnodist\n,\nnopip\n=\noptions\n.\nnopip\n,\nverbose\n=\noptions\n.\nverbose\n)\nfor\nd\nin\noptions\n.\ndirs\n:\nbuilder\n.\ncreate\n(\nd\n)\nif\n__name__\n==\n'__main__'\n:\nrc\n=\n1\ntry\n:\nmain\n()\nrc\n=\n0\nexcept\nException\nas\ne\n:\nprint\n(\n'Error:\n%s\n'\n%\ne\n,\nfile\n=\nsys\n.\nstderr\n)\nsys\n.\nexit\n(\nrc\n)\nThis script is also available for download\nonline\n.\nTable of Contents\nvenv\nâ Creation of virtual environments\nCreating virtual environments\nHow venvs work\nAPI\nAn example of extending\nEnvBuilder\nPrevious topic\nensurepip\nâ Bootstrapping the\npip\ninstaller\nNext topic\nzipapp\nâ Manage executable Python zip archives\nThis page\nReport a bug\nShow source\nÂ«\nNavigation\nindex\nmodules\n|\nnext\n|\nprevious\n|\nPython\n»\n3.14.0 Documentation\n»\nThe Python Standard Library\n»\nSoftware Packaging and Distribution\n»\nvenv\nâ Creation of virtual environments\n|\nTheme\nAuto\nLight\nDark\n|\n©\nCopyright\n2001 Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee\nHistory and License\nfor more information.\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\nLast updated on Nov 29, 2025 (04:57 UTC).\nFound a bug\n?\nCreated using\nSphinx\n8.2.3.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.python.org/3/library/venv.html"}}
{"text": "Introduction | Documentation | Poetry - Python dependency management and packaging made easy\nHome\n2.2\nmain\n2.2\n1.8\nUse dark mode\nOpen menu\nClose menu\nDocumentation\nBlog\nHistory\nIntroduction\nIntroduction\nBasic usage\nManaging dependencies\nLibraries\nCommands\nConfiguration\nRepositories\nManaging environments\nDependency specification\nPlugins\nThe pyproject.toml file\nContributing to Poetry\nCommunity\nFAQ\npre-commit hooks\nBuilding extension modules\nIntroduction\nIntroduction\n#\nPoetry is a tool for\ndependency management\nand\npackaging\nin Python.\nIt allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\nPoetry offers a lockfile to ensure repeatable installs, and can build your project for distribution.\nSystem requirements\n#\nPoetry requires\nPython 3.9+\n. It is multi-platform and the goal is to make it work equally well\non Linux, macOS and Windows.\nInstallation\n#\nNote\nIf you are viewing documentation for the development branch, you may wish to install a preview or development version of Poetry.\nSee the\nadvanced\ninstallation instructions to use a preview or alternate version of Poetry.\nWith pipx\nWith pipx\nWith the official installer\nManually (advanced)\nCI recommendations\npipx\nis used to install Python CLI applications globally while still isolating them in virtual environments.\npipx\nwill manage upgrades and uninstalls when used to install Poetry.\nInstall pipx\nIf\npipx\nis not already installed, you can follow any of the options in the\nofficial pipx installation instructions\n.\nAny non-ancient version of\npipx\nwill do.\nInstall Poetry\npipx install poetry\nInstall Poetry (advanced)\nNote\nYou can skip this step, if you simply want the latest version and already installed Poetry as described in the\nprevious step. This step details advanced usages of this installation method. For example, installing Poetry from\nsource, having multiple versions installed at the same time etc.\npipx\ncan install different versions of Poetry, using the same syntax as pip:\npipx install\npoetry\n==\n1.8.4\npipx\ncan also install versions of Poetry in parallel, which allows for easy testing of alternate or prerelease\nversions. Each version is given a unique, user-specified suffix, which will be used to create a unique binary name:\npipx install --suffix\n=\n@1.8.4\npoetry\n==\n1.8.4\npoetry@1.8.4 --version\npipx install --suffix\n=\n@preview --pip-args\n=\n--pre poetry\npoetry@preview --version\nFinally,\npipx\ncan install any valid\npip requirement spec\n, which\nallows for installations of the development version from\ngit\n, or even for local testing of pull requests:\npipx install --suffix @main git+https://github.com/python-poetry/poetry.git@main\npipx install --suffix @pr1234 git+https://github.com/python-poetry/poetry.git@refs/pull/1234/head\nUpdate Poetry\npipx upgrade poetry\nUninstall Poetry\npipx uninstall poetry\nWe provide a custom installer that will install Poetry in a new virtual environment\nand allows Poetry to manage its own environment.\nInstall Poetry\nThe installer script is available directly at\ninstall.python-poetry.org\n,\nand is developed in\nits own repository\n.\nThe script can be executed directly (i.e. ‘curl python’) or downloaded and then executed from disk\n(e.g. in a CI environment).\nLinux, macOS, Windows (WSL)\ncurl -sSL https://install.python-poetry.org\n|\npython3 -\nWindows (Powershell)\n(\nInvoke-WebRequest\n-Uri\nhttps\n:\n//\ninstall\n.\npython-poetry\n.\norg\n-UseBasicParsing\n).\nContent\n|\npy\n-\nNote\nIf you have installed Python through the Microsoft Store, replace\npy\nwith\npython\nin the command\nabove.\nInstall Poetry (advanced)\nNote\nYou can skip this step, if you simply want the latest version and already installed Poetry as described in the\nprevious step. This step details advanced usages of this installation method. For example, installing Poetry from\nsource, using a pre-release build, configuring a different installation location etc.\nBy default, Poetry is installed into a platform and user-specific directory:\n~/Library/Application Support/pypoetry\non macOS.\n~/.local/share/pypoetry\non Linux/Unix.\n%APPDATA%\\pypoetry\non Windows.\nIf you wish to change this, you may define the\n$POETRY_HOME\nenvironment variable:\ncurl -sSL https://install.python-poetry.org\n|\nPOETRY_HOME\n=\n/etc/poetry python3 -\nIf you want to install prerelease versions, you can do so by passing the\n--preview\noption to the installation script\nor by using the\n$POETRY_PREVIEW\nenvironment variable:\ncurl -sSL https://install.python-poetry.org\n|\npython3 - --preview\ncurl -sSL https://install.python-poetry.org\n|\nPOETRY_PREVIEW\n=\n1\npython3 -\nSimilarly, if you want to install a specific version, you can use\n--version\noption or the\n$POETRY_VERSION\nenvironment variable:\ncurl -sSL https://install.python-poetry.org\n|\npython3 - --version 1.8.4\ncurl -sSL https://install.python-poetry.org\n|\nPOETRY_VERSION\n=\n1.8.4 python3 -\nYou can also install Poetry from a\ngit\nrepository by using the\n--git\noption:\ncurl -sSL https://install.python-poetry.org\n|\npython3 - --git https://github.com/python-poetry/poetry.git@main\nIf you want to install different versions of Poetry in parallel, a good approach is the installation with pipx and suffix.\nAdd Poetry to your PATH\nThe installer creates a\npoetry\nwrapper in a well-known, platform-specific directory:\n$HOME/.local/bin\non Unix.\n%APPDATA%\\Python\\Scripts\non Windows.\n$POETRY_HOME/bin\nif\n$POETRY_HOME\nis set.\nIf this directory is not present in your\n$PATH\n, you can add it in order to invoke Poetry\nas\npoetry\n.\nAlternatively, the full path to the\npoetry\nbinary can always be used:\n~/Library/Application Support/pypoetry/venv/bin/poetry\non macOS.\n~/.local/share/pypoetry/venv/bin/poetry\non Linux/Unix.\n%APPDATA%\\pypoetry\\venv\\Scripts\\poetry\non Windows.\n$POETRY_HOME/venv/bin/poetry\nif\n$POETRY_HOME\nis set.\nUse Poetry\nOnce Poetry is installed and in your\n$PATH\n, you can execute the following:\npoetry --version\nIf you see something like\nPoetry (version 2.0.0)\n, your installation is ready to use!\nUpdate Poetry\nPoetry is able to update itself when installed using the official installer.\nWarning\nEspecially on Windows,\nself update\nmay be problematic\nso that a re-install with the installer should be preferred.\npoetry self update\nIf you want to install pre-release versions, you can use the\n--preview\noption.\npoetry self update --preview\nAnd finally, if you want to install a specific version, you can pass it as an argument\nto\nself update\n.\npoetry self update 1.8.4\nUninstall Poetry\nIf you decide Poetry isn’t your thing, you can completely remove it from your system\nby running the installer again with the\n--uninstall\noption or by setting\nthe\nPOETRY_UNINSTALL\nenvironment variable before executing the installer.\ncurl -sSL https://install.python-poetry.org\n|\npython3 - --uninstall\ncurl -sSL https://install.python-poetry.org\n|\nPOETRY_UNINSTALL\n=\n1\npython3 -\nPoetry can be installed manually using\npip\nand the\nvenv\nmodule. By doing so you will essentially perform the steps carried\nout by the official installer. As this is an advanced installation method, these instructions are Unix-only and omit specific\nexamples such as installing from\ngit\n.\nThe variable\n$VENV_PATH\nwill be used to indicate the path at which the virtual environment was created.\npython3 -m venv\n$VENV_PATH\n$VENV_PATH\n/bin/pip install -U pip setuptools\n$VENV_PATH\n/bin/pip install poetry\nPoetry will be available at\n$VENV_PATH/bin/poetry\nand can be invoked directly or symlinked elsewhere.\nTo uninstall Poetry, simply delete the entire\n$VENV_PATH\ndirectory.\nUnlike development environments, where making use of the latest tools is desirable, in a CI environment reproducibility\nshould be made the priority. Here are some suggestions for installing Poetry in such an environment.\nVersion pinning\nWhatever method you use, it is highly recommended to explicitly control the version of Poetry used, so that you are able\nto upgrade after performing your own validation. Each install method has a different syntax for setting the version that\nis used in the following examples.\nUsing pipx\nJust as\npipx\nis a powerful tool for development use, it is equally useful in a CI environment\nand should be one of your top choices for use of Poetry in CI.\npipx install\npoetry\n==\n2.0.0\nUsing install.python-poetry.org\nNote\nThe official installer script (\ninstall.python-poetry.org\n) offers a streamlined and\nsimplified installation of Poetry, sufficient for developer use or for simple pipelines. However, in a CI environment\nthe other two supported installation methods (pipx and manual) should be seriously considered.\nDownloading a copy of the installer script to a place accessible by your CI pipelines (or maintaining a copy of the\nrepository\n) is strongly suggested, to ensure your\npipeline’s stability and to maintain control over what code is executed.\nBy default, the installer will install to a user-specific directory. In more complex pipelines that may make accessing\nPoetry difficult (especially in cases like multi-stage container builds). It is highly suggested to make use of\n$POETRY_HOME\nwhen using the official installer in CI, as that way the exact paths can be controlled.\nexport\nPOETRY_HOME\n=\n/opt/poetry\npython3 install-poetry.py --version 2.0.0\n$POETRY_HOME\n/bin/poetry --version\nUsing pip (aka manually)\nFor maximum control in your CI environment, installation with\npip\nis fully supported and something you should\nconsider. While this requires more explicit commands and knowledge of Python packaging from you, it in return offers the\nbest debugging experience, and leaves you subject to the fewest external tools.\nexport\nPOETRY_HOME\n=\n/opt/poetry\npython3 -m venv\n$POETRY_HOME\n$POETRY_HOME\n/bin/pip install\npoetry\n==\n2.0.0\n$POETRY_HOME\n/bin/poetry --version\nNote\nIf you install Poetry via\npip\n, ensure you have Poetry installed into an isolated environment that is\nnot the same\nas the target environment managed by Poetry. If Poetry and your project are installed into the same environment, Poetry\nis likely to upgrade or uninstall its own dependencies (causing hard-to-debug and understand errors).\nWarning\nPoetry should always be installed in a dedicated virtual environment to isolate it from the rest of your system.\nEach of the above described installation methods ensures that.\nIt should in no case be installed in the environment of the project that is to be managed by Poetry.\nThis ensures that Poetry’s own dependencies will not be accidentally upgraded or uninstalled.\nIn addition, the isolated virtual environment in which poetry is installed should not be activated for running poetry commands.\nEnable tab completion for Bash, Fish, or Zsh\n#\npoetry\nsupports generating completion scripts for Bash, Fish, and Zsh.\nNote\nYou may need to restart your shell in order for these changes to take effect.\nSee\npoetry help completions\nfor full details, but the gist is as simple as using one of the following:\nBash\n#\nAuto-loaded (recommended)\n#\npoetry completions bash >> ~/.bash_completion\nLazy-loaded\n#\npoetry completions bash >\n${\nXDG_DATA_HOME\n:-\n~/.local/share\n}\n/bash-completion/completions/poetry\nFish\n#\npoetry\ncompletions\nfish\n>\n~/.config/\nfish\n/completions/poetry.\nfish\nZsh\n#\npoetry completions zsh > ~/.zfunc/_poetry\nYou must then add the following lines in your\n~/.zshrc\n, if they do not already exist:\nfpath\n+=\n~/.zfunc\nautoload -Uz compinit\n&&\ncompinit\nOh My Zsh\n#\nmkdir\n$ZSH_CUSTOM\n/plugins/poetry\npoetry completions zsh >\n$ZSH_CUSTOM\n/plugins/poetry/_poetry\nYou must then add\npoetry\nto your plugins array in\n~/.zshrc\n:\nplugins(\npoetry\n...\n)\nPrezto\n#\npoetry completions zsh > ~/.zprezto/modules/completion/external/src/_poetry\nIf completions still don’t work, try removing\n~/.cache/prezto/zcompcache\nand starting a new shell.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://python-poetry.org/docs/"}}
{"text": "Basic usage | Documentation | Poetry - Python dependency management and packaging made easy\nHome\n2.2\nmain\n2.2\n1.8\nUse dark mode\nOpen menu\nClose menu\nDocumentation\nBlog\nHistory\nBasic usage\nIntroduction\nBasic usage\nManaging dependencies\nLibraries\nCommands\nConfiguration\nRepositories\nManaging environments\nDependency specification\nPlugins\nThe pyproject.toml file\nContributing to Poetry\nCommunity\nFAQ\npre-commit hooks\nBuilding extension modules\nBasic usage\nBasic usage\n#\nFor the basic usage introduction we will be installing\npendulum\n, a datetime library.\nIf you have not yet installed Poetry, refer to the\nIntroduction\nchapter.\nProject setup\n#\nFirst, let’s create our new project, let’s call it\npoetry-demo\n:\npoetry new poetry-demo\nThis will create the\npoetry-demo\ndirectory with the following content:\npoetry-demo\n├── pyproject.toml\n├── README.md\n├── src\n│   └── poetry_demo\n│       └── __init__.py\n└── tests\n└── __init__.py\nThe\npyproject.toml\nfile is what is the most important here. This will orchestrate\nyour project and its dependencies. For now, it looks like this:\n[\nproject\n]\nname\n=\n\"poetry-demo\"\nversion\n=\n\"0.1.0\"\ndescription\n=\n\"\"\nauthors\n=\n[\n{\nname\n=\n\"Sébastien Eustace\"\n,\nemail\n=\n\"sebastien@eustace.io\"\n}\n]\nreadme\n=\n\"README.md\"\nrequires-python\n=\n\">=3.9\"\ndependencies\n=\n[\n]\n[\nbuild-system\n]\nrequires\n=\n[\n\"poetry-core>=2.0.0,<3.0.0\"\n]\nbuild-backend\n=\n\"poetry.core.masonry.api\"\nPoetry assumes your package contains a package with the same name as\nproject.name\nlocated in the root of your\nproject. If this is not the case, populate\ntool.poetry.packages\nto specify\nyour packages and their locations.\nSimilarly, the traditional\nMANIFEST.in\nfile is replaced by the\nproject.readme\n,\ntool.poetry.include\n, and\ntool.poetry.exclude\nsections.\ntool.poetry.exclude\nis additionally implicitly populated by your\n.gitignore\n. For\nfull documentation on the project format, see the\npyproject section\nof the documentation.\nSetting a Python Version\n#\nNote\nUnlike with other packages, Poetry will not automatically install a python interpreter for you.\nIf you want to run Python files in your package like a script or application, you must\nbring your own\npython interpreter to run them.\nPoetry will require you to explicitly specify what versions of Python you intend to support, and its universal locking\nwill guarantee that your project is installable (and all dependencies claim support for) all supported Python versions.\nAgain, it’s important to remember that – unlike other dependencies – setting a Python version is merely specifying which versions of Python you intend to support.\nFor example, in this\npyproject.toml\nfile:\n[\nproject\n]\nrequires-python\n=\n\">=3.9\"\nwe are allowing any version of Python 3 that is greater or equal than\n3.9.0\n.\nWhen you run\npoetry install\n, you must have access to some version of a Python interpreter that satisfies this constraint available on your system.\nPoetry will not install a Python interpreter for you.\nInitialising a pre-existing project\n#\nInstead of creating a new project, Poetry can be used to ‘initialize’ a pre-populated\ndirectory. To interactively create a\npyproject.toml\nfile in directory\npre-existing-project\n:\ncd\npre-existing-project\npoetry init\nOperating modes\n#\nPoetry can be operated in two different modes. The default mode is the\npackage mode\n, which is the right mode\nif you want to package your project into an sdist or a wheel and perhaps publish it to a package index.\nIn this mode, some metadata such as\nname\nand\nversion\n, which are required for packaging, are mandatory.\nFurther, the project itself will be installed in editable mode when running\npoetry install\n.\nIf you want to use Poetry only for dependency management but not for packaging, you can use the\nnon-package mode\n:\n[\ntool\n.\npoetry\n]\npackage-mode\n=\nfalse\nIn this mode, metadata such as\nname\nand\nversion\nare optional.\nTherefore, it is not possible to build a distribution or publish the project to a package index.\nFurther, when running\npoetry install\n, Poetry does not try to install the project itself,\nbut only its dependencies (same as\npoetry install --no-root\n).\nNote\nIn the\npyproject section\nyou can see which fields are required in package mode.\nSpecifying dependencies\n#\nIf you want to add dependencies to your project, you can specify them in the\nproject\nsection.\n[\nproject\n]\n# ...\ndependencies\n=\n[\n\"pendulum (>=2.1,<3.0)\"\n]\nAs you can see, it takes a mapping of\npackage names\nand\nversion constraints\n.\nPoetry uses this information to search for the right set of files in package “repositories” that you register\nin the\ntool.poetry.source\nsection, or on\nPyPI\nby default.\nAlso, instead of modifying the\npyproject.toml\nfile by hand, you can use the\nadd\ncommand.\n$ poetry add pendulum\nIt will automatically find a suitable version constraint\nand install\nthe package and sub-dependencies.\nPoetry supports a rich\ndependency specification\nsyntax, including caret,\ntilde, wildcard, inequality and\nmultiple constraints\nrequirements.\nUsing your virtual environment\n#\nBy default, Poetry creates a virtual environment in\n{cache-dir}/virtualenvs\n.\nYou can change the\ncache-dir\nvalue\nby editing the Poetry configuration.\nAdditionally, you can use the\nvirtualenvs.in-project\nconfiguration variable to create\nvirtual environments within your project directory.\nThere are several ways to run commands within this virtual environment.\nNote\nExternal virtual environment management\nPoetry will detect and respect an existing virtual environment that has been externally activated. This is a powerful\nmechanism that is intended to be an alternative to Poetry’s built-in, simplified environment management.\nTo take advantage of this, simply activate a virtual environment using your preferred method or tooling, before running\nany Poetry commands that expect to manipulate an environment.\nUsing\npoetry run\n#\nTo run your script simply use\npoetry run python your_script.py\n.\nLikewise if you have command line tools such as\npytest\nor\nblack\nyou can run them using\npoetry run pytest\n.\nNote\nIf managing your own virtual environment externally, you do not need to use\npoetry run\nsince\nyou will, presumably, already have activated that virtual environment and made available the correct python instance.\nFor example, these commands should output the same python path:\nconda activate your_env_name\nwhich python\npoetry run which python\npoetry env activate\nwhich python\nActivating the virtual environment\n#\nSee\nActivating the virtual environment\n.\nVersion constraints\n#\nIn our example, we are requesting the\npendulum\npackage with the version constraint\n>=2.1.0 <3.0.0\n.\nThis means any version greater or equal to 2.1.0 and less than 3.0.0.\nPlease read\nDependency specification\nfor more in-depth information on versions, how versions relate to each other, and on the different ways you can specify\ndependencies.\nNote\nHow does Poetry download the right files?\nWhen you specify a dependency in\npyproject.toml\n, Poetry first takes the name of the package\nthat you have requested and searches for it in any repository you have registered using the\nrepositories\nkey.\nIf you have not registered any extra repositories, or it does not find a package with that name in the\nrepositories you have specified, it falls back to PyPI.\nWhen Poetry finds the right package, it then attempts to find the best match for the version constraint you have\nspecified.\nInstalling dependencies\n#\nTo install the defined dependencies for your project, just run the\ninstall\ncommand.\npoetry install\nWhen you run this command, one of two things may happen:\nInstalling without\npoetry.lock\n#\nIf you have never run the command before and there is also no\npoetry.lock\nfile present,\nPoetry simply resolves all dependencies listed in your\npyproject.toml\nfile and downloads the latest version of their files.\nWhen Poetry has finished installing, it writes all the packages and their exact versions that it downloaded to the\npoetry.lock\nfile,\nlocking the project to those specific versions.\nYou should commit the\npoetry.lock\nfile to your project repo so that all people working on the project are locked to the same versions of dependencies (more below).\nInstalling with\npoetry.lock\n#\nThis brings us to the second scenario. If there is already a\npoetry.lock\nfile as well as a\npyproject.toml\nfile\nwhen you run\npoetry install\n, it means either you ran the\ninstall\ncommand before,\nor someone else on the project ran the\ninstall\ncommand and committed the\npoetry.lock\nfile to the project (which is good).\nEither way, running\ninstall\nwhen a\npoetry.lock\nfile is present resolves and installs all dependencies that you listed in\npyproject.toml\n,\nbut Poetry uses the exact versions listed in\npoetry.lock\nto ensure that the package versions are consistent for everyone working on your project.\nAs a result you will have all dependencies requested by your\npyproject.toml\nfile,\nbut they may not all be at the very latest available versions\n(some dependencies listed in the\npoetry.lock\nfile may have released newer versions since the file was created).\nThis is by design, it ensures that your project does not break because of unexpected changes in dependencies.\nCommitting your\npoetry.lock\nfile to version control\n#\nAs an application developer\n#\nApplication developers commit\npoetry.lock\nto get more reproducible builds.\nCommitting this file to VC is important because it will cause anyone who sets up the project\nto use the exact same versions of the dependencies that you are using.\nYour CI server, production machines, other developers in your team,\neverything and everyone runs on the same dependencies,\nwhich mitigates the potential for bugs affecting only some parts of the deployments.\nEven if you develop alone, in six months when reinstalling the project you can feel confident\nthe dependencies installed are still working even if your dependencies released many new versions since then.\n(See note below about using the update command.)\nWarning\nIf you have added the recommended\n[build-system]\nsection to your project’s pyproject.toml then you\ncan\nsuccessfully install your project and its dependencies into a virtual environment using a command like\npip install -e .\n. However, pip will not use the lock file to determine dependency versions as the poetry-core build system is intended for library developers (see next section).\nAs a library developer\n#\nLibrary developers have more to consider. Your users are application developers, and your library will run in a Python environment you don’t control.\nThe application ignores your library’s lock file. It can use whatever dependency version meets the constraints in your\npyproject.toml\n. The application will probably use the latest compatible dependency version. If your library’s\npoetry.lock\nfalls behind some new dependency version that breaks things for your users, you’re likely to be the last to find out about it.\nA simple way to avoid such a scenario is to omit the\npoetry.lock\nfile. However, by doing so, you sacrifice reproducibility and performance to a certain extent. Without a lockfile, it can be difficult to find the reason for failing tests, because in addition to obvious code changes an unnoticed library update might be the culprit. Further, Poetry will have to lock before installing a dependency if\npoetry.lock\nhas been omitted. Depending on the number of dependencies, locking may take a significant amount of time.\nIf you do not want to give up the reproducibility and performance benefits, consider a regular refresh of\npoetry.lock\nto stay up-to-date and reduce the risk of sudden breakage for users.\nInstalling dependencies only\n#\nThe current project is installed in\neditable\nmode by default.\nIf you want to install the dependencies only, run the\ninstall\ncommand with the\n--no-root\nflag:\npoetry install --no-root\nUpdating dependencies to their latest versions\n#\nAs mentioned above, the\npoetry.lock\nfile prevents you from automatically getting the latest versions\nof your dependencies.\nTo update to the latest versions, use the\nupdate\ncommand.\nThis will fetch the latest matching versions (according to your\npyproject.toml\nfile)\nand update the lock file with the new versions.\n(This is equivalent to deleting the\npoetry.lock\nfile and running\ninstall\nagain.)\nNote\nPoetry will display a\nWarning\nwhen executing an install command if\npoetry.lock\nand\npyproject.toml\nare not synchronized.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://python-poetry.org/docs/basic-usage/"}}
{"text": "Commands | Documentation | Poetry - Python dependency management and packaging made easy\nHome\n2.2\nmain\n2.2\n1.8\nUse dark mode\nOpen menu\nClose menu\nDocumentation\nBlog\nHistory\nCommands\nIntroduction\nBasic usage\nManaging dependencies\nLibraries\nCommands\nConfiguration\nRepositories\nManaging environments\nDependency specification\nPlugins\nThe pyproject.toml file\nContributing to Poetry\nCommunity\nFAQ\npre-commit hooks\nBuilding extension modules\nCommands\nCommands\n#\nYou’ve already learned how to use the command-line interface to do some things.\nThis chapter documents all the available commands.\nTo get help from the command-line, simply call\npoetry\nto see the complete list of commands,\nthen\n--help\ncombined with any of those can give you more information.\nGlobal Options\n#\n--verbose (-v|vv|vvv)\n: Increase the verbosity of messages: “-v” for normal output, “-vv” for more verbose output and “-vvv” for debug.\n--help (-h)\n: Display help information.\n--quiet (-q)\n: Do not output any message.\n--ansi\n: Force ANSI output.\n--no-ansi\n: Disable ANSI output.\n--version (-V)\n: Display this application version.\n--no-interaction (-n)\n: Do not ask any interactive question.\n--no-plugins\n: Disables plugins.\n--no-cache\n: Disables Poetry source caches.\n--directory=DIRECTORY (-C)\n: The working directory for the Poetry command (defaults to the current working directory). All command-line arguments will be resolved relative to the given directory.\n--project=PROJECT (-P)\n: Specify another path as the project root. All command-line arguments will be resolved relative to the current working directory or directory specified using\n--directory\noption if used.\nabout\n#\nThe\nabout\ncommand displays global information about Poetry, including the current version and version of\npoetry-core\n.\npoetry about\nadd\n#\nThe\nadd\ncommand adds required packages to your\npyproject.toml\nand installs them.\nIf you do not specify a version constraint, poetry will attempt to use the latest version.\npoetry add requests pendulum\nNote\nA package is looked up, by default, only from\nPyPI\n.\nYou can modify the default source (PyPI);\nor add and use\nSupplemental Package Sources\nor\nExplicit Package Sources\n.\nFor more information, refer to the\nPackage Sources\ndocumentation.\nYou can also specify a constraint when adding a package:\n# Allow >=2.0.5, <3.0.0 versions\npoetry add pendulum@^2.0.5\n# Allow >=2.0.5, <2.1.0 versions\npoetry add pendulum@~2.0.5\n# Allow >=2.0.5 versions, without upper bound\npoetry add\n\"pendulum>=2.0.5\"\n# Allow only 2.0.5 version\npoetry add\npendulum\n==\n2.0.5\nNote\nSee the\nDependency specification\npage for more information about the\n@\noperator.\nIf you try to add a package that is already present, you will get an error.\nHowever, if you specify a constraint, like above, the dependency will be updated\nby using the specified constraint.\nIf you want to get the latest version of an already\npresent dependency, you can use the special\nlatest\nconstraint:\npoetry add pendulum@latest\nNote\nSee the\nDependency specification\nfor more information on setting the version constraints for a package.\nYou can also add\ngit\ndependencies:\npoetry add git+https://github.com/sdispater/pendulum.git\nor use ssh instead of https:\npoetry add git+ssh://git@github.com/sdispater/pendulum.git\n# or alternatively:\npoetry add git+ssh://git@github.com:sdispater/pendulum.git\nIf you need to checkout a specific branch, tag or revision,\nyou can specify it when using\nadd\n:\npoetry add git+https://github.com/sdispater/pendulum.git#develop\npoetry add git+https://github.com/sdispater/pendulum.git#2.0.5\n# or using SSH instead:\npoetry add git+ssh://git@github.com:sdispater/pendulum.git#develop\npoetry add git+ssh://git@github.com:sdispater/pendulum.git#2.0.5\nor reference a subdirectory:\npoetry add git+https://github.com/myorg/mypackage_with_subdirs.git@main#subdirectory\n=\nsubdir\nYou can also add a local directory or file:\npoetry add ./my-package/\npoetry add ../my-package/dist/my-package-0.1.0.tar.gz\npoetry add ../my-package/dist/my_package-0.1.0.whl\nIf you want the dependency to be installed in editable mode you can use the\n--editable\noption.\npoetry add --editable ./my-package/\npoetry add --editable git+ssh://github.com/sdispater/pendulum.git#develop\nAlternatively, you can specify it in the\npyproject.toml\nfile. It means that changes in the local directory will be reflected directly in environment.\n[\ntool\n.\npoetry\n.\ndependencies\n]\nmy-package\n=\n{\npath\n=\n\"../my/path\"\n,\ndevelop\n=\ntrue\n}\nNote\nThe\ndevelop\nattribute is a Poetry-specific feature, so it is not included in the package distribution metadata.\nIn other words, it is only considered when using Poetry to install the project.\nIf the package(s) you want to install provide extras, you can specify them\nwhen adding the package:\npoetry add\n\"requests[security,socks]\"\npoetry add\n\"requests[security,socks]~=2.22.0\"\npoetry add\n\"git+https://github.com/pallets/flask.git@1.1.1[dotenv,dev]\"\nWarning\nSome shells may treat square braces (\n[\nand\n]\n) as special characters. It is suggested to always quote arguments containing these characters to prevent unexpected shell expansion.\nIf you want to add a package to a specific group of dependencies, you can use the\n--group (-G)\noption:\npoetry add mkdocs --group docs\nSee\nDependency groups\nfor more information\nabout dependency groups.\nOptions\n#\n--group (-G)\n: The group to add the dependency to.\n--dev (-D)\n: Add package as development dependency. (shortcut for\n-G dev\n)\n--editable (-e)\n: Add vcs/path dependencies as editable.\n--extras (-E)\n: Extras to activate for the dependency. (multiple values allowed)\n--optional\n: Add as an optional dependency to an extra.\n--python\n: Python version for which the dependency must be installed.\n--platform\n: Platforms for which the dependency must be installed.\n--markers\n: Environment markers which describe when the dependency should be installed.\n--source\n: Name of the source to use to install the package.\n--allow-prereleases\n: Accept prereleases.\n--dry-run\n: Output the operations but do not execute anything (implicitly enables\n--verbose\n).\n--lock\n: Do not perform install (only update the lockfile).\nbuild\n#\nThe\nbuild\ncommand builds the source and wheels archives.\npoetry build\nThe command will trigger the build system defined in the\npyproject.toml\nfile according to\nPEP 517\n.\nIf necessary the build process happens in an isolated environment.\nOptions\n#\n--format (-f)\n: Limit the format to either\nwheel\nor\nsdist\n.\n--clean\n: Clean output directory before building.\n--local-version (-l)\n: Add or replace a local version label to the build (deprecated).\n--output (-o)\n: Set output directory for build artifacts. Default is\ndist\n.\n--config-settings=<key>=<value> (-c)\n: Config settings to be passed to the build back-end. (multiple allowed)\nNote\nWhen using\n--local-version\n, the identifier must be\nPEP 440\ncompliant. This is useful for adding build numbers, platform specificities, etc. for private packages.\n--local-version\nis deprecated and will be removed in a future version of Poetry.\nUse\n--config-settings local-version=<version>\ninstead.\nWarning\nLocal version identifiers SHOULD NOT be used when publishing upstream projects to a public index server, but MAY be\nused to identify private builds created directly from the project source.\nSee\nPEP 440\nfor more information.\ncache\n#\nThe\ncache\ncommand groups subcommands to interact with Poetry’s cache.\ncache clear\n#\nThe\ncache clear\ncommand removes packages from a cached repository.\nFor example, to clear the whole cache of packages from the\nPyPI\nrepository, run:\npoetry cache clear PyPI --all\nTo only remove a specific package from a cache, you have to specify the cache entry in the following form\ncache:package:version\n:\npoetry cache clear pypi:requests:2.24.0\ncache list\n#\nThe\ncache list\ncommand lists Poetry’s available caches.\npoetry cache list\ncheck\n#\nThe\ncheck\ncommand validates the content of the\npyproject.toml\nfile\nand its consistency with the\npoetry.lock\nfile.\nIt returns a detailed report if there are any errors.\nNote\nThis command is also available as a pre-commit hook. See\npre-commit hooks\nfor more information.\npoetry check\nOptions\n#\n--lock\n: Verifies that\npoetry.lock\nexists for the current\npyproject.toml\n.\n--strict\n: Fail if check reports warnings.\nconfig\n#\nThe\nconfig\ncommand allows you to edit poetry config settings and repositories.\npoetry config --list\nUsage\n#\npoetry config\n[\noptions\n]\n[\nsetting-key\n]\n[\nsetting-value1\n]\n...\n[\nsetting-valueN\n]\nsetting-key\nis a configuration option name and\nsetting-value1\nis a configuration value.\nSee\nConfiguration\nfor all available settings.\nWarning\nUse\n--\nto terminate option parsing if your values may start with a hyphen (\n-\n), e.g.\npoetry config http-basic.custom-repo gitlab-ci-token --\n${\nGITLAB_JOB_TOKEN\n}\nWithout\n--\nthis command will fail if\n${GITLAB_JOB_TOKEN}\nstarts with a hyphen.\nOptions\n#\n--unset\n: Remove the configuration element named by\nsetting-key\n.\n--list\n: Show the list of current config variables.\n--local\n: Set/Get settings that are specific to a project (in the local configuration file\npoetry.toml\n).\n--migrate\n: Migrate outdated configuration settings.\ndebug\n#\nThe\ndebug\ncommand groups subcommands that are useful for, as the name suggests, debugging issues you might have\nwhen using Poetry with your projects.\ndebug info\n#\nThe\ndebug info\ncommand shows debug information about Poetry and your project’s virtual environment.\ndebug resolve\n#\nThe\ndebug resolve\ncommand helps when debugging dependency resolution issues. The command attempts to resolve your\ndependencies and list the chosen packages and versions.\ndebug tags\n#\nThe\ndebug tags\ncommand is useful when you want to see the supported packaging tags for your project’s active\nvirtual environment. This is useful when Poetry cannot install any known binary distributions for a dependency.\nenv\n#\nThe\nenv\ncommand groups subcommands to interact with the virtualenvs\nassociated with a specific project.\nSee\nManaging environments\nfor more information about these commands.\nenv activate\n#\nThe\nenv activate\ncommand prints the command to activate a virtual environment in your current shell.\nNote\nThis command does not activate the virtual environment, but only displays the activation command, for more information\non how to use this command see\nhere\n.\nenv info\n#\nThe\nenv info\ncommand displays information about the current environment.\nOptions\n#\n--path (-p)\n: Only display the environment’s path.\n--executable (-e)\n: Only display the environment’s python executable path.\nenv list\n#\nThe\nenv list\ncommand lists all virtualenvs associated with the current project.\nOptions\n#\n--full-path\n: Output the full paths of the virtualenvs.\nenv remove\n#\nThe\nenv remove\ncommand removes virtual environments associated with the project. You can specify multiple Python\nexecutables or virtual environment names to remove all matching ones. Alternatively, you can remove all associated\nvirtual environments using the\n--all\noption.\nNote\nIf\nvirtualenvs.in-project\nconfig is set to\ntrue\n, no argument or option is required. Your in project virtual environment is removed.\nArguments\n#\npython\n: The python executables associated with, or names of the virtual environments which are to be removed. Can be specified multiple times.\nOptions\n#\n--all\n: Remove all managed virtual environments associated with the project.\nenv use\n#\nThe\nenv use\ncommand activates or creates a new virtualenv for the current project.\nArguments\n#\npython\n: The python executable to use. This can be a version number (if not on Windows) or a path to the python binary.\nexport\n#\nWarning\nThis command is provided by the\nExport Poetry Plugin\n.\nThe plugin is no longer installed by default with Poetry 2.0.\nSee\nUsing plugins\nfor information on how to install a plugin.\nAs described in\nProject plugins\n,\nyou can also define in your\npyproject.toml\nthat the plugin is required for the development of your project:\n[\ntool\n.\npoetry\n.\nrequires-plugins\n]\npoetry-plugin-export\n=\n\">=1.8\"\nNote\nThe\nexport\ncommand is also available as a pre-commit hook.\nSee\npre-commit hooks\nfor more information.\nhelp\n#\nThe\nhelp\ncommand displays global help, or help for a specific command.\nTo display global help:\npoetry\nhelp\nTo display help for a specific command, for instance\nshow\n:\npoetry\nhelp\nshow\nNote\nThe\n--help\noption can also be passed to any command to get help for a specific command.\nFor instance:\npoetry show --help\ninit\n#\nThis command will help you create a\npyproject.toml\nfile interactively\nby prompting you to provide basic information about your package.\nIt will interactively ask you to fill in the fields, while using some smart defaults.\npoetry init\nOptions\n#\n--name\n: Name of the package.\n--description\n: Description of the package.\n--author\n: Author of the package.\n--python\nCompatible Python versions.\n--dependency\n: Package to require with a version constraint. Should be in format\nfoo:1.0.0\n.\n--dev-dependency\n: Development requirements, see\n--dependency\n.\ninstall\n#\nThe\ninstall\ncommand reads the\npyproject.toml\nfile from the current project,\nresolves the dependencies, and installs them.\nNote\nNormally, you should prefer\npoetry sync\nto\npoetry install\nto avoid untracked outdated packages.\nHowever, if you have set\nvirtualenvs.create = false\nto install dependencies into your system environment,\nwhich is discouraged, or\nvirtualenvs.options.system-site-packages = true\nto make\nsystem site-packages available in your virtual environment, you should use\npoetry install\nbecause\npoetry sync\nwill normally not work well in these cases.\npoetry install\nIf there is a\npoetry.lock\nfile in the current directory,\nit will use the exact versions from there instead of resolving them.\nThis ensures that everyone using the library will get the same versions of the dependencies.\nIf there is no\npoetry.lock\nfile, Poetry will create one after dependency resolution.\nIf you want to exclude one or more dependency groups for the installation, you can use\nthe\n--without\noption.\npoetry install --without test,docs\nYou can also select optional dependency groups with the\n--with\noption.\npoetry install --with test,docs\nTo install all dependency groups including the optional groups, use the\n--all-groups\nflag.\npoetry install --all-groups\nIt’s also possible to only install specific dependency groups by using the\nonly\noption.\npoetry install --only test,docs\nTo only install the project itself with no dependencies, use the\n--only-root\nflag.\npoetry install --only-root\nSee\nDependency groups\nfor more information\nabout dependency groups.\nYou can also specify the extras you want installed\nby passing the\n-E|--extras\noption (See\nExtras\nfor more info).\nPass\n--all-extras\nto install all defined extras for a project.\npoetry install --extras\n\"mysql pgsql\"\npoetry install -E mysql -E pgsql\npoetry install --all-extras\nAny extras not specified will be kept but not installed:\npoetry install --extras\n\"A B\"\n# C is kept if already installed\nIf you want to remove unspecified extras, use the\nsync\ncommand.\nBy default\npoetry\nwill install your project’s package every time you run\ninstall\n:\n$ poetry install\nInstalling dependencies from lock file\nNo dependencies to install or update\n- Installing <your-package-name>\n(\nx.x.x\n)\nIf you want to skip this installation, use the\n--no-root\noption.\npoetry install --no-root\nSimilar to\n--no-root\nyou can use\n--no-directory\nto skip directory path dependencies:\npoetry install --no-directory\nThis is mainly useful for caching in CI or when building Docker images. See the\nFAQ entry\nfor more information on this option.\nBy default\npoetry\ndoes not compile Python source files to bytecode during installation.\nThis speeds up the installation process, but the first execution may take a little more\ntime because Python then compiles source files to bytecode automatically.\nIf you want to compile source files to bytecode during installation,\nyou can use the\n--compile\noption:\npoetry install --compile\nOptions\n#\n--without\n: The dependency groups to ignore.\n--with\n: The optional dependency groups to include.\n--only\n: The only dependency groups to include.\n--only-root\n: Install only the root project, exclude all dependencies.\n--sync\n: Synchronize the environment with the locked packages and the specified groups. (\nDeprecated\n, use\npoetry sync\ninstead)\n--no-root\n: Do not install the root package (your project).\n--no-directory\n: Skip all directory path dependencies (including transitive ones).\n--dry-run\n: Output the operations but do not execute anything (implicitly enables\n--verbose\n).\n--extras (-E)\n: Features to install (multiple values allowed).\n--all-extras\n: Install all extra features (conflicts with\n--extras\n).\n--all-groups\n: Install dependencies from all groups (conflicts with\n--only\n,\n--with\n, and\n--without\n).\n--compile\n: Compile Python source files to bytecode.\nNote\nWhen\n--only\nis specified,\n--with\nand\n--without\noptions are ignored.\nlist\n#\nThe\nlist\ncommand displays all the available Poetry commands.\npoetry list\nlock\n#\nThis command locks (without installing) the dependencies specified in\npyproject.toml\n.\nNote\nBy default, packages that have already been added to the lock file before will not be updated.\nTo update all dependencies to the latest available compatible versions, use\npoetry update --lock\nor\npoetry lock --regenerate\n, which normally produce the same result.\nThis command is also available as a pre-commit hook. See\npre-commit hooks\nfor more information.\npoetry lock\nOptions\n#\n--regenerate\n: Ignore existing lock file and overwrite it with a new lock file created from scratch.\nnew\n#\nThis command will help you kickstart your new Python project by creating a new Poetry project. By default, a\nsrc\nlayout is chosen.\npoetry new my-package\nwill create a folder as follows:\nmy-package\n├── pyproject.toml\n├── README.md\n├── src\n│   └── my_package\n│       └── __init__.py\n└── tests\n└── __init__.py\nIf you want to name your project differently than the folder, you can pass\nthe\n--name\noption:\npoetry new my-folder --name my-package\nIf you want to use a\nflat\nproject layout, you can use the\n--flat\noption:\npoetry new --flat my-package\nThat will create a folder structure as follows:\nmy-package\n├── pyproject.toml\n├── README.md\n├── my_package\n│   └── __init__.py\n└── tests\n└── __init__.py\nNote\nFor an overview of the differences between\nflat\nand\nsrc\nlayouts, please see\nhere\n.\nThe\n--name\noption is smart enough to detect namespace packages and create\nthe required structure for you.\npoetry new --name my.package my-package\nwill create the following structure:\nmy-package\n├── pyproject.toml\n├── README.md\n├── src\n│   └── my\n│       └── package\n│           └── __init__.py\n└── tests\n└── __init__.py\nOptions\n#\n--interactive (-i)\n: Allow interactive specification of project configuration.\n--name\n: Set the resulting package name.\n--flat\n: Use the flat layout for the project.\n--readme\n: Specify the readme file extension. Default is\nmd\n. If you intend to publish to PyPI\nkeep the\nrecommendations for a PyPI-friendly README\nin mind.\n--description\n: Description of the package.\n--author\n: Author of the package.\n--python\nCompatible Python versions.\n--dependency\n: Package to require with a version constraint. Should be in format\nfoo:1.0.0\n.\n--dev-dependency\n: Development requirements, see\n--dependency\n.\npublish\n#\nThis command publishes the package, previously built with the\nbuild\ncommand, to the remote repository.\nIt will automatically register the package before uploading if this is the first time it is submitted.\npoetry publish\nIt can also build the package if you pass it the\n--build\noption.\nNote\nSee\nPublishable Repositories\nfor more information on how to configure and use publishable repositories.\nOptions\n#\n--repository (-r)\n: The repository to register the package to (default:\npypi\n).\nShould match a repository name set by the\nconfig\ncommand.\n--username (-u)\n: The username to access the repository.\n--password (-p)\n: The password to access the repository.\n--cert\n: Certificate authority to access the repository.\n--client-cert\n: Client certificate to access the repository.\n--dist-dir\n: Dist directory where built artifacts are stored. Default is\ndist\n.\n--build\n: Build the package before publishing.\n--dry-run\n: Perform all actions except upload the package.\n--skip-existing\n: Ignore errors from files already existing in the repository.\nNote\nSee\nConfiguring Credentials\nfor more information on how to configure credentials.\npython\n#\nThe\npython\nnamespace groups subcommands to manage Python versions.\nWarning\nThis is an experimental feature, and can change behaviour in upcoming releases.\nIntroduced in 2.1.0\npython install\n#\nThe\npython install\ncommand installs the specified Python version from the Python Standalone Builds project.\npoetry python install <PYTHON_VERSION>\nOptions\n#\n--clean\n: Clean up installation if check fails.\n--free-threaded\n: Use free-threaded version if available.\n--implementation\n: Python implementation to use. (cpython, pypy)\n--reinstall\n: Reinstall if installation already exists.\npython list\n#\nThe\npython list\ncommand shows Python versions available in the environment. This includes both installed and\ndiscovered System managed and Poetry managed installations.\npoetry python list\nOptions\n#\n--all\n: List all versions, including those available for download.\n--implementation\n: Python implementation to search for.\n--managed\n: List only Poetry managed Python versions.\npython remove\n#\nThe\npython remove\ncommand removes the specified Python version if managed by Poetry.\npoetry python remove <PYTHON_VERSION>\nOptions\n#\n--implementation\n: Python implementation to use. (cpython, pypy)\nremove\n#\nThe\nremove\ncommand removes a package from the current\nlist of installed packages.\npoetry remove pendulum\nIf you want to remove a package from a specific group of dependencies, you can use the\n--group (-G)\noption:\npoetry remove mkdocs --group docs\nSee\nDependency groups\nfor more information\nabout dependency groups.\nOptions\n#\n--group (-G)\n: The group to remove the dependency from.\n--dev (-D)\n: Removes a package from the development dependencies. (shortcut for\n-G dev\n)\n--dry-run\n: Outputs the operations but will not execute anything (implicitly enables\n--verbose\n).\n--lock\n: Do not perform operations (only update the lockfile).\nrun\n#\nThe\nrun\ncommand executes the given command inside the project’s virtualenv.\npoetry run python -V\nIt can also execute one of the scripts defined in\npyproject.toml\n.\nSo, if you have a script defined like this:\n[project]\n[project]\n[tool.poetry]\n[\nproject\n]\n# ...\n[\nproject\n.\nscripts\n]\nmy-script\n=\n\"my_module:main\"\n[\ntool\n.\npoetry\n.\nscripts\n]\nmy-script\n=\n\"my_module:main\"\nYou can execute it like so:\npoetry run my-script\nNote that this command has no option.\nsearch\n#\nThis command searches for packages on a remote index.\npoetry search requests pendulum\nNote\nPyPI no longer allows for the search of packages without a browser. Please use\nhttps://pypi.org/search\n(via a browser) instead.\nFor more information, please see\nwarehouse documentation\nand this\ndiscussion\n.\nself\n#\nThe\nself\nnamespace groups subcommands to manage the Poetry installation itself.\nNote\nUse of these commands will create the required\npyproject.toml\nand\npoetry.lock\nfiles in your\nconfiguration directory\n.\nWarning\nEspecially on Windows,\nself\ncommands that update or remove packages may be problematic\nso that other methods for installing plugins and updating Poetry are recommended.\nSee\nUsing plugins\nand\nInstalling Poetry\nfor more information.\nself add\n#\nThe\nself add\ncommand installs Poetry plugins and make them available at runtime. Additionally, it can\nalso be used to upgrade Poetry’s own dependencies or inject additional packages into the runtime\nenvironment\nNote\nThe\nself add\ncommand works exactly like the\nadd\ncommand\n. However, is different in that the packages\nmanaged are for Poetry’s runtime environment.\nThe package specification formats supported by the\nself add\ncommand are the same as the ones supported\nby the\nadd\ncommand\n.\nFor example, to install the\npoetry-plugin-export\nplugin, you can run:\npoetry self add poetry-plugin-export\nTo update to the latest\npoetry-core\nversion, you can run:\npoetry self add poetry-core@latest\nTo add a keyring provider\nartifacts-keyring\n, you can run:\npoetry self add artifacts-keyring\nOptions\n#\n--editable (-e)\n: Add vcs/path dependencies as editable.\n--extras (-E)\n: Extras to activate for the dependency. (multiple values allowed)\n--allow-prereleases\n: Accept prereleases.\n--source\n: Name of the source to use to install the package.\n--dry-run\n: Output the operations but do not execute anything (implicitly enables\n--verbose\n).\nself install\n#\nThe\nself install\ncommand ensures all additional packages specified are installed in the current\nruntime environment.\nNote\nThe\nself install\ncommand works similar to the\ninstall\ncommand\n. However,\nit is different in that the packages managed are for Poetry’s runtime environment.\npoetry self install\nOptions\n#\n--sync\n: Synchronize the environment with the locked packages and the specified groups. (\nDeprecated\n, use\npoetry self sync\ninstead)\n--dry-run\n: Output the operations but do not execute anything (implicitly enables\n--verbose\n).\nself lock\n#\nThe\nself lock\ncommand reads this Poetry installation’s system\npyproject.toml\nfile. The system\ndependencies are locked in the corresponding\npoetry.lock\nfile.\npoetry self lock\nOptions\n#\n--regenerate\n: Ignore existing lock file and overwrite it with a new lock file created from scratch.\nself remove\n#\nThe\nself remove\ncommand removes an installed addon package.\npoetry self remove poetry-plugin-export\nOptions\n#\n--dry-run\n: Outputs the operations but will not execute anything (implicitly enables\n--verbose\n).\nself show\n#\nThe\nself show\ncommand behaves similar to the show command, but\nworking within Poetry’s runtime environment. This lists all packages installed within\nthe Poetry install environment.\nTo show only additional packages that have been added via self add and their\ndependencies use\nself show --addons\n.\npoetry self show\nOptions\n#\n--addons\n: List only add-on packages installed.\n--tree\n: List the dependencies as a tree.\n--latest (-l)\n: Show the latest version.\n--outdated (-o)\n: Show the latest version but only for packages that are outdated.\n--format (-f)\n: Specify the output format (\njson\nor\ntext\n). Default is\ntext\n.\njson\ncannot be combined with the\n--tree\noption.\nself show plugins\n#\nThe\nself show plugins\ncommand lists all the currently installed plugins.\npoetry self show plugins\nself sync\n#\nThe\nself sync\ncommand ensures all additional (and no other) packages specified\nare installed in the current runtime environment.\nNote\nThe\nself sync\ncommand works similar to the\nsync\ncommand\n. However,\nit is different in that the packages managed are for Poetry’s runtime environment.\npoetry self sync\nOptions\n#\n--dry-run\n: Output the operations but do not execute anything (implicitly enables\n--verbose\n).\nself update\n#\nThe\nself update\ncommand updates Poetry version in its current runtime environment.\nNote\nThe\nself update\ncommand works exactly like the\nupdate\ncommand\n. However,\nis different in that the packages managed are for Poetry’s runtime environment.\npoetry self update\nOptions\n#\n--preview\n: Allow the installation of pre-release versions.\n--dry-run\n: Output the operations but do not execute anything (implicitly enables\n--verbose\n).\nshell\n#\nThe\nshell\ncommand was moved to a plugin:\npoetry-plugin-shell\nshow\n#\nTo list all the available packages, you can use the\nshow\ncommand.\npoetry show\nIf you want to see the details of a certain package, you can pass the package name.\npoetry show pendulum\nname        : pendulum\nversion     : 1.4.2\ndescription : Python datetimes made easy\ndependencies\n- python-dateutil >\n=\n2.6.1\n- tzlocal >\n=\n1.4\n- pytzdata >\n=\n2017.2.2\nrequired by\n- calendar requires >\n=\n1.4.0\nOptions\n#\n--without\n: The dependency groups to ignore.\n--why\n: When showing the full list, or a\n--tree\nfor a single package, display whether they are a direct dependency or required by other packages.\n--with\n: The optional dependency groups to include.\n--only\n: The only dependency groups to include.\n--tree\n: List the dependencies as a tree.\n--latest (-l)\n: Show the latest version.\n--outdated (-o)\n: Show the latest version but only for packages that are outdated.\n--all (-a)\n: Show all packages (even those not compatible with current system).\n--top-level (-T)\n: Only show explicitly defined packages.\n--no-truncate\n: Do not truncate the output based on the terminal width.\n--format (-f)\n: Specify the output format (\njson\nor\ntext\n). Default is\ntext\n.\njson\ncannot be combined with the\n--tree\noption.\nNote\nWhen\n--only\nis specified,\n--with\nand\n--without\noptions are ignored.\nsource\n#\nThe\nsource\nnamespace groups subcommands to manage repository sources for a Poetry project.\nsource add\n#\nThe\nsource add\ncommand adds source configuration to the project.\nFor example, to add the\npypi-test\nsource, you can run:\npoetry\nsource\nadd --priority supplemental pypi-test https://test.pypi.org/simple/\nYou cannot use the name\npypi\nfor a custom repository as it is reserved for use by\nthe default PyPI source. However, you can set the priority of PyPI:\npoetry\nsource\nadd --priority\n=\nexplicit pypi\nOptions\n#\n--priority\n: Set the priority of this source. Accepted values are:\nprimary\n,\nsupplemental\n, and\nexplicit\n. Refer to the dedicated sections in\nRepositories\nfor more information.\nsource show\n#\nThe\nsource show\ncommand displays information on all configured sources for the project.\npoetry\nsource\nshow\nOptionally, you can show information of one or more sources by specifying their names.\npoetry\nsource\nshow pypi-test\nNote\nThis command will only show sources configured via the\npyproject.toml\nand does not include the implicit default PyPI.\nsource remove\n#\nThe\nsource remove\ncommand removes a configured source from your\npyproject.toml\n.\npoetry\nsource\nremove pypi-test\nsync\n#\nThe\nsync\ncommand makes sure that the project’s environment is in sync with the\npoetry.lock\nfile.\nIt is similar to\npoetry install\nbut it additionally removes packages that are not tracked in the lock file.\npoetry sync\nIf there is a\npoetry.lock\nfile in the current directory,\nit will use the exact versions from there instead of resolving them.\nThis ensures that everyone using the library will get the same versions of the dependencies.\nIf there is no\npoetry.lock\nfile, Poetry will create one after dependency resolution.\nIf you want to exclude one or more dependency groups for the installation, you can use\nthe\n--without\noption.\npoetry sync --without test,docs\nYou can also select optional dependency groups with the\n--with\noption.\npoetry sync --with test,docs\nTo install all dependency groups including the optional groups, use the\n--all-groups\nflag.\npoetry sync --all-groups\nIt’s also possible to only install specific dependency groups by using the\nonly\noption.\npoetry sync --only test,docs\nTo only install the project itself with no dependencies, use the\n--only-root\nflag.\npoetry sync --only-root\nSee\nDependency groups\nfor more information\nabout dependency groups.\nYou can also specify the extras you want installed\nby passing the\n-E|--extras\noption (See\nExtras\nfor more info).\nPass\n--all-extras\nto install all defined extras for a project.\npoetry sync --extras\n\"mysql pgsql\"\npoetry sync -E mysql -E pgsql\npoetry sync --all-extras\nAny extras not specified will always be removed.\npoetry sync --extras\n\"A B\"\n# C is removed\nBy default\npoetry\nwill install your project’s package every time you run\nsync\n:\n$ poetry sync\nInstalling dependencies from lock file\nNo dependencies to install or update\n- Installing <your-package-name>\n(\nx.x.x\n)\nIf you want to skip this installation, use the\n--no-root\noption.\npoetry sync --no-root\nSimilar to\n--no-root\nyou can use\n--no-directory\nto skip directory path dependencies:\npoetry sync --no-directory\nThis is mainly useful for caching in CI or when building Docker images. See the\nFAQ entry\nfor more information on this option.\nBy default\npoetry\ndoes not compile Python source files to bytecode during installation.\nThis speeds up the installation process, but the first execution may take a little more\ntime because Python then compiles source files to bytecode automatically.\nIf you want to compile source files to bytecode during installation,\nyou can use the\n--compile\noption:\npoetry sync --compile\nOptions\n#\n--without\n: The dependency groups to ignore.\n--with\n: The optional dependency groups to include.\n--only\n: The only dependency groups to include.\n--only-root\n: Install only the root project, exclude all dependencies.\n--no-root\n: Do not install the root package (your project).\n--no-directory\n: Skip all directory path dependencies (including transitive ones).\n--dry-run\n: Output the operations but do not execute anything (implicitly enables\n--verbose\n).\n--extras (-E)\n: Features to install (multiple values allowed).\n--all-extras\n: Install all extra features (conflicts with\n--extras\n).\n--all-groups\n: Install dependencies from all groups (conflicts with\n--only\n,\n--with\n, and\n--without\n).\n--compile\n: Compile Python source files to bytecode.\nNote\nWhen\n--only\nis specified,\n--with\nand\n--without\noptions are ignored.\nupdate\n#\nIn order to get the latest versions of the dependencies and to update the\npoetry.lock\nfile,\nyou should use the\nupdate\ncommand.\npoetry update\nThis will resolve all dependencies of the project and write the exact versions into\npoetry.lock\n.\nIf you just want to update a few packages and not all, you can list them as such:\npoetry update requests toml\nNote that this will not update versions for dependencies outside their\nversion constraints\nspecified in the\npyproject.toml\nfile.\nIn other terms,\npoetry update foo\nwill be a no-op if the version constraint\nspecified for\nfoo\nis\n~2.3\nor\n2.3\nand\n2.4\nis available.\nIn order for\nfoo\nto be updated, you must update the constraint, for example\n^2.3\n.\nYou can do this using the\nadd\ncommand.\nOptions\n#\n--without\n: The dependency groups to ignore.\n--with\n: The optional dependency groups to include.\n--only\n: The only dependency groups to include.\n--dry-run\n: Outputs the operations but will not execute anything (implicitly enables\n--verbose\n).\n--lock\n: Do not perform install (only update the lockfile).\n--sync\n: Synchronize the environment with the locked packages and the specified groups.\nNote\nWhen\n--only\nis specified,\n--with\nand\n--without\noptions are ignored.\nversion\n#\nThis command shows the current version of the project or bumps the version of\nthe project and writes the new version back to\npyproject.toml\nif a valid\nbump rule is provided.\nThe new version should be a valid\nPEP 440\nstring or a valid bump rule:\npatch\n,\nminor\n,\nmajor\n,\nprepatch\n,\npreminor\n,\npremajor\n,\nprerelease\n.\nNote\nIf you would like to use semantic versioning for your project, please see\nhere\n.\nThe table below illustrates the effect of these rules with concrete examples.\nrule\nbefore\nafter\nmajor\n1.3.0\n2.0.0\nminor\n2.1.4\n2.2.0\npatch\n4.1.1\n4.1.2\npremajor\n1.0.2\n2.0.0a0\npreminor\n1.0.2\n1.1.0a0\nprepatch\n1.0.2\n1.0.3a0\nprerelease\n1.0.2\n1.0.3a0\nprerelease\n1.0.3a0\n1.0.3a1\nprerelease\n1.0.3b0\n1.0.3b1\nThe option\n--next-phase\nallows the increment of prerelease phase versions.\nrule\nbefore\nafter\nprerelease –next-phase\n1.0.3a0\n1.0.3b0\nprerelease –next-phase\n1.0.3b0\n1.0.3rc0\nprerelease –next-phase\n1.0.3rc0\n1.0.3\nOptions\n#\n--next-phase\n: Increment the phase of the current version.\n--short (-s)\n: Output the version number only.\n--dry-run\n: Do not update pyproject.toml file.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://python-poetry.org/docs/cli/"}}
{"text": "uv\nSkip to content\nuv\nAn extremely fast Python package and project manager, written in Rust.\nInstalling\nTrio\n's dependencies with a warm cache.\nHighlights\n🚀 A single tool to replace\npip\n,\npip-tools\n,\npipx\n,\npoetry\n,\npyenv\n,\ntwine\n,\nvirtualenv\n,\nand more.\n⚡️\n10-100x faster\nthan\npip\n.\n🗂️ Provides\ncomprehensive project management\n, with a\nuniversal lockfile\n.\n❇️\nRuns scripts\n, with support for\ninline dependency metadata\n.\n🐍\nInstalls and manages\nPython versions.\n🛠️\nRuns and installs\ntools published as Python packages.\n🔩 Includes a\npip-compatible interface\nfor a performance boost with a\nfamiliar CLI.\n🏢 Supports Cargo-style\nworkspaces\nfor scalable projects.\n💾 Disk-space efficient, with a\nglobal cache\nfor dependency deduplication.\n⏬ Installable without Rust or Python via\ncurl\nor\npip\n.\n🖥️ Supports macOS, Linux, and Windows.\nuv is backed by\nAstral\n, the creators of\nRuff\n.\nInstallation\nInstall uv with our official standalone installer:\nmacOS and Linux\nWindows\n$\ncurl\n-LsSf\nhttps://astral.sh/uv/install.sh\n|\nsh\nPS>\npowershell\n-ExecutionPolicy\nByPass\n-c\n\"irm https://astral.sh/uv/install.ps1 | iex\"\nThen, check out the\nfirst steps\nor read on for a brief overview.\nTip\nuv may also be installed with pip, Homebrew, and more. See all of the methods on the\ninstallation page\n.\nProjects\nuv manages project dependencies and environments, with support for lockfiles, workspaces, and more,\nsimilar to\nrye\nor\npoetry\n:\n$\nuv\ninit\nexample\nInitialized project `example` at `/home/user/example`\n$\ncd\nexample\n$\nuv\nadd\nruff\nCreating virtual environment at: .venv\nResolved 2 packages in 170ms\nBuilt example @ file:///home/user/example\nPrepared 2 packages in 627ms\nInstalled 2 packages in 1ms\n+ example==0.1.0 (from file:///home/user/example)\n+ ruff==0.5.4\n$\nuv\nrun\nruff\ncheck\nAll checks passed!\n$\nuv\nlock\nResolved 2 packages in 0.33ms\n$\nuv\nsync\nResolved 2 packages in 0.70ms\nAudited 1 package in 0.02ms\nSee the\nproject guide\nto get started.\nuv also supports building and publishing projects, even if they're not managed with uv. See the\npackaging guide\nto learn more.\nScripts\nuv manages dependencies and environments for single-file scripts.\nCreate a new script and add inline metadata declaring its dependencies:\n$\necho\n'import requests; print(requests.get(\"https://astral.sh\"))'\n>\nexample.py\n$\nuv\nadd\n--script\nexample.py\nrequests\nUpdated `example.py`\nThen, run the script in an isolated virtual environment:\n$\nuv\nrun\nexample.py\nReading inline script metadata from: example.py\nInstalled 5 packages in 12ms\n<Response [200]>\nSee the\nscripts guide\nto get started.\nTools\nuv executes and installs command-line tools provided by Python packages, similar to\npipx\n.\nRun a tool in an ephemeral environment using\nuvx\n(an alias for\nuv tool run\n):\n$\nuvx\npycowsay\n'hello world!'\nResolved 1 package in 167ms\nInstalled 1 package in 9ms\n+ pycowsay==0.0.0.2\n\"\"\"\n------------\n< hello world! >\n------------\n\\   ^__^\n\\  (oo)\\_______\n(__)\\       )\\/\\\n||----w |\n||     ||\nInstall a tool with\nuv tool install\n:\n$\nuv\ntool\ninstall\nruff\nResolved 1 package in 6ms\nInstalled 1 package in 2ms\n+ ruff==0.5.4\nInstalled 1 executable: ruff\n$\nruff\n--version\nruff 0.5.4\nSee the\ntools guide\nto get started.\nPython versions\nuv installs Python and allows quickly switching between versions.\nInstall multiple Python versions:\n$\nuv\npython\ninstall\n3\n.10\n3\n.11\n3\n.12\nSearching for Python versions matching: Python 3.10\nSearching for Python versions matching: Python 3.11\nSearching for Python versions matching: Python 3.12\nInstalled 3 versions in 3.42s\n+ cpython-3.10.14-macos-aarch64-none\n+ cpython-3.11.9-macos-aarch64-none\n+ cpython-3.12.4-macos-aarch64-none\nDownload Python versions as needed:\n$\nuv\nvenv\n--python\n3\n.12.0\nUsing CPython 3.12.0\nCreating virtual environment at: .venv\nActivate with: source .venv/bin/activate\n$\nuv\nrun\n--python\n[email protected]\n--\npython\nPython 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:30)\n[PyPy 7.3.11 with GCC Apple LLVM 13.1.6 (clang-1316.0.21.2.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>>\nUse a specific Python version in the current directory:\n$\nuv\npython\npin\n3\n.11\nPinned `.python-version` to `3.11`\nSee the\ninstalling Python guide\nto get started.\nThe pip interface\nuv provides a drop-in replacement for common\npip\n,\npip-tools\n, and\nvirtualenv\ncommands.\nuv extends their interfaces with advanced features, such as dependency version overrides,\nplatform-independent resolutions, reproducible resolutions, alternative resolution strategies, and\nmore.\nMigrate to uv without changing your existing workflows — and experience a 10-100x speedup — with the\nuv pip\ninterface.\nCompile requirements into a platform-independent requirements file:\n$\nuv\npip\ncompile\ndocs/requirements.in\n\\\n--universal\n\\\n--output-file\ndocs/requirements.txt\nResolved 43 packages in 12ms\nCreate a virtual environment:\n$\nuv\nvenv\nUsing CPython 3.12.3\nCreating virtual environment at: .venv\nActivate with: source .venv/bin/activate\nInstall the locked requirements:\n$\nuv\npip\nsync\ndocs/requirements.txt\nResolved 43 packages in 11ms\nInstalled 43 packages in 208ms\n+ babel==2.15.0\n+ black==24.4.2\n+ certifi==2024.7.4\n...\nSee the\npip interface documentation\nto get started.\nLearn more\nSee the\nfirst steps\nor jump straight to the\nguides\nto start using uv.\nMay 18, 2025\nBack to top", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.astral.sh/uv/"}}
{"text": "Index | uv\nSkip to content\nGetting started\nTo help you get started with uv, we'll cover a few important topics:\nInstalling uv\nFirst steps after installation\nAn overview of uv's features\nHow to get help\nRead on, or jump ahead to another section:\nGet going quickly with\nguides\nfor common workflows.\nLearn more about the core\nconcepts\nin uv.\nUse the\nreference\ndocumentation to find details about something specific.\nAugust 16, 2024\nBack to top", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.astral.sh/uv/getting-started/"}}
{"text": "Get Started - pytest documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\npytest documentation\nGet Started\nHow-to guides\nHow to invoke pytest\nHow to write and report assertions in tests\nHow to use fixtures\nHow to mark test functions with attributes\nHow to parametrize fixtures and test functions\nHow to use subtests\nHow to use temporary directories and files in tests\nHow to monkeypatch/mock modules and environments\nHow to run doctests\nHow to re-run failed tests and maintain state between test runs\nHow to handle test failures\nManaging pytest’s output\nHow to manage logging\nHow to capture stdout/stderr output\nHow to capture warnings\nHow to use skip and xfail to deal with tests that cannot succeed\nHow to install and use plugins\nWriting plugins\nWriting hook functions\nHow to use pytest with an existing test suite\nHow to use\nunittest\n-based tests with pytest\nHow to implement xunit-style set-up\nHow to set up bash completion\nReference guides\nAPI Reference\nFixtures reference\nConfiguration\nExit codes\nPytest Plugin List\nExplanation\nAnatomy of a test\nAbout fixtures\nGood Integration Practices\npytest import mechanisms and\nsys.path\n/\nPYTHONPATH\nTyping in pytest\nCI Pipelines\nFlaky tests\nExamples and customization tricks\nDemo of Python failure reports with pytest\nBasic patterns and examples\nParametrizing tests\nWorking with custom markers\nA session-fixture which can look at all collected tests\nChanging standard (Python) test discovery\nWorking with non-python tests\nUsing a custom directory collector\nAbout the project\nChangelog\nContributing\nBackwards Compatibility Policy\nHistory\nPython version support\nSponsor\npytest for enterprise\nLicense\nContact channels\nUseful links\npytest @ PyPI\npytest @ GitHub\nIssue Tracker\nPDF Documentation\nBack to top\nGet Started\n¶\nInstall\npytest\n¶\nRun the following command in your command line:\npip\ninstall\n-U\npytest\nCheck that you installed the correct version:\n$\npytest\n--version\npytest\n9\n.0.1\nCreate your first test\n¶\nCreate a new file called\ntest_sample.py\n, containing a function, and a test:\n# content of test_sample.py\ndef\nfunc\n(\nx\n):\nreturn\nx\n+\n1\ndef\ntest_answer\n():\nassert\nfunc\n(\n3\n)\n==\n5\nThe test\n$ pytest\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y\nrootdir: /home/sweet/project\ncollected 1 item\ntest_sample.py\nF\n[100%]\n================================= FAILURES =================================\n_______________________________ test_answer ________________________________\ndef test_answer():\n>       assert func(3) == 5\nE       assert 4 == 5\nE        +  where 4 = func(3)\ntest_sample.py\n:6: AssertionError\n========================= short test summary info ==========================\nFAILED\ntest_sample.py::\ntest_answer\n- assert 4 == 5\n============================\n1 failed\nin 0.12s =============================\nThe\n[100%]\nrefers to the overall progress of running all test cases. After it finishes, pytest then shows a failure report because\nfunc(3)\ndoes not return\n5\n.\nNote\nYou can use the\nassert\nstatement to verify test expectations. pytest’s\nAdvanced assertion introspection\nwill intelligently report intermediate values of the assert expression so you can avoid the many names\nof JUnit legacy methods\n.\nRun multiple tests\n¶\npytest\nwill run all files of the form\ntest_*.py\nor\n*_test.py\nin the current directory and its subdirectories. More generally, it follows\nstandard test discovery rules\n.\nAssert that a certain exception is raised\n¶\nUse the\nraises\nhelper to assert that some code raises an exception:\n# content of test_sysexit.py\nimport\npytest\ndef\nf\n():\nraise\nSystemExit\n(\n1\n)\ndef\ntest_mytest\n():\nwith\npytest\n.\nraises\n(\nSystemExit\n):\nf\n()\nExecute the test function with “quiet” reporting mode:\n$ pytest -q test_sysexit.py\n.\n[100%]\n1 passed\nin 0.12s\nNote\nThe\n-q/--quiet\nflag keeps the output brief in this and following examples.\nSee\nAssertions about approximate equality\nfor specifying more details about the expected exception.\nGroup multiple tests in a class\n¶\nOnce you develop multiple tests, you may want to group them into a class. pytest makes it easy to create a class containing more than one test:\n# content of test_class.py\nclass\nTestClass\n:\ndef\ntest_one\n(\nself\n):\nx\n=\n\"this\"\nassert\n\"h\"\nin\nx\ndef\ntest_two\n(\nself\n):\nx\n=\n\"hello\"\nassert\nhasattr\n(\nx\n,\n\"check\"\n)\npytest\ndiscovers all tests following its\nConventions for Python test discovery\n, so it finds both\ntest_\nprefixed functions. There is no need to subclass anything, but make sure to prefix your class with\nTest\notherwise the class will be skipped. We can simply run the module by passing its filename:\n$ pytest -q test_class.py\n.\nF\n[100%]\n================================= FAILURES =================================\n____________________________ TestClass.test_two ____________________________\nself = <test_class.TestClass object at 0xdeadbeef0001>\ndef test_two(self):\nx = \"hello\"\n>       assert hasattr(x, \"check\")\nE       AssertionError: assert False\nE        +  where False = hasattr('hello', 'check')\ntest_class.py\n:8: AssertionError\n========================= short test summary info ==========================\nFAILED\ntest_class.py::\nTestClass::test_two\n- AssertionError: assert False\n1 failed\n,\n1 passed\nin 0.12s\nThe first test passed and the second failed. You can easily see the intermediate values in the assertion to help you understand the reason for the failure.\nGrouping tests in classes can be beneficial for the following reasons:\nTest organization\nSharing fixtures for tests only in that particular class\nApplying marks at the class level and having them implicitly apply to all tests\nSomething to be aware of when grouping tests inside classes is that each test has a unique instance of the class.\nHaving each test share the same class instance would be very detrimental to test isolation and would promote poor test practices.\nThis is outlined below:\n# content of test_class_demo.py\nclass\nTestClassDemoInstance\n:\nvalue\n=\n0\ndef\ntest_one\n(\nself\n):\nself\n.\nvalue\n=\n1\nassert\nself\n.\nvalue\n==\n1\ndef\ntest_two\n(\nself\n):\nassert\nself\n.\nvalue\n==\n1\n$ pytest -k TestClassDemoInstance -q\n.\nF\n[100%]\n================================= FAILURES =================================\n______________________ TestClassDemoInstance.test_two ______________________\nself = <test_class_demo.TestClassDemoInstance object at 0xdeadbeef0002>\ndef test_two(self):\n>       assert self.value == 1\nE       assert 0 == 1\nE        +  where 0 = <test_class_demo.TestClassDemoInstance object at 0xdeadbeef0002>.value\ntest_class_demo.py\n:9: AssertionError\n========================= short test summary info ==========================\nFAILED\ntest_class_demo.py::\nTestClassDemoInstance::test_two\n- assert 0 == 1\n1 failed\n,\n1 passed\nin 0.12s\nNote that attributes added at class level are\nclass attributes\n, so they will be shared between tests.\nCompare floating-point values with pytest.approx\n¶\npytest\nalso provides a number of utilities to make writing tests easier.\nFor example, you can use\npytest.approx()\nto compare floating-point\nvalues that may have small rounding errors:\n# content of test_approx.py\nimport\npytest\ndef\ntest_sum\n():\nassert\n(\n0.1\n+\n0.2\n)\n==\npytest\n.\napprox\n(\n0.3\n)\nThis avoids the need for manual tolerance checks or using\nmath.isclose\nand works with scalars, lists, and NumPy arrays.\nRequest a unique temporary directory for functional tests\n¶\npytest\nprovides\nBuiltin fixtures/function arguments\nto request arbitrary resources, like a unique temporary directory:\n# content of test_tmp_path.py\ndef\ntest_needsfiles\n(\ntmp_path\n):\nprint\n(\ntmp_path\n)\nassert\n0\nList the name\ntmp_path\nin the test function signature and\npytest\nwill lookup and call a fixture factory to create the resource before performing the test function call. Before the test runs,\npytest\ncreates a unique-per-test-invocation temporary directory:\n$ pytest -q test_tmp_path.py\nF\n[100%]\n================================= FAILURES =================================\n_____________________________ test_needsfiles ______________________________\ntmp_path = PosixPath('PYTEST_TMPDIR/test_needsfiles0')\ndef test_needsfiles(tmp_path):\nprint(tmp_path)\n>       assert 0\nE       assert 0\ntest_tmp_path.py\n:3: AssertionError\n--------------------------- Captured stdout call ---------------------------\nPYTEST_TMPDIR/test_needsfiles0\n========================= short test summary info ==========================\nFAILED\ntest_tmp_path.py::\ntest_needsfiles\n- assert 0\n1 failed\nin 0.12s\nMore info on temporary directory handling is available at\nTemporary directories and files\n.\nFind out what kind of builtin\npytest fixtures\nexist with the command:\npytest\n--fixtures\n# shows builtin and custom fixtures\nNote that this command omits fixtures with leading\n_\nunless the\n-v\noption is added.\nContinue reading\n¶\nCheck out additional pytest resources to help you customize tests for your unique workflow:\n“\nHow to invoke pytest\n” for command line invocation examples\n“\nHow to use pytest with an existing test suite\n” for working with preexisting tests\n“\nHow to mark test functions with attributes\n” for information on the\npytest.mark\nmechanism\n“\nFixtures reference\n” for providing a functional baseline to your tests\n“\nWriting plugins\n” for managing and writing plugins\n“\nGood Integration Practices\n” for virtualenv and test layouts\nOn this page\nGet Started\nInstall\npytest\nCreate your first test\nRun multiple tests\nAssert that a certain exception is raised\nGroup multiple tests in a class\nCompare floating-point values with pytest.approx\nRequest a unique temporary directory for functional tests\nContinue reading", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.pytest.org/en/stable/getting-started.html"}}
{"text": "How to use fixtures - pytest documentation\nContents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nSkip to content\npytest documentation\nGet Started\nHow-to guides\nHow to invoke pytest\nHow to write and report assertions in tests\nHow to use fixtures\nHow to mark test functions with attributes\nHow to parametrize fixtures and test functions\nHow to use subtests\nHow to use temporary directories and files in tests\nHow to monkeypatch/mock modules and environments\nHow to run doctests\nHow to re-run failed tests and maintain state between test runs\nHow to handle test failures\nManaging pytest’s output\nHow to manage logging\nHow to capture stdout/stderr output\nHow to capture warnings\nHow to use skip and xfail to deal with tests that cannot succeed\nHow to install and use plugins\nWriting plugins\nWriting hook functions\nHow to use pytest with an existing test suite\nHow to use\nunittest\n-based tests with pytest\nHow to implement xunit-style set-up\nHow to set up bash completion\nReference guides\nAPI Reference\nFixtures reference\nConfiguration\nExit codes\nPytest Plugin List\nExplanation\nAnatomy of a test\nAbout fixtures\nGood Integration Practices\npytest import mechanisms and\nsys.path\n/\nPYTHONPATH\nTyping in pytest\nCI Pipelines\nFlaky tests\nExamples and customization tricks\nDemo of Python failure reports with pytest\nBasic patterns and examples\nParametrizing tests\nWorking with custom markers\nA session-fixture which can look at all collected tests\nChanging standard (Python) test discovery\nWorking with non-python tests\nUsing a custom directory collector\nAbout the project\nChangelog\nContributing\nBackwards Compatibility Policy\nHistory\nPython version support\nSponsor\npytest for enterprise\nLicense\nContact channels\nUseful links\npytest @ PyPI\npytest @ GitHub\nIssue Tracker\nPDF Documentation\nBack to top\nHow to use fixtures\n¶\nSee also\nAbout fixtures\nSee also\nFixtures reference\n“Requesting” fixtures\n¶\nAt a basic level, test functions request fixtures they require by declaring\nthem as arguments.\nWhen pytest goes to run a test, it looks at the parameters in that test\nfunction’s signature, and then searches for fixtures that have the same names as\nthose parameters. Once pytest finds them, it runs those fixtures, captures what\nthey returned (if anything), and passes those objects into the test function as\narguments.\nQuick example\n¶\nimport\npytest\nclass\nFruit\n:\ndef\n__init__\n(\nself\n,\nname\n):\nself\n.\nname\n=\nname\nself\n.\ncubed\n=\nFalse\ndef\ncube\n(\nself\n):\nself\n.\ncubed\n=\nTrue\nclass\nFruitSalad\n:\ndef\n__init__\n(\nself\n,\n*\nfruit_bowl\n):\nself\n.\nfruit\n=\nfruit_bowl\nself\n.\n_cube_fruit\n()\ndef\n_cube_fruit\n(\nself\n):\nfor\nfruit\nin\nself\n.\nfruit\n:\nfruit\n.\ncube\n()\n# Arrange\n@pytest\n.\nfixture\ndef\nfruit_bowl\n():\nreturn\n[\nFruit\n(\n\"apple\"\n),\nFruit\n(\n\"banana\"\n)]\ndef\ntest_fruit_salad\n(\nfruit_bowl\n):\n# Act\nfruit_salad\n=\nFruitSalad\n(\n*\nfruit_bowl\n)\n# Assert\nassert\nall\n(\nfruit\n.\ncubed\nfor\nfruit\nin\nfruit_salad\n.\nfruit\n)\nIn this example,\ntest_fruit_salad\n“\nrequests\n”\nfruit_bowl\n(i.e.\ndef\ntest_fruit_salad(fruit_bowl):\n), and when pytest sees this, it will\nexecute the\nfruit_bowl\nfixture function and pass the object it returns into\ntest_fruit_salad\nas the\nfruit_bowl\nargument.\nHere’s roughly\nwhat’s happening if we were to do it by hand:\ndef\nfruit_bowl\n():\nreturn\n[\nFruit\n(\n\"apple\"\n),\nFruit\n(\n\"banana\"\n)]\ndef\ntest_fruit_salad\n(\nfruit_bowl\n):\n# Act\nfruit_salad\n=\nFruitSalad\n(\n*\nfruit_bowl\n)\n# Assert\nassert\nall\n(\nfruit\n.\ncubed\nfor\nfruit\nin\nfruit_salad\n.\nfruit\n)\n# Arrange\nbowl\n=\nfruit_bowl\n()\ntest_fruit_salad\n(\nfruit_bowl\n=\nbowl\n)\nFixtures can\nrequest\nother fixtures\n¶\nOne of pytest’s greatest strengths is its extremely flexible fixture system. It\nallows us to boil down complex requirements for tests into more simple and\norganized functions, where we only need to have each one describe the things\nthey are dependent on. We’ll get more into this further down, but for now,\nhere’s a quick example to demonstrate how fixtures can use other fixtures:\n# contents of test_append.py\nimport\npytest\n# Arrange\n@pytest\n.\nfixture\ndef\nfirst_entry\n():\nreturn\n\"a\"\n# Arrange\n@pytest\n.\nfixture\ndef\norder\n(\nfirst_entry\n):\nreturn\n[\nfirst_entry\n]\ndef\ntest_string\n(\norder\n):\n# Act\norder\n.\nappend\n(\n\"b\"\n)\n# Assert\nassert\norder\n==\n[\n\"a\"\n,\n\"b\"\n]\nNotice that this is the same example from above, but very little changed. The\nfixtures in pytest\nrequest\nfixtures just like tests. All the same\nrequesting\nrules apply to fixtures that do for tests. Here’s how this\nexample would work if we did it by hand:\ndef\nfirst_entry\n():\nreturn\n\"a\"\ndef\norder\n(\nfirst_entry\n):\nreturn\n[\nfirst_entry\n]\ndef\ntest_string\n(\norder\n):\n# Act\norder\n.\nappend\n(\n\"b\"\n)\n# Assert\nassert\norder\n==\n[\n\"a\"\n,\n\"b\"\n]\nentry\n=\nfirst_entry\n()\nthe_list\n=\norder\n(\nfirst_entry\n=\nentry\n)\ntest_string\n(\norder\n=\nthe_list\n)\nFixtures are reusable\n¶\nOne of the things that makes pytest’s fixture system so powerful, is that it\ngives us the ability to define a generic setup step that can be reused over and\nover, just like a normal function would be used. Two different tests can request\nthe same fixture and have pytest give each test their own result from that\nfixture.\nThis is extremely useful for making sure tests aren’t affected by each other. We\ncan use this system to make sure each test gets its own fresh batch of data and\nis starting from a clean state so it can provide consistent, repeatable results.\nHere’s an example of how this can come in handy:\n# contents of test_append.py\nimport\npytest\n# Arrange\n@pytest\n.\nfixture\ndef\nfirst_entry\n():\nreturn\n\"a\"\n# Arrange\n@pytest\n.\nfixture\ndef\norder\n(\nfirst_entry\n):\nreturn\n[\nfirst_entry\n]\ndef\ntest_string\n(\norder\n):\n# Act\norder\n.\nappend\n(\n\"b\"\n)\n# Assert\nassert\norder\n==\n[\n\"a\"\n,\n\"b\"\n]\ndef\ntest_int\n(\norder\n):\n# Act\norder\n.\nappend\n(\n2\n)\n# Assert\nassert\norder\n==\n[\n\"a\"\n,\n2\n]\nEach test here is being given its own copy of that\nlist\nobject,\nwhich means the\norder\nfixture is getting executed twice (the same\nis true for the\nfirst_entry\nfixture). If we were to do this by hand as\nwell, it would look something like this:\ndef\nfirst_entry\n():\nreturn\n\"a\"\ndef\norder\n(\nfirst_entry\n):\nreturn\n[\nfirst_entry\n]\ndef\ntest_string\n(\norder\n):\n# Act\norder\n.\nappend\n(\n\"b\"\n)\n# Assert\nassert\norder\n==\n[\n\"a\"\n,\n\"b\"\n]\ndef\ntest_int\n(\norder\n):\n# Act\norder\n.\nappend\n(\n2\n)\n# Assert\nassert\norder\n==\n[\n\"a\"\n,\n2\n]\nentry\n=\nfirst_entry\n()\nthe_list\n=\norder\n(\nfirst_entry\n=\nentry\n)\ntest_string\n(\norder\n=\nthe_list\n)\nentry\n=\nfirst_entry\n()\nthe_list\n=\norder\n(\nfirst_entry\n=\nentry\n)\ntest_int\n(\norder\n=\nthe_list\n)\nA test/fixture can\nrequest\nmore than one fixture at a time\n¶\nTests and fixtures aren’t limited to\nrequesting\na single fixture at a time.\nThey can request as many as they like. Here’s another quick example to\ndemonstrate:\n# contents of test_append.py\nimport\npytest\n# Arrange\n@pytest\n.\nfixture\ndef\nfirst_entry\n():\nreturn\n\"a\"\n# Arrange\n@pytest\n.\nfixture\ndef\nsecond_entry\n():\nreturn\n2\n# Arrange\n@pytest\n.\nfixture\ndef\norder\n(\nfirst_entry\n,\nsecond_entry\n):\nreturn\n[\nfirst_entry\n,\nsecond_entry\n]\n# Arrange\n@pytest\n.\nfixture\ndef\nexpected_list\n():\nreturn\n[\n\"a\"\n,\n2\n,\n3.0\n]\ndef\ntest_string\n(\norder\n,\nexpected_list\n):\n# Act\norder\n.\nappend\n(\n3.0\n)\n# Assert\nassert\norder\n==\nexpected_list\nFixtures can be\nrequested\nmore than once per test (return values are cached)\n¶\nFixtures can also be\nrequested\nmore than once during the same test, and\npytest won’t execute them again for that test. This means we can\nrequest\nfixtures in multiple fixtures that are dependent on them (and even again in the\ntest itself) without those fixtures being executed more than once.\n# contents of test_append.py\nimport\npytest\n# Arrange\n@pytest\n.\nfixture\ndef\nfirst_entry\n():\nreturn\n\"a\"\n# Arrange\n@pytest\n.\nfixture\ndef\norder\n():\nreturn\n[]\n# Act\n@pytest\n.\nfixture\ndef\nappend_first\n(\norder\n,\nfirst_entry\n):\nreturn\norder\n.\nappend\n(\nfirst_entry\n)\ndef\ntest_string_only\n(\nappend_first\n,\norder\n,\nfirst_entry\n):\n# Assert\nassert\norder\n==\n[\nfirst_entry\n]\nIf a\nrequested\nfixture was executed once for every time it was\nrequested\nduring a test, then this test would fail because both\nappend_first\nand\ntest_string_only\nwould see\norder\nas an empty list (i.e.\n[]\n), but\nsince the return value of\norder\nwas cached (along with any side effects\nexecuting it may have had) after the first time it was called, both the test and\nappend_first\nwere referencing the same object, and the test saw the effect\nappend_first\nhad on that object.\nAutouse fixtures (fixtures you don’t have to request)\n¶\nSometimes you may want to have a fixture (or even several) that you know all\nyour tests will depend on. “Autouse” fixtures are a convenient way to make all\ntests automatically\nrequest\nthem. This can cut out a\nlot of redundant\nrequests\n, and can even provide more advanced fixture usage\n(more on that further down).\nWe can make a fixture an autouse fixture by passing in\nautouse=True\nto the\nfixture’s decorator. Here’s a simple example for how they can be used:\n# contents of test_append.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nfirst_entry\n():\nreturn\n\"a\"\n@pytest\n.\nfixture\ndef\norder\n(\nfirst_entry\n):\nreturn\n[]\n@pytest\n.\nfixture\n(\nautouse\n=\nTrue\n)\ndef\nappend_first\n(\norder\n,\nfirst_entry\n):\nreturn\norder\n.\nappend\n(\nfirst_entry\n)\ndef\ntest_string_only\n(\norder\n,\nfirst_entry\n):\nassert\norder\n==\n[\nfirst_entry\n]\ndef\ntest_string_and_int\n(\norder\n,\nfirst_entry\n):\norder\n.\nappend\n(\n2\n)\nassert\norder\n==\n[\nfirst_entry\n,\n2\n]\nIn this example, the\nappend_first\nfixture is an autouse fixture. Because it\nhappens automatically, both tests are affected by it, even though neither test\nrequested\nit. That doesn’t mean they\ncan’t\nbe\nrequested\nthough; just\nthat it isn’t\nnecessary\n.\nScope: sharing fixtures across classes, modules, packages or session\n¶\nFixtures requiring network access depend on connectivity and are\nusually time-expensive to create.  Extending the previous example, we\ncan add a\nscope=\"module\"\nparameter to the\n@pytest.fixture\ninvocation\nto cause a\nsmtp_connection\nfixture function, responsible to create a connection to a preexisting SMTP server, to only be invoked\nonce per test\nmodule\n(the default is to invoke once per test\nfunction\n).\nMultiple test functions in a test module will thus\neach receive the same\nsmtp_connection\nfixture instance, thus saving time.\nPossible values for\nscope\nare:\nfunction\n,\nclass\n,\nmodule\n,\npackage\nor\nsession\n.\nThe next example puts the fixture function into a separate\nconftest.py\nfile\nso that tests from multiple test modules in the directory can\naccess the fixture function:\n# content of conftest.py\nimport\nsmtplib\nimport\npytest\n@pytest\n.\nfixture\n(\nscope\n=\n\"module\"\n)\ndef\nsmtp_connection\n():\nreturn\nsmtplib\n.\nSMTP\n(\n\"smtp.gmail.com\"\n,\n587\n,\ntimeout\n=\n5\n)\n# content of test_module.py\ndef\ntest_ehlo\n(\nsmtp_connection\n):\nresponse\n,\nmsg\n=\nsmtp_connection\n.\nehlo\n()\nassert\nresponse\n==\n250\nassert\nb\n\"smtp.gmail.com\"\nin\nmsg\nassert\n0\n# for demo purposes\ndef\ntest_noop\n(\nsmtp_connection\n):\nresponse\n,\nmsg\n=\nsmtp_connection\n.\nnoop\n()\nassert\nresponse\n==\n250\nassert\n0\n# for demo purposes\nHere, the\ntest_ehlo\nneeds the\nsmtp_connection\nfixture value.  pytest\nwill discover and call the\n@pytest.fixture\nmarked\nsmtp_connection\nfixture function.  Running the test looks like this:\n$ pytest test_module.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y\nrootdir: /home/sweet/project\ncollected 2 items\ntest_module.py\nFF\n[100%]\n================================= FAILURES =================================\n________________________________ test_ehlo _________________________________\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0001>\ndef test_ehlo(smtp_connection):\nresponse, msg = smtp_connection.ehlo()\nassert response == 250\nassert b\"smtp.gmail.com\" in msg\n>       assert 0  # for demo purposes\n^^^^^^^^\nE       assert 0\ntest_module.py\n:7: AssertionError\n________________________________ test_noop _________________________________\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0001>\ndef test_noop(smtp_connection):\nresponse, msg = smtp_connection.noop()\nassert response == 250\n>       assert 0  # for demo purposes\n^^^^^^^^\nE       assert 0\ntest_module.py\n:13: AssertionError\n========================= short test summary info ==========================\nFAILED\ntest_module.py::\ntest_ehlo\n- assert 0\nFAILED\ntest_module.py::\ntest_noop\n- assert 0\n============================\n2 failed\nin 0.12s =============================\nYou see the two\nassert\n0\nfailing and more importantly you can also see\nthat the\nexactly same\nsmtp_connection\nobject was passed into the\ntwo test functions because pytest shows the incoming argument values in the\ntraceback.  As a result, the two test functions using\nsmtp_connection\nrun\nas quick as a single one because they reuse the same instance.\nIf you decide that you rather want to have a session-scoped\nsmtp_connection\ninstance, you can simply declare it:\n@pytest\n.\nfixture\n(\nscope\n=\n\"session\"\n)\ndef\nsmtp_connection\n():\n# the returned fixture value will be shared for\n# all tests requesting it\n...\nFixture scopes\n¶\nFixtures are created when first requested by a test, and are destroyed based on their\nscope\n:\nfunction\n: the default scope, the fixture is destroyed at the end of the test.\nclass\n: the fixture is destroyed during teardown of the last test in the class.\nmodule\n: the fixture is destroyed during teardown of the last test in the module.\npackage\n: the fixture is destroyed during teardown of the last test in the package where the fixture is defined, including sub-packages and sub-directories within it.\nsession\n: the fixture is destroyed at the end of the test session.\nNote\nPytest only caches one instance of a fixture at a time, which\nmeans that when using a parametrized fixture, pytest may invoke a fixture more than once in\nthe given scope.\nDynamic scope\n¶\nAdded in version 5.2.\nIn some cases, you might want to change the scope of the fixture without changing the code.\nTo do that, pass a callable to\nscope\n. The callable must return a string with a valid scope\nand will be executed only once - during the fixture definition. It will be called with two\nkeyword arguments -\nfixture_name\nas a string and\nconfig\nwith a configuration object.\nThis can be especially useful when dealing with fixtures that need time for setup, like spawning\na docker container. You can use the command-line argument to control the scope of the spawned\ncontainers for different environments. See the example below.\ndef\ndetermine_scope\n(\nfixture_name\n,\nconfig\n):\nif\nconfig\n.\ngetoption\n(\n\"--keep-containers\"\n,\nNone\n):\nreturn\n\"session\"\nreturn\n\"function\"\n@pytest\n.\nfixture\n(\nscope\n=\ndetermine_scope\n)\ndef\ndocker_container\n():\nyield\nspawn_container\n()\nTeardown/Cleanup (AKA Fixture finalization)\n¶\nWhen we run our tests, we’ll want to make sure they clean up after themselves so\nthey don’t mess with any other tests (and also so that we don’t leave behind a\nmountain of test data to bloat the system). Fixtures in pytest offer a very\nuseful teardown system, which allows us to define the specific steps necessary\nfor each fixture to clean up after itself.\nThis system can be leveraged in two ways.\n1.\nyield\nfixtures (recommended)\n¶\n“Yield” fixtures\nyield\ninstead of\nreturn\n. With these\nfixtures, we can run some code and pass an object back to the requesting\nfixture/test, just like with the other fixtures. The only differences are:\nreturn\nis swapped out for\nyield\n.\nAny teardown code for that fixture is placed\nafter\nthe\nyield\n.\nOnce pytest figures out a linear order for the fixtures, it will run each one up\nuntil it returns or yields, and then move on to the next fixture in the list to\ndo the same thing.\nOnce the test is finished, pytest will go back down the list of fixtures, but in\nthe\nreverse order\n, taking each one that yielded, and running the code inside\nit that was\nafter\nthe\nyield\nstatement.\nAs a simple example, consider this basic email module:\n# content of emaillib.py\nclass\nMailAdminClient\n:\ndef\ncreate_user\n(\nself\n):\nreturn\nMailUser\n()\ndef\ndelete_user\n(\nself\n,\nuser\n):\n# do some cleanup\npass\nclass\nMailUser\n:\ndef\n__init__\n(\nself\n):\nself\n.\ninbox\n=\n[]\ndef\nsend_email\n(\nself\n,\nemail\n,\nother\n):\nother\n.\ninbox\n.\nappend\n(\nemail\n)\ndef\nclear_mailbox\n(\nself\n):\nself\n.\ninbox\n.\nclear\n()\nclass\nEmail\n:\ndef\n__init__\n(\nself\n,\nsubject\n,\nbody\n):\nself\n.\nsubject\n=\nsubject\nself\n.\nbody\n=\nbody\nLet’s say we want to test sending email from one user to another. We’ll have to\nfirst make each user, then send the email from one user to the other, and\nfinally assert that the other user received that message in their inbox. If we\nwant to clean up after the test runs, we’ll likely have to make sure the other\nuser’s mailbox is emptied before deleting that user, otherwise the system may\ncomplain.\nHere’s what that might look like:\n# content of test_emaillib.py\nfrom\nemaillib\nimport\nEmail\n,\nMailAdminClient\nimport\npytest\n@pytest\n.\nfixture\ndef\nmail_admin\n():\nreturn\nMailAdminClient\n()\n@pytest\n.\nfixture\ndef\nsending_user\n(\nmail_admin\n):\nuser\n=\nmail_admin\n.\ncreate_user\n()\nyield\nuser\nmail_admin\n.\ndelete_user\n(\nuser\n)\n@pytest\n.\nfixture\ndef\nreceiving_user\n(\nmail_admin\n):\nuser\n=\nmail_admin\n.\ncreate_user\n()\nyield\nuser\nuser\n.\nclear_mailbox\n()\nmail_admin\n.\ndelete_user\n(\nuser\n)\ndef\ntest_email_received\n(\nsending_user\n,\nreceiving_user\n):\nemail\n=\nEmail\n(\nsubject\n=\n\"Hey!\"\n,\nbody\n=\n\"How's it going?\"\n)\nsending_user\n.\nsend_email\n(\nemail\n,\nreceiving_user\n)\nassert\nemail\nin\nreceiving_user\n.\ninbox\nBecause\nreceiving_user\nis the last fixture to run during setup, it’s the first to run\nduring teardown.\nThere is a risk that even having the order right on the teardown side of things\ndoesn’t guarantee a safe cleanup. That’s covered in a bit more detail in\nSafe teardowns\n.\n$ pytest -q test_emaillib.py\n.\n[100%]\n1 passed\nin 0.12s\nHandling errors for yield fixture\n¶\nIf a yield fixture raises an exception before yielding, pytest won’t try to run\nthe teardown code after that yield fixture’s\nyield\nstatement. But, for every\nfixture that has already run successfully for that test, pytest will still\nattempt to tear them down as it normally would.\n2. Adding finalizers directly\n¶\nWhile yield fixtures are considered to be the cleaner and more straightforward\noption, there is another choice, and that is to add “finalizer” functions\ndirectly to the test’s\nrequest-context\nobject. It brings a similar result as\nyield fixtures, but requires a bit more verbosity.\nIn order to use this approach, we have to request the\nrequest-context\nobject\n(just like we would request another fixture) in the fixture we need to add\nteardown code for, and then pass a callable, containing that teardown code, to\nits\naddfinalizer\nmethod.\nWe have to be careful though, because pytest will run that finalizer once it’s\nbeen added, even if that fixture raises an exception after adding the finalizer.\nSo to make sure we don’t run the finalizer code when we wouldn’t need to, we\nwould only add the finalizer once the fixture would have done something that\nwe’d need to teardown.\nHere’s how the previous example would look using the\naddfinalizer\nmethod:\n# content of test_emaillib.py\nfrom\nemaillib\nimport\nEmail\n,\nMailAdminClient\nimport\npytest\n@pytest\n.\nfixture\ndef\nmail_admin\n():\nreturn\nMailAdminClient\n()\n@pytest\n.\nfixture\ndef\nsending_user\n(\nmail_admin\n):\nuser\n=\nmail_admin\n.\ncreate_user\n()\nyield\nuser\nmail_admin\n.\ndelete_user\n(\nuser\n)\n@pytest\n.\nfixture\ndef\nreceiving_user\n(\nmail_admin\n,\nrequest\n):\nuser\n=\nmail_admin\n.\ncreate_user\n()\ndef\ndelete_user\n():\nmail_admin\n.\ndelete_user\n(\nuser\n)\nrequest\n.\naddfinalizer\n(\ndelete_user\n)\nreturn\nuser\n@pytest\n.\nfixture\ndef\nemail\n(\nsending_user\n,\nreceiving_user\n,\nrequest\n):\n_email\n=\nEmail\n(\nsubject\n=\n\"Hey!\"\n,\nbody\n=\n\"How's it going?\"\n)\nsending_user\n.\nsend_email\n(\n_email\n,\nreceiving_user\n)\ndef\nempty_mailbox\n():\nreceiving_user\n.\nclear_mailbox\n()\nrequest\n.\naddfinalizer\n(\nempty_mailbox\n)\nreturn\n_email\ndef\ntest_email_received\n(\nreceiving_user\n,\nemail\n):\nassert\nemail\nin\nreceiving_user\n.\ninbox\nIt’s a bit longer than yield fixtures and a bit more complex, but it\ndoes offer some nuances for when you’re in a pinch.\n$ pytest -q test_emaillib.py\n.\n[100%]\n1 passed\nin 0.12s\nNote on finalizer order\n¶\nFinalizers are executed in a first-in-last-out order.\nFor yield fixtures, the first teardown code to run is from the right-most fixture, i.e. the last test parameter.\n# content of test_finalizers.py\nimport\npytest\ndef\ntest_bar\n(\nfix_w_yield1\n,\nfix_w_yield2\n):\nprint\n(\n\"test_bar\"\n)\n@pytest\n.\nfixture\ndef\nfix_w_yield1\n():\nyield\nprint\n(\n\"after_yield_1\"\n)\n@pytest\n.\nfixture\ndef\nfix_w_yield2\n():\nyield\nprint\n(\n\"after_yield_2\"\n)\n$ pytest -s test_finalizers.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y\nrootdir: /home/sweet/project\ncollected 1 item\ntest_finalizers.py test_bar\n.after_yield_2\nafter_yield_1\n============================\n1 passed\nin 0.12s =============================\nFor finalizers, the first fixture to run is last call to\nrequest.addfinalizer\n.\n# content of test_finalizers.py\nfrom\nfunctools\nimport\npartial\nimport\npytest\n@pytest\n.\nfixture\ndef\nfix_w_finalizers\n(\nrequest\n):\nrequest\n.\naddfinalizer\n(\npartial\n(\nprint\n,\n\"finalizer_2\"\n))\nrequest\n.\naddfinalizer\n(\npartial\n(\nprint\n,\n\"finalizer_1\"\n))\ndef\ntest_bar\n(\nfix_w_finalizers\n):\nprint\n(\n\"test_bar\"\n)\n$ pytest -s test_finalizers.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y\nrootdir: /home/sweet/project\ncollected 1 item\ntest_finalizers.py test_bar\n.finalizer_1\nfinalizer_2\n============================\n1 passed\nin 0.12s =============================\nThis is so because yield fixtures use\naddfinalizer\nbehind the scenes: when the fixture executes,\naddfinalizer\nregisters a function that resumes the generator, which in turn calls the teardown code.\nSafe teardowns\n¶\nThe fixture system of pytest is\nvery\npowerful, but it’s still being run by a\ncomputer, so it isn’t able to figure out how to safely teardown everything we\nthrow at it. If we aren’t careful, an error in the wrong spot might leave stuff\nfrom our tests behind, and that can cause further issues pretty quickly.\nFor example, consider the following tests (based off of the mail example from\nabove):\n# content of test_emaillib.py\nfrom\nemaillib\nimport\nEmail\n,\nMailAdminClient\nimport\npytest\n@pytest\n.\nfixture\ndef\nsetup\n():\nmail_admin\n=\nMailAdminClient\n()\nsending_user\n=\nmail_admin\n.\ncreate_user\n()\nreceiving_user\n=\nmail_admin\n.\ncreate_user\n()\nemail\n=\nEmail\n(\nsubject\n=\n\"Hey!\"\n,\nbody\n=\n\"How's it going?\"\n)\nsending_user\n.\nsend_email\n(\nemail\n,\nreceiving_user\n)\nyield\nreceiving_user\n,\nemail\nreceiving_user\n.\nclear_mailbox\n()\nmail_admin\n.\ndelete_user\n(\nsending_user\n)\nmail_admin\n.\ndelete_user\n(\nreceiving_user\n)\ndef\ntest_email_received\n(\nsetup\n):\nreceiving_user\n,\nemail\n=\nsetup\nassert\nemail\nin\nreceiving_user\n.\ninbox\nThis version is a lot more compact, but it’s also harder to read, doesn’t have a\nvery descriptive fixture name, and none of the fixtures can be reused easily.\nThere’s also a more serious issue, which is that if any of those steps in the\nsetup raise an exception, none of the teardown code will run.\nOne option might be to go with the\naddfinalizer\nmethod instead of yield\nfixtures, but that might get pretty complex and difficult to maintain (and it\nwouldn’t be compact anymore).\n$ pytest -q test_emaillib.py\n.\n[100%]\n1 passed\nin 0.12s\nSafe fixture structure\n¶\nThe safest and simplest fixture structure requires limiting fixtures to only\nmaking one state-changing action each, and then bundling them together with\ntheir teardown code, as\nthe email examples above\nshowed.\nThe chance that a state-changing operation can fail but still modify state is\nnegligible, as most of these operations tend to be\ntransaction\n-based (at least at the\nlevel of testing where state could be left behind). So if we make sure that any\nsuccessful state-changing action gets torn down by moving it to a separate\nfixture function and separating it from other, potentially failing\nstate-changing actions, then our tests will stand the best chance at leaving\nthe test environment the way they found it.\nFor an example, let’s say we have a website with a login page, and we have\naccess to an admin API where we can generate users. For our test, we want to:\nCreate a user through that admin API\nLaunch a browser using Selenium\nGo to the login page of our site\nLog in as the user we created\nAssert that their name is in the header of the landing page\nWe wouldn’t want to leave that user in the system, nor would we want to leave\nthat browser session running, so we’ll want to make sure the fixtures that\ncreate those things clean up after themselves.\nHere’s what that might look like:\nNote\nFor this example, certain fixtures (i.e.\nbase_url\nand\nadmin_credentials\n) are implied to exist elsewhere. So for now, let’s\nassume they exist, and we’re just not looking at them.\nfrom\nuuid\nimport\nuuid4\nfrom\nurllib.parse\nimport\nurljoin\nfrom\nselenium.webdriver\nimport\nChrome\nimport\npytest\nfrom\nsrc.utils.pages\nimport\nLoginPage\n,\nLandingPage\nfrom\nsrc.utils\nimport\nAdminApiClient\nfrom\nsrc.utils.data_types\nimport\nUser\n@pytest\n.\nfixture\ndef\nadmin_client\n(\nbase_url\n,\nadmin_credentials\n):\nreturn\nAdminApiClient\n(\nbase_url\n,\n**\nadmin_credentials\n)\n@pytest\n.\nfixture\ndef\nuser\n(\nadmin_client\n):\n_user\n=\nUser\n(\nname\n=\n\"Susan\"\n,\nusername\n=\nf\n\"testuser-\n{\nuuid4\n()\n}\n\"\n,\npassword\n=\n\"P4$$word\"\n)\nadmin_client\n.\ncreate_user\n(\n_user\n)\nyield\n_user\nadmin_client\n.\ndelete_user\n(\n_user\n)\n@pytest\n.\nfixture\ndef\ndriver\n():\n_driver\n=\nChrome\n()\nyield\n_driver\n_driver\n.\nquit\n()\n@pytest\n.\nfixture\ndef\nlogin\n(\ndriver\n,\nbase_url\n,\nuser\n):\ndriver\n.\nget\n(\nurljoin\n(\nbase_url\n,\n\"/login\"\n))\npage\n=\nLoginPage\n(\ndriver\n)\npage\n.\nlogin\n(\nuser\n)\n@pytest\n.\nfixture\ndef\nlanding_page\n(\ndriver\n,\nlogin\n):\nreturn\nLandingPage\n(\ndriver\n)\ndef\ntest_name_on_landing_page_after_login\n(\nlanding_page\n,\nuser\n):\nassert\nlanding_page\n.\nheader\n==\nf\n\"Welcome,\n{\nuser\n.\nname\n}\n!\"\nThe way the dependencies are laid out means it’s unclear if the\nuser\nfixture would execute before the\ndriver\nfixture. But that’s ok, because\nthose are atomic operations, and so it doesn’t matter which one runs first\nbecause the sequence of events for the test is still\nlinearizable\n. But what\ndoes\nmatter is\nthat, no matter which one runs first, if the one raises an exception while the\nother would not have, neither will have left anything behind. If\ndriver\nexecutes before\nuser\n, and\nuser\nraises an exception, the driver will\nstill quit, and the user was never made. And if\ndriver\nwas the one to raise\nthe exception, then the driver would never have been started and the user would\nnever have been made.\nRunning multiple\nassert\nstatements safely\n¶\nSometimes you may want to run multiple asserts after doing all that setup, which\nmakes sense as, in more complex systems, a single action can kick off multiple\nbehaviors. pytest has a convenient way of handling this and it combines a bunch\nof what we’ve gone over so far.\nAll that’s needed is stepping up to a larger scope, then having the\nact\nstep defined as an autouse fixture, and finally, making sure all the fixtures\nare targeting that higher level scope.\nLet’s pull\nan example from above\n, and tweak it a\nbit. Let’s say that in addition to checking for a welcome message in the header,\nwe also want to check for a sign out button, and a link to the user’s profile.\nLet’s take a look at how we can structure that so we can run multiple asserts\nwithout having to repeat all those steps again.\nNote\nFor this example, certain fixtures (i.e.\nbase_url\nand\nadmin_credentials\n) are implied to exist elsewhere. So for now, let’s\nassume they exist, and we’re just not looking at them.\n# contents of tests/end_to_end/test_login.py\nfrom\nuuid\nimport\nuuid4\nfrom\nurllib.parse\nimport\nurljoin\nfrom\nselenium.webdriver\nimport\nChrome\nimport\npytest\nfrom\nsrc.utils.pages\nimport\nLoginPage\n,\nLandingPage\nfrom\nsrc.utils\nimport\nAdminApiClient\nfrom\nsrc.utils.data_types\nimport\nUser\n@pytest\n.\nfixture\n(\nscope\n=\n\"class\"\n)\ndef\nadmin_client\n(\nbase_url\n,\nadmin_credentials\n):\nreturn\nAdminApiClient\n(\nbase_url\n,\n**\nadmin_credentials\n)\n@pytest\n.\nfixture\n(\nscope\n=\n\"class\"\n)\ndef\nuser\n(\nadmin_client\n):\n_user\n=\nUser\n(\nname\n=\n\"Susan\"\n,\nusername\n=\nf\n\"testuser-\n{\nuuid4\n()\n}\n\"\n,\npassword\n=\n\"P4$$word\"\n)\nadmin_client\n.\ncreate_user\n(\n_user\n)\nyield\n_user\nadmin_client\n.\ndelete_user\n(\n_user\n)\n@pytest\n.\nfixture\n(\nscope\n=\n\"class\"\n)\ndef\ndriver\n():\n_driver\n=\nChrome\n()\nyield\n_driver\n_driver\n.\nquit\n()\n@pytest\n.\nfixture\n(\nscope\n=\n\"class\"\n)\ndef\nlanding_page\n(\ndriver\n,\nlogin\n):\nreturn\nLandingPage\n(\ndriver\n)\nclass\nTestLandingPageSuccess\n:\n@pytest\n.\nfixture\n(\nscope\n=\n\"class\"\n,\nautouse\n=\nTrue\n)\ndef\nlogin\n(\nself\n,\ndriver\n,\nbase_url\n,\nuser\n):\ndriver\n.\nget\n(\nurljoin\n(\nbase_url\n,\n\"/login\"\n))\npage\n=\nLoginPage\n(\ndriver\n)\npage\n.\nlogin\n(\nuser\n)\ndef\ntest_name_in_header\n(\nself\n,\nlanding_page\n,\nuser\n):\nassert\nlanding_page\n.\nheader\n==\nf\n\"Welcome,\n{\nuser\n.\nname\n}\n!\"\ndef\ntest_sign_out_button\n(\nself\n,\nlanding_page\n):\nassert\nlanding_page\n.\nsign_out_button\n.\nis_displayed\n()\ndef\ntest_profile_link\n(\nself\n,\nlanding_page\n,\nuser\n):\nprofile_href\n=\nurljoin\n(\nbase_url\n,\nf\n\"/profile?id=\n{\nuser\n.\nprofile_id\n}\n\"\n)\nassert\nlanding_page\n.\nprofile_link\n.\nget_attribute\n(\n\"href\"\n)\n==\nprofile_href\nNotice that the methods are only referencing\nself\nin the signature as a\nformality. No state is tied to the actual test class as it might be in the\nunittest.TestCase\nframework. Everything is managed by the pytest fixture\nsystem.\nEach method only has to request the fixtures that it actually needs without\nworrying about order. This is because the\nact\nfixture is an autouse fixture,\nand it made sure all the other fixtures executed before it. There’s no more\nchanges of state that need to take place, so the tests are free to make as many\nnon-state-changing queries as they want without risking stepping on the toes of\nthe other tests.\nThe\nlogin\nfixture is defined inside the class as well, because not every one\nof the other tests in the module will be expecting a successful login, and the\nact\nmay need to\nbe handled a little differently for another test class. For example, if we\nwanted to write another test scenario around submitting bad credentials, we\ncould handle it by adding something like this to the test file:\nclass\nTestLandingPageBadCredentials\n:\n@pytest\n.\nfixture\n(\nscope\n=\n\"class\"\n)\ndef\nfaux_user\n(\nself\n,\nuser\n):\n_user\n=\ndeepcopy\n(\nuser\n)\n_user\n.\npassword\n=\n\"badpass\"\nreturn\n_user\ndef\ntest_raises_bad_credentials_exception\n(\nself\n,\nlogin_page\n,\nfaux_user\n):\nwith\npytest\n.\nraises\n(\nBadCredentialsException\n):\nlogin_page\n.\nlogin\n(\nfaux_user\n)\nFixtures can introspect the requesting test context\n¶\nFixture functions can accept the\nrequest\nobject\nto introspect the “requesting” test function, class or module context.\nFurther extending the previous\nsmtp_connection\nfixture example, let’s\nread an optional server URL from the test module which uses our fixture:\n# content of conftest.py\nimport\nsmtplib\nimport\npytest\n@pytest\n.\nfixture\n(\nscope\n=\n\"module\"\n)\ndef\nsmtp_connection\n(\nrequest\n):\nserver\n=\ngetattr\n(\nrequest\n.\nmodule\n,\n\"smtpserver\"\n,\n\"smtp.gmail.com\"\n)\nsmtp_connection\n=\nsmtplib\n.\nSMTP\n(\nserver\n,\n587\n,\ntimeout\n=\n5\n)\nyield\nsmtp_connection\nprint\n(\nf\n\"finalizing\n{\nsmtp_connection\n}\n(\n{\nserver\n}\n)\"\n)\nsmtp_connection\n.\nclose\n()\nWe use the\nrequest.module\nattribute to optionally obtain an\nsmtpserver\nattribute from the test module.  If we just execute\nagain, nothing much has changed:\n$ pytest -s -q --tb=no test_module.py\nFFfinalizing <smtplib.SMTP object at 0xdeadbeef0002> (smtp.gmail.com)\n========================= short test summary info ==========================\nFAILED\ntest_module.py::\ntest_ehlo\n- assert 0\nFAILED\ntest_module.py::\ntest_noop\n- assert 0\n2 failed\nin 0.12s\nLet’s quickly create another test module that actually sets the\nserver URL in its module namespace:\n# content of test_anothersmtp.py\nsmtpserver\n=\n\"mail.python.org\"\n# will be read by smtp fixture\ndef\ntest_showhelo\n(\nsmtp_connection\n):\nassert\n0\n,\nsmtp_connection\n.\nhelo\n()\nRunning it:\n$ pytest -qq --tb=short test_anothersmtp.py\nF\n[100%]\n================================= FAILURES =================================\n______________________________ test_showhelo _______________________________\ntest_anothersmtp.py\n:6: in test_showhelo\nassert 0, smtp_connection.helo()\nE   AssertionError: (250, b'mail.python.org')\nE   assert 0\n------------------------- Captured stdout teardown -------------------------\nfinalizing <smtplib.SMTP object at 0xdeadbeef0003> (mail.python.org)\n========================= short test summary info ==========================\nFAILED\ntest_anothersmtp.py::\ntest_showhelo\n- AssertionError: (250, b'mail....\nvoila! The\nsmtp_connection\nfixture function picked up our mail server name\nfrom the module namespace.\nUsing markers to pass data to fixtures\n¶\nUsing the\nrequest\nobject, a fixture can also access\nmarkers which are applied to a test function. This can be useful to pass data\ninto a fixture from a test:\nimport\npytest\n@pytest\n.\nfixture\ndef\nfixt\n(\nrequest\n):\nmarker\n=\nrequest\n.\nnode\n.\nget_closest_marker\n(\n\"fixt_data\"\n)\nif\nmarker\nis\nNone\n:\n# Handle missing marker in some way...\ndata\n=\nNone\nelse\n:\ndata\n=\nmarker\n.\nargs\n[\n0\n]\n# Do something with the data\nreturn\ndata\n@pytest\n.\nmark\n.\nfixt_data\n(\n42\n)\ndef\ntest_fixt\n(\nfixt\n):\nassert\nfixt\n==\n42\nFactories as fixtures\n¶\nThe “factory as fixture” pattern can help in situations where the result\nof a fixture is needed multiple times in a single test. Instead of returning\ndata directly, the fixture instead returns a function which generates the data.\nThis function can then be called multiple times in the test.\nFactories can have parameters as needed:\n@pytest\n.\nfixture\ndef\nmake_customer_record\n():\ndef\n_make_customer_record\n(\nname\n):\nreturn\n{\n\"name\"\n:\nname\n,\n\"orders\"\n:\n[]}\nreturn\n_make_customer_record\ndef\ntest_customer_records\n(\nmake_customer_record\n):\ncustomer_1\n=\nmake_customer_record\n(\n\"Lisa\"\n)\ncustomer_2\n=\nmake_customer_record\n(\n\"Mike\"\n)\ncustomer_3\n=\nmake_customer_record\n(\n\"Meredith\"\n)\nIf the data created by the factory requires managing, the fixture can take care of that:\n@pytest\n.\nfixture\ndef\nmake_customer_record\n():\ncreated_records\n=\n[]\ndef\n_make_customer_record\n(\nname\n):\nrecord\n=\nmodels\n.\nCustomer\n(\nname\n=\nname\n,\norders\n=\n[])\ncreated_records\n.\nappend\n(\nrecord\n)\nreturn\nrecord\nyield\n_make_customer_record\nfor\nrecord\nin\ncreated_records\n:\nrecord\n.\ndestroy\n()\ndef\ntest_customer_records\n(\nmake_customer_record\n):\ncustomer_1\n=\nmake_customer_record\n(\n\"Lisa\"\n)\ncustomer_2\n=\nmake_customer_record\n(\n\"Mike\"\n)\ncustomer_3\n=\nmake_customer_record\n(\n\"Meredith\"\n)\nParametrizing fixtures\n¶\nFixture functions can be parametrized in which case they will be called\nmultiple times, each time executing the set of dependent tests, i.e. the\ntests that depend on this fixture.  Test functions usually do not need\nto be aware of their re-running.  Fixture parametrization helps to\nwrite exhaustive functional tests for components which themselves can be\nconfigured in multiple ways.\nExtending the previous example, we can flag the fixture to create two\nsmtp_connection\nfixture instances which will cause all tests using the fixture\nto run twice.  The fixture function gets access to each parameter\nthrough the special\nrequest\nobject:\n# content of conftest.py\nimport\nsmtplib\nimport\npytest\n@pytest\n.\nfixture\n(\nscope\n=\n\"module\"\n,\nparams\n=\n[\n\"smtp.gmail.com\"\n,\n\"mail.python.org\"\n])\ndef\nsmtp_connection\n(\nrequest\n):\nsmtp_connection\n=\nsmtplib\n.\nSMTP\n(\nrequest\n.\nparam\n,\n587\n,\ntimeout\n=\n5\n)\nyield\nsmtp_connection\nprint\n(\nf\n\"finalizing\n{\nsmtp_connection\n}\n\"\n)\nsmtp_connection\n.\nclose\n()\nThe main change is the declaration of\nparams\nwith\n@pytest.fixture\n, a list of values\nfor each of which the fixture function will execute and can access\na value via\nrequest.param\n.  No test function code needs to change.\nSo let’s just do another run:\n$ pytest -q test_module.py\nFFFF\n[100%]\n================================= FAILURES =================================\n________________________ test_ehlo[smtp.gmail.com] _________________________\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0004>\ndef test_ehlo(smtp_connection):\nresponse, msg = smtp_connection.ehlo()\nassert response == 250\nassert b\"smtp.gmail.com\" in msg\n>       assert 0  # for demo purposes\n^^^^^^^^\nE       assert 0\ntest_module.py\n:7: AssertionError\n________________________ test_noop[smtp.gmail.com] _________________________\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0004>\ndef test_noop(smtp_connection):\nresponse, msg = smtp_connection.noop()\nassert response == 250\n>       assert 0  # for demo purposes\n^^^^^^^^\nE       assert 0\ntest_module.py\n:13: AssertionError\n________________________ test_ehlo[mail.python.org] ________________________\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0005>\ndef test_ehlo(smtp_connection):\nresponse, msg = smtp_connection.ehlo()\nassert response == 250\n>       assert b\"smtp.gmail.com\" in msg\nE       AssertionError: assert b'smtp.gmail.com' in b'mail.python.org\\nPIPELINING\\nSIZE 51200000\\nETRN\\nSTARTTLS\\nAUTH DIGEST-MD5 NTLM CRAM-MD5\\nENHANCEDSTATUSCODES\\n8BITMIME\\nDSN\\nSMTPUTF8\\nCHUNKING'\ntest_module.py\n:6: AssertionError\n-------------------------- Captured stdout setup ---------------------------\nfinalizing <smtplib.SMTP object at 0xdeadbeef0004>\n________________________ test_noop[mail.python.org] ________________________\nsmtp_connection = <smtplib.SMTP object at 0xdeadbeef0005>\ndef test_noop(smtp_connection):\nresponse, msg = smtp_connection.noop()\nassert response == 250\n>       assert 0  # for demo purposes\n^^^^^^^^\nE       assert 0\ntest_module.py\n:13: AssertionError\n------------------------- Captured stdout teardown -------------------------\nfinalizing <smtplib.SMTP object at 0xdeadbeef0005>\n========================= short test summary info ==========================\nFAILED\ntest_module.py::\ntest_ehlo[smtp.gmail.com]\n- assert 0\nFAILED\ntest_module.py::\ntest_noop[smtp.gmail.com]\n- assert 0\nFAILED\ntest_module.py::\ntest_ehlo[mail.python.org]\n- AssertionError: asser...\nFAILED\ntest_module.py::\ntest_noop[mail.python.org]\n- assert 0\n4 failed\nin 0.12s\nWe see that our two test functions each ran twice, against the different\nsmtp_connection\ninstances.  Note also, that with the\nmail.python.org\nconnection the second test fails in\ntest_ehlo\nbecause a\ndifferent server string is expected than what arrived.\npytest will build a string that is the test ID for each fixture value\nin a parametrized fixture, e.g.\ntest_ehlo[smtp.gmail.com]\nand\ntest_ehlo[mail.python.org]\nin the above examples.  These IDs can\nbe used with\n-k\nto select specific cases to run, and they will\nalso identify the specific case when one is failing.  Running pytest\nwith\n--collect-only\nwill show the generated IDs.\nNumbers, strings, booleans and\nNone\nwill have their usual string\nrepresentation used in the test ID. For other objects, pytest will\nmake a string based on the argument name.  It is possible to customise\nthe string used in a test ID for a certain fixture value by using the\nids\nkeyword argument:\n# content of test_ids.py\nimport\npytest\n@pytest\n.\nfixture\n(\nparams\n=\n[\n0\n,\n1\n],\nids\n=\n[\n\"spam\"\n,\n\"ham\"\n])\ndef\na\n(\nrequest\n):\nreturn\nrequest\n.\nparam\ndef\ntest_a\n(\na\n):\npass\ndef\nidfn\n(\nfixture_value\n):\nif\nfixture_value\n==\n0\n:\nreturn\n\"eggs\"\nelse\n:\nreturn\nNone\n@pytest\n.\nfixture\n(\nparams\n=\n[\n0\n,\n1\n],\nids\n=\nidfn\n)\ndef\nb\n(\nrequest\n):\nreturn\nrequest\n.\nparam\ndef\ntest_b\n(\nb\n):\npass\nThe above shows how\nids\ncan be either a list of strings to use or\na function which will be called with the fixture value and then\nhas to return a string to use.  In the latter case if the function\nreturns\nNone\nthen pytest’s auto-generated ID will be used.\nRunning the above tests results in the following test IDs being used:\n$ pytest --collect-only\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y\nrootdir: /home/sweet/project\ncollected 12 items\n<Dir fixtures.rst-232>\n<Module test_anothersmtp.py>\n<Function test_showhelo[smtp.gmail.com]>\n<Function test_showhelo[mail.python.org]>\n<Module test_emaillib.py>\n<Function test_email_received>\n<Module test_finalizers.py>\n<Function test_bar>\n<Module test_ids.py>\n<Function test_a[spam]>\n<Function test_a[ham]>\n<Function test_b[eggs]>\n<Function test_b[1]>\n<Module test_module.py>\n<Function test_ehlo[smtp.gmail.com]>\n<Function test_noop[smtp.gmail.com]>\n<Function test_ehlo[mail.python.org]>\n<Function test_noop[mail.python.org]>\n======================= 12 tests collected in 0.12s ========================\nUsing marks with parametrized fixtures\n¶\npytest.param()\ncan be used to apply marks in values sets of parametrized fixtures in the same way\nthat they can be used with\n@pytest.mark.parametrize\n.\nExample:\n# content of test_fixture_marks.py\nimport\npytest\n@pytest\n.\nfixture\n(\nparams\n=\n[\n0\n,\n1\n,\npytest\n.\nparam\n(\n2\n,\nmarks\n=\npytest\n.\nmark\n.\nskip\n)])\ndef\ndata_set\n(\nrequest\n):\nreturn\nrequest\n.\nparam\ndef\ntest_data\n(\ndata_set\n):\npass\nRunning this test will\nskip\nthe invocation of\ndata_set\nwith value\n2\n:\n$ pytest test_fixture_marks.py -v\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y -- $PYTHON_PREFIX/bin/python\ncachedir: .pytest_cache\nrootdir: /home/sweet/project\ncollecting ...\ncollected 3 items\ntest_fixture_marks.py::test_data[0]\nPASSED\n[ 33%]\ntest_fixture_marks.py::test_data[1]\nPASSED\n[ 66%]\ntest_fixture_marks.py::test_data[2]\nSKIPPED\n(unconditional skip)\n[100%]\n=======================\n2 passed\n,\n1 skipped\nin 0.12s =======================\nModularity: using fixtures from a fixture function\n¶\nIn addition to using fixtures in test functions, fixture functions\ncan use other fixtures themselves.  This contributes to a modular design\nof your fixtures and allows reuse of framework-specific fixtures across\nmany projects.  As a simple example, we can extend the previous example\nand instantiate an object\napp\nwhere we stick the already defined\nsmtp_connection\nresource into it:\n# content of test_appsetup.py\nimport\npytest\nclass\nApp\n:\ndef\n__init__\n(\nself\n,\nsmtp_connection\n):\nself\n.\nsmtp_connection\n=\nsmtp_connection\n@pytest\n.\nfixture\n(\nscope\n=\n\"module\"\n)\ndef\napp\n(\nsmtp_connection\n):\nreturn\nApp\n(\nsmtp_connection\n)\ndef\ntest_smtp_connection_exists\n(\napp\n):\nassert\napp\n.\nsmtp_connection\nHere we declare an\napp\nfixture which receives the previously defined\nsmtp_connection\nfixture and instantiates an\nApp\nobject with it.  Let’s run it:\n$ pytest -v test_appsetup.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y -- $PYTHON_PREFIX/bin/python\ncachedir: .pytest_cache\nrootdir: /home/sweet/project\ncollecting ...\ncollected 2 items\ntest_appsetup.py::test_smtp_connection_exists[smtp.gmail.com]\nPASSED\n[ 50%]\ntest_appsetup.py::test_smtp_connection_exists[mail.python.org]\nPASSED\n[100%]\n============================\n2 passed\nin 0.12s =============================\nDue to the parametrization of\nsmtp_connection\n, the test will run twice with two\ndifferent\nApp\ninstances and respective smtp servers.  There is no\nneed for the\napp\nfixture to be aware of the\nsmtp_connection\nparametrization because pytest will fully analyse the fixture dependency graph.\nNote that the\napp\nfixture has a scope of\nmodule\nand uses a\nmodule-scoped\nsmtp_connection\nfixture.  The example would still work if\nsmtp_connection\nwas cached on a\nsession\nscope: it is fine for fixtures to use\n“broader” scoped fixtures but not the other way round:\nA session-scoped fixture could not use a module-scoped one in a\nmeaningful way.\nAutomatic grouping of tests by fixture instances\n¶\npytest minimizes the number of active fixtures during test runs.\nIf you have a parametrized fixture, then all the tests using it will\nfirst execute with one instance and then finalizers are called\nbefore the next fixture instance is created.  Among other things,\nthis eases testing of applications which create and use global state.\nThe following example uses two parametrized fixtures, one of which is\nscoped on a per-module basis, and all the functions perform\nprint\ncalls\nto show the setup/teardown flow:\n# content of test_module.py\nimport\npytest\n@pytest\n.\nfixture\n(\nscope\n=\n\"module\"\n,\nparams\n=\n[\n\"mod1\"\n,\n\"mod2\"\n])\ndef\nmodarg\n(\nrequest\n):\nparam\n=\nrequest\n.\nparam\nprint\n(\n\"  SETUP modarg\"\n,\nparam\n)\nyield\nparam\nprint\n(\n\"  TEARDOWN modarg\"\n,\nparam\n)\n@pytest\n.\nfixture\n(\nscope\n=\n\"function\"\n,\nparams\n=\n[\n1\n,\n2\n])\ndef\notherarg\n(\nrequest\n):\nparam\n=\nrequest\n.\nparam\nprint\n(\n\"  SETUP otherarg\"\n,\nparam\n)\nyield\nparam\nprint\n(\n\"  TEARDOWN otherarg\"\n,\nparam\n)\ndef\ntest_0\n(\notherarg\n):\nprint\n(\n\"  RUN test0 with otherarg\"\n,\notherarg\n)\ndef\ntest_1\n(\nmodarg\n):\nprint\n(\n\"  RUN test1 with modarg\"\n,\nmodarg\n)\ndef\ntest_2\n(\notherarg\n,\nmodarg\n):\nprint\n(\nf\n\"  RUN test2 with otherarg\n{\notherarg\n}\nand modarg\n{\nmodarg\n}\n\"\n)\nLet’s run the tests in verbose mode and with looking at the print-output:\n$ pytest -v -s test_module.py\n=========================== test session starts ============================\nplatform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y -- $PYTHON_PREFIX/bin/python\ncachedir: .pytest_cache\nrootdir: /home/sweet/project\ncollecting ...\ncollected 8 items\ntest_module.py::test_0[1]   SETUP otherarg 1\nRUN test0 with otherarg 1\nPASSED  TEARDOWN otherarg 1\ntest_module.py::test_0[2]   SETUP otherarg 2\nRUN test0 with otherarg 2\nPASSED  TEARDOWN otherarg 2\ntest_module.py::test_1[mod1]   SETUP modarg mod1\nRUN test1 with modarg mod1\nPASSED\ntest_module.py::test_2[mod1-1]   SETUP otherarg 1\nRUN test2 with otherarg 1 and modarg mod1\nPASSED  TEARDOWN otherarg 1\ntest_module.py::test_2[mod1-2]   SETUP otherarg 2\nRUN test2 with otherarg 2 and modarg mod1\nPASSED  TEARDOWN otherarg 2\ntest_module.py::test_1[mod2]   TEARDOWN modarg mod1\nSETUP modarg mod2\nRUN test1 with modarg mod2\nPASSED\ntest_module.py::test_2[mod2-1]   SETUP otherarg 1\nRUN test2 with otherarg 1 and modarg mod2\nPASSED  TEARDOWN otherarg 1\ntest_module.py::test_2[mod2-2]   SETUP otherarg 2\nRUN test2 with otherarg 2 and modarg mod2\nPASSED  TEARDOWN otherarg 2\nTEARDOWN modarg mod2\n============================\n8 passed\nin 0.12s =============================\nYou can see that the parametrized module-scoped\nmodarg\nresource caused an\nordering of test execution that lead to the fewest possible “active” resources.\nThe finalizer for the\nmod1\nparametrized resource was executed before the\nmod2\nresource was setup.\nIn particular notice that test_0 is completely independent and finishes first.\nThen test_1 is executed with\nmod1\n, then test_2 with\nmod1\n, then test_1\nwith\nmod2\nand finally test_2 with\nmod2\n.\nThe\notherarg\nparametrized resource (having function scope) was set up before\nand teared down after every test that used it.\nUse fixtures in classes and modules with\nusefixtures\n¶\nSometimes test functions do not directly need access to a fixture object.\nFor example, tests may require to operate with an empty directory as the\ncurrent working directory but otherwise do not care for the concrete\ndirectory.  Here is how you can use the standard\ntempfile\nand pytest fixtures to\nachieve it.  We separate the creation of the fixture into a\nconftest.py\nfile:\n# content of conftest.py\nimport\nos\nimport\ntempfile\nimport\npytest\n@pytest\n.\nfixture\ndef\ncleandir\n():\nwith\ntempfile\n.\nTemporaryDirectory\n()\nas\nnewpath\n:\nold_cwd\n=\nos\n.\ngetcwd\n()\nos\n.\nchdir\n(\nnewpath\n)\nyield\nos\n.\nchdir\n(\nold_cwd\n)\nand declare its use in a test module via a\nusefixtures\nmarker:\n# content of test_setenv.py\nimport\nos\nimport\npytest\n@pytest\n.\nmark\n.\nusefixtures\n(\n\"cleandir\"\n)\nclass\nTestDirectoryInit\n:\ndef\ntest_cwd_starts_empty\n(\nself\n):\nassert\nos\n.\nlistdir\n(\nos\n.\ngetcwd\n())\n==\n[]\nwith\nopen\n(\n\"myfile\"\n,\n\"w\"\n,\nencoding\n=\n\"utf-8\"\n)\nas\nf\n:\nf\n.\nwrite\n(\n\"hello\"\n)\ndef\ntest_cwd_again_starts_empty\n(\nself\n):\nassert\nos\n.\nlistdir\n(\nos\n.\ngetcwd\n())\n==\n[]\nDue to the\nusefixtures\nmarker, the\ncleandir\nfixture\nwill be required for the execution of each test method, just as if\nyou specified a “cleandir” function argument to each of them.  Let’s run it\nto verify our fixture is activated and the tests pass:\n$ pytest -q\n..\n[100%]\n2 passed\nin 0.12s\nYou can specify multiple fixtures like this:\n@pytest\n.\nmark\n.\nusefixtures\n(\n\"cleandir\"\n,\n\"anotherfixture\"\n)\ndef\ntest\n():\n...\nand you may specify fixture usage at the test module level using\npytestmark\n:\npytestmark\n=\npytest\n.\nmark\n.\nusefixtures\n(\n\"cleandir\"\n)\nIt is also possible to put fixtures required by all tests in your project\ninto a configuration file:\n# content of pytest.toml\n[pytest]\nusefixtures\n=\n[\n\"cleandir\"\n]\nWarning\nNote this mark has no effect in\nfixture functions\n. For example,\nthis\nwill not work as expected\n:\n@pytest\n.\nmark\n.\nusefixtures\n(\n\"my_other_fixture\"\n)\n@pytest\n.\nfixture\ndef\nmy_fixture_that_sadly_wont_use_my_other_fixture\n():\n...\nThis generates a deprecation warning, and will become an error in Pytest 8.\nOverriding fixtures on various levels\n¶\nIn relatively large test suite, you most likely need to\noverride\na\nglobal\nor\nroot\nfixture with a\nlocally\ndefined one, keeping the test code readable and maintainable.\nOverride a fixture on a folder (conftest) level\n¶\nGiven the tests file structure is:\ntests\n/\nconftest\n.\npy\n# content of tests/conftest.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nusername\n():\nreturn\n'username'\ntest_something\n.\npy\n# content of tests/test_something.py\ndef\ntest_username\n(\nusername\n):\nassert\nusername\n==\n'username'\nsubfolder\n/\nconftest\n.\npy\n# content of tests/subfolder/conftest.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nusername\n(\nusername\n):\nreturn\n'overridden-'\n+\nusername\ntest_something_else\n.\npy\n# content of tests/subfolder/test_something_else.py\ndef\ntest_username\n(\nusername\n):\nassert\nusername\n==\n'overridden-username'\nAs you can see, a fixture with the same name can be overridden for certain test folder level.\nNote that the\nbase\nor\nsuper\nfixture can be accessed from the\noverriding\nfixture easily - used in the example above.\nOverride a fixture on a test module level\n¶\nGiven the tests file structure is:\ntests\n/\nconftest\n.\npy\n# content of tests/conftest.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nusername\n():\nreturn\n'username'\ntest_something\n.\npy\n# content of tests/test_something.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nusername\n(\nusername\n):\nreturn\n'overridden-'\n+\nusername\ndef\ntest_username\n(\nusername\n):\nassert\nusername\n==\n'overridden-username'\ntest_something_else\n.\npy\n# content of tests/test_something_else.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nusername\n(\nusername\n):\nreturn\n'overridden-else-'\n+\nusername\ndef\ntest_username\n(\nusername\n):\nassert\nusername\n==\n'overridden-else-username'\nIn the example above, a fixture with the same name can be overridden for certain test module.\nOverride a fixture with direct test parametrization\n¶\nGiven the tests file structure is:\ntests\n/\nconftest\n.\npy\n# content of tests/conftest.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nusername\n():\nreturn\n'username'\n@pytest\n.\nfixture\ndef\nother_username\n(\nusername\n):\nreturn\n'other-'\n+\nusername\ntest_something\n.\npy\n# content of tests/test_something.py\nimport\npytest\n@pytest\n.\nmark\n.\nparametrize\n(\n'username'\n,\n[\n'directly-overridden-username'\n])\ndef\ntest_username\n(\nusername\n):\nassert\nusername\n==\n'directly-overridden-username'\n@pytest\n.\nmark\n.\nparametrize\n(\n'username'\n,\n[\n'directly-overridden-username-other'\n])\ndef\ntest_username_other\n(\nother_username\n):\nassert\nother_username\n==\n'other-directly-overridden-username-other'\nIn the example above, a fixture value is overridden by the test parameter value. Note that the value of the fixture\ncan be overridden this way even if the test doesn’t use it directly (doesn’t mention it in the function prototype).\nOverride a parametrized fixture with non-parametrized one and vice versa\n¶\nGiven the tests file structure is:\ntests\n/\nconftest\n.\npy\n# content of tests/conftest.py\nimport\npytest\n@pytest\n.\nfixture\n(\nparams\n=\n[\n'one'\n,\n'two'\n,\n'three'\n])\ndef\nparametrized_username\n(\nrequest\n):\nreturn\nrequest\n.\nparam\n@pytest\n.\nfixture\ndef\nnon_parametrized_username\n(\nrequest\n):\nreturn\n'username'\ntest_something\n.\npy\n# content of tests/test_something.py\nimport\npytest\n@pytest\n.\nfixture\ndef\nparametrized_username\n():\nreturn\n'overridden-username'\n@pytest\n.\nfixture\n(\nparams\n=\n[\n'one'\n,\n'two'\n,\n'three'\n])\ndef\nnon_parametrized_username\n(\nrequest\n):\nreturn\nrequest\n.\nparam\ndef\ntest_username\n(\nparametrized_username\n):\nassert\nparametrized_username\n==\n'overridden-username'\ndef\ntest_parametrized_username\n(\nnon_parametrized_username\n):\nassert\nnon_parametrized_username\nin\n[\n'one'\n,\n'two'\n,\n'three'\n]\ntest_something_else\n.\npy\n# content of tests/test_something_else.py\ndef\ntest_username\n(\nparametrized_username\n):\nassert\nparametrized_username\nin\n[\n'one'\n,\n'two'\n,\n'three'\n]\ndef\ntest_username\n(\nnon_parametrized_username\n):\nassert\nnon_parametrized_username\n==\n'username'\nIn the example above, a parametrized fixture is overridden with a non-parametrized version, and\na non-parametrized fixture is overridden with a parametrized version for certain test module.\nThe same applies for the test folder level obviously.\nUsing fixtures from other projects\n¶\nUsually projects that provide pytest support will use\nentry points\n,\nso just installing those projects into an environment will make those fixtures available for use.\nIn case you want to use fixtures from a project that does not use entry points, you can\ndefine\npytest_plugins\nin your top\nconftest.py\nfile to register that module\nas a plugin.\nSuppose you have some fixtures in\nmylibrary.fixtures\nand you want to reuse them into your\napp/tests\ndirectory.\nAll you need to do is to define\npytest_plugins\nin\napp/tests/conftest.py\npointing to that module.\npytest_plugins\n=\n\"mylibrary.fixtures\"\nThis effectively registers\nmylibrary.fixtures\nas a plugin, making all its fixtures and\nhooks available to tests in\napp/tests\n.\nNote\nSometimes users will\nimport\nfixtures from other projects for use, however this is not\nrecommended: importing fixtures into a module will register them in pytest\nas\ndefined\nin that module.\nThis has minor consequences, such as appearing multiple times in\npytest\n--help\n,\nbut it is not\nrecommended\nbecause this behavior might change/stop working\nin future versions.\nOn this page\nHow to use fixtures\n“Requesting” fixtures\nQuick example\nFixtures can\nrequest\nother fixtures\nFixtures are reusable\nA test/fixture can\nrequest\nmore than one fixture at a time\nFixtures can be\nrequested\nmore than once per test (return values are cached)\nAutouse fixtures (fixtures you don’t have to request)\nScope: sharing fixtures across classes, modules, packages or session\nFixture scopes\nDynamic scope\nTeardown/Cleanup (AKA Fixture finalization)\n1.\nyield\nfixtures (recommended)\nHandling errors for yield fixture\n2. Adding finalizers directly\nNote on finalizer order\nSafe teardowns\nSafe fixture structure\nRunning multiple\nassert\nstatements safely\nFixtures can introspect the requesting test context\nUsing markers to pass data to fixtures\nFactories as fixtures\nParametrizing fixtures\nUsing marks with parametrized fixtures\nModularity: using fixtures from a fixture function\nAutomatic grouping of tests by fixture instances\nUse fixtures in classes and modules with\nusefixtures\nOverriding fixtures on various levels\nOverride a fixture on a folder (conftest) level\nOverride a fixture on a test module level\nOverride a fixture with direct test parametrization\nOverride a parametrized fixture with non-parametrized one and vice versa\nUsing fixtures from other projects", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.pytest.org/en/stable/how-to/fixtures.html"}}
