{"text": "Getting started with Ansible — Ansible Community Documentation\nBlog\nAnsible community forum\nDocumentation\nAnsible Community Documentation\nGetting started with Ansible\nEdit on GitHub\nGetting started with Ansible\n\nAnsible automates the management of remote systems and controls their desired state.\nAs shown in the preceding figure, most Ansible environments have three main components:\nControl node\nA system on which Ansible is installed.\nYou run Ansible commands such as\nansible\nor\nansible-inventory\non a control node.\nInventory\nA list of managed nodes that are logically organized.\nYou create an inventory on the control node to describe host deployments to Ansible.\nManaged node\nA remote system, or host, that Ansible controls.\nIntroduction to Ansible\nStart automating with Ansible\nBuilding an inventory\nCreating a playbook\nAnsible concepts", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.ansible.com/ansible/latest/getting_started/index.html"}}
{"text": "Using Ansible playbooks — Ansible Community Documentation\nBlog\nAnsible community forum\nDocumentation\nAnsible Community Documentation\nUsing Ansible playbooks\nEdit on GitHub\nUsing Ansible playbooks\n\nNote\nMaking Open Source More Inclusive\nRed Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. We ask that you open an issue or pull request if you come upon a term that we have missed. For more details, see\nour CTO Chris Wright’s message\n.\nWelcome to the Ansible playbooks guide.\nPlaybooks are automation blueprints, in\nYAML\nformat, that Ansible uses to deploy and configure nodes in an inventory.\nThis guide introduces you to playbooks and then covers different use cases for tasks and plays, such as:\nExecuting tasks with elevated privileges or as a different user.\nUsing loops to repeat tasks for items in a list.\nDelegating playbooks to execute tasks on different machines.\nRunning conditional tasks and evaluating conditions with playbook tests.\nUsing blocks to group sets of tasks.\nYou can also learn how to use Ansible playbooks more effectively by using collections, creating reusable files and roles, including and importing playbooks, and running selected parts of a playbook with tags.\nAnsible playbooks\nPlaybook syntax\nPlaybook execution\nAnsible-Pull\nVerifying playbooks\nWorking with playbooks\nTemplating (Jinja2)\nUsing filters to manipulate data\nTests\nLookups\nPython3 in templates\nThe now function: get the current time\nThe undef function: add hint for undefined variables\nLoops\nControlling where tasks run: delegation and local actions\nConditionals\nBlocks\nHandlers: running operations on change\nError handling in playbooks\nSetting the remote environment\nWorking with language-specific version managers\nReusing Ansible artifacts\nRoles\nModule defaults\nInteractive input: prompts\nUsing variables\nDiscovering variables: facts and magic variables\nPlay Argument Validation\nSpecification Format\nSample Specification\nPlaybook Example: Continuous Delivery and Rolling Upgrades\nExecuting playbooks\nValidating tasks: check mode and diff mode\nUnderstanding privilege escalation: become\nTags\nExecuting playbooks for troubleshooting\nDebugging tasks\nAsynchronous actions and polling\nControlling playbook execution: strategies and more\nAdvanced playbook syntax\nUnsafe or raw strings\nYAML anchors and aliases: sharing variable values\nManipulating data\nLoops and list comprehensions\nComplex Type transformations", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.ansible.com/ansible/latest/playbook_guide/index.html"}}
{"text": "Building Ansible inventories — Ansible Community Documentation\nBlog\nAnsible community forum\nDocumentation\nAnsible Community Documentation\nBuilding Ansible inventories\nEdit on GitHub\nBuilding Ansible inventories\n\nNote\nMaking Open Source More Inclusive\nRed Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. We ask that you open an issue or pull request if you come upon a term that we have missed. For more details, see\nour CTO Chris Wright’s message\n.\nWelcome to the guide to building Ansible inventories.\nAn inventory is a list of managed nodes, or hosts, that Ansible deploys and configures.\nThis guide introduces you to inventories and covers the following topics:\nCreating inventories to track a list of servers and devices that you want to automate.\nUsing dynamic inventories to track cloud services with servers and devices that are constantly starting and stopping.\nUsing patterns to automate specific sub-sets of an inventory.\nExpanding and refining the connection methods Ansible uses for your inventory.\nHow to build your inventory\nInventory basics: formats, hosts, and groups\nPassing multiple inventory sources\nOrganizing inventory in a directory\nAdding variables to inventory\nAssigning a variable to one machine: host variables\nDefining variables in INI format\nAssigning a variable to many machines: group variables\nOrganizing host and group variables\nHow variables are merged\nConnecting to hosts: behavioral inventory parameters\nInventory setup examples\nWorking with dynamic inventory\nInventory script example: Cobbler\nOther inventory scripts\nUsing inventory directories and multiple inventory sources\nStatic groups of dynamic groups\nPatterns: targeting hosts and groups\nUsing patterns\nCommon patterns\nLimitations of patterns\nPattern processing order\nAdvanced pattern options\nPatterns and ad-hoc commands\nPatterns and ansible-playbook flags\nConnection methods and details\nControlPersist and paramiko\nSetting a remote user\nSetting up SSH keys\nRunning against localhost\nManaging host key checking\nOther connection methods", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.ansible.com/ansible/latest/inventory_guide/index.html"}}
{"text": "Using Ansible command line tools — Ansible Community Documentation\nBlog\nAnsible community forum\nDocumentation\nAnsible Community Documentation\nUsing Ansible command line tools\nEdit on GitHub\nUsing Ansible command line tools\n\nNote\nMaking Open Source More Inclusive\nRed Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. We ask that you open an issue or pull request if you come upon a term that we have missed. For more details, see\nour CTO Chris Wright’s message\n.\nWelcome to the guide for using Ansible command line tools.\nAnsible provides ad hoc commands and several utilities for performing various operations and automation tasks.\nIntroduction to ad hoc commands\nWhy use ad hoc commands?\nUse cases for ad hoc tasks\nWorking with command line tools\nansible\nansible-config\nansible-console\nansible-doc\nansible-galaxy\nansible-inventory\nansible-playbook\nansible-pull\nansible-vault\nAnsible CLI cheatsheet\nansible-playbook\nansible-galaxy\nansible\nansible-doc\nSee also\nAnsible Navigator\nA command-line tool and a TUI that provides a convenient user interface for most\nof the native Ansible command-line utilities and allows to run Ansible automation content\ninside containers (\nExecution Environments\n)", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://docs.ansible.com/ansible/latest/command_guide/index.html"}}
{"text": "What is Terraform | Terraform | HashiCorp Developer\nHashiConf 2025\nDon't miss the live stream of HashiConf Day 2 happening now\nView live stream\nTerraform\nSearch\nCommand or control key\nK key\nSign in\nSign up\nTheme\nTerraform Home\nWhat is Terraform?\nTerraform is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently.\nHashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.\nHands On:\nTry the Get Started tutorials to start managing infrastructure on popular cloud providers:\nAmazon Web Services\n,\nAzure\n,\nGoogle Cloud Platform\n,\nOracle Cloud Infrastructure\n, and\nDocker\n.\nHow does Terraform work?\nTerraform creates and manages resources on cloud platforms and other services through their application programming interfaces (APIs). Providers enable Terraform to work with virtually any platform or service with an accessible API.\nHashiCorp and the Terraform community have already written\nthousands of providers\nto manage many different types of resources and services. You can find all publicly available providers on the\nTerraform Registry\n, including Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, and many more.\nThe core Terraform workflow consists of three stages:\nWrite:\nYou define resources, which may be across multiple cloud providers and services. For example, you might create a configuration to deploy an application on virtual machines in a Virtual Private Cloud (VPC) network with security groups and a load balancer.\nPlan:\nTerraform creates an execution plan describing the infrastructure it will create, update, or destroy based on the existing infrastructure and your configuration.\nApply:\nOn approval, Terraform performs the proposed operations in the correct order, respecting any resource dependencies. For example, if you update the properties of a VPC and change the number of virtual machines in that VPC, Terraform will recreate the VPC before scaling the virtual machines.\nWhy Terraform?\nHashiCorp co-founder and CTO Armon Dadgar explains how Terraform solves infrastructure challenges.\nManage any infrastructure\nFind providers for many of the platforms and services you already use in the\nTerraform Registry\n. You can also\nwrite your own\n. Terraform takes an\nimmutable approach to infrastructure\n, reducing the complexity of upgrading or modifying your services and infrastructure.\nTrack your infrastructure\nTerraform generates a plan and prompts you for your approval before modifying your infrastructure. It also keeps track of your real infrastructure in a\nstate file\n, which acts as a source of truth for your environment. Terraform uses the state file to determine the changes to make to your infrastructure so that it will match your configuration.\nAutomate changes\nTerraform configuration files are declarative, meaning that they describe the end state of your infrastructure. You do not need to write step-by-step instructions to create resources because Terraform handles the underlying logic. Terraform builds a resource graph to determine resource dependencies and creates or modifies non-dependent resources in parallel. This allows Terraform to provision resources efficiently.\nStandardize configurations\nTerraform supports reusable configuration components called\nmodules\nthat define configurable collections of infrastructure, saving time and encouraging best practices. You can use publicly available modules from the Terraform Registry, or write your own.\nCollaborate\nSince your configuration is written in a file, you can commit it to a Version Control System (VCS) and use\nHCP Terraform\nto efficiently manage Terraform workflows across teams. HCP Terraform runs Terraform in a consistent, reliable environment and provides secure access to shared state and secret data, role-based access controls, a private registry for sharing both modules and providers, and more.\nTip:\nLearn more about\nTerraform use cases\nand\nhow Terraform compares to alternatives\n.\nCommunity\nWe welcome questions, suggestions, and contributions from the community.\nAsk questions in\nHashiCorp Discuss\n.\nRead our\ncontributing guide\n.\nSubmit an issue\nfor bugs and feature requests.\nEdit this page on GitHub", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://developer.hashicorp.com/terraform/intro"}}
{"text": "Overview - Configuration Language | Terraform | HashiCorp Developer\nHashiConf 2025\nDon't miss the live stream of HashiConf Day 2 happening now\nView live stream\nTerraform\nSearch\nCommand or control key\nK key\nSign in\nSign up\nTheme\nTerraform Home\nTerraform Language Documentation\nUse the Terraform configuration language to describe the infrastructure that Terraform manages.\nThis is the documentation for Terraform's configuration language. It is relevant\nto users of\nTerraform CLI\n,\nHCP Terraform\n, and\nTerraform Enterprise\n. Terraform's language is\nits primary user interface. Configuration files you write in Terraform\nlanguage tell Terraform what plugins to install, what infrastructure to create,\nand what data to fetch. Terraform language also lets you define dependencies\nbetween resources and create multiple similar resources from a single\nconfiguration block.\nHands-on:\nTry the\nWrite Terraform Configuration\ntutorials.\nAbout the Terraform Language\nThe main purpose of the Terraform language is declaring\nresources\n, which represent infrastructure objects. All other\nlanguage features exist only to make the definition of resources more flexible\nand convenient.\nA\nTerraform configuration\nis a complete document in the Terraform language\nthat tells Terraform how to manage a given collection of infrastructure. A\nconfiguration can consist of multiple files and directories.\nThe syntax of the Terraform language consists of only a few basic elements:\nresource\n\"aws_vpc\"\n\"main\"\n{\ncidr_block\n=\nvar.base_cidr_block\n}\n<\nBLOCK\nTYPE\n>\n\"<BLOCK LABEL>\"\n\"<BLOCK LABEL>\"\n{\n# Block body\n<\nIDENTIFIER\n>\n=\n<\nEXPRESSION\n>\n# Argument\n}\nBlocks\nare containers for other content and usually represent the\nconfiguration of some kind of object, like a resource. Blocks have a\nblock type,\ncan have zero or more\nlabels,\nand have a\nbody\nthat contains\nany number of arguments and nested blocks. Most of Terraform's features are\ncontrolled by top-level blocks in a configuration file.\nArguments\nassign a value to a name. They appear within blocks.\nExpressions\nrepresent a value, either literally or by referencing and\ncombining other values. They appear as values for arguments, or within other\nexpressions.\nThe Terraform language is declarative, describing an intended goal rather than\nthe steps to reach that goal. The ordering of blocks and the files they are\norganized into are generally not significant; Terraform only considers implicit\nand explicit relationships between resources when determining an order of\noperations.\nExample\nThe following example describes a simple network topology for Amazon Web\nServices, just to give a sense of the overall structure and syntax of the\nTerraform language. Similar configurations can be created for other virtual\nnetwork services, using resource types defined by other providers, and a\npractical network configuration will often contain additional elements not\nshown here.\nterraform\n{\nrequired_providers\n{\naws\n=\n{\nsource\n=\n\"hashicorp/aws\"\nversion\n=\n\"~> 1.0.4\"\n}\n}\n}\nvariable\n\"aws_region\"\n{}\nvariable\n\"base_cidr_block\"\n{\ndescription\n=\n\"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\"\ndefault\n=\n\"10.1.0.0/16\"\n}\nvariable\n\"availability_zones\"\n{\ndescription\n=\n\"A list of availability zones in which to create subnets\"\ntype\n=\nlist\n(\nstring\n)\n}\nprovider\n\"aws\"\n{\nregion\n=\nvar.aws_region\n}\nresource\n\"aws_vpc\"\n\"main\"\n{\n# Referencing the base_cidr_block variable allows the network address\n# to be changed without modifying the configuration.\ncidr_block\n=\nvar.base_cidr_block\n}\nresource\n\"aws_subnet\"\n\"az\"\n{\n# Create one subnet for each given availability zone.\ncount\n=\nlength(var\n.\navailability_zones)\n# For each subnet, use one of the specified availability zones.\navailability_zone\n=\nvar.availability_zones[count.index]\n# By referencing the aws_vpc.main object, Terraform knows that the subnet\n# must be created only after the VPC is created.\nvpc_id\n=\naws_vpc.main.id\n# Built-in functions and operators can be used for simple transformations of\n# values, such as computing a subnet address. Here we create a /20 prefix for\n# each subnet, using consecutive addresses for each availability zone,\n# such as 10.1.16.0/20 .\ncidr_block\n=\ncidrsubnet(aws_vpc\n.\nmain\n.\ncidr_block\n,\n4\n,\ncount\n.\nindex\n+\n1\n)\n}\nEdit this page on GitHub", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://developer.hashicorp.com/terraform/language"}}
{"text": "Terraform CLI Documentation | Terraform | HashiCorp Developer\nHashiConf 2025\nDon't miss the live stream of HashiConf Day 2 happening now\nView live stream\nTerraform\nSearch\nCommand or control key\nK key\nSign in\nSign up\nTheme\nTerraform Home\nTerraform CLI Documentation\nLearn Terraform's CLI-based workflows. You can use the CLI alone or with HCP Terraform or Terraform Enterprise.\nHands-on:\nTry the\nTerraform: Get Started\ntutorials.\nThis documentation provides reference information about Terraform CLI commands,\nas well as instructions for using commands to provision infrastructure and manage the\ninfrastructure lifecyle. It is relevant to anyone working with Terraform's CLI-based\nworkflows, including people who use Terraform CLI by itself, as well as those who\nuse Terraform CLI in conjunction with HCTP Terraform or Terraform Enterprise.\nFor information about the Terraform configuration language syntax and coding patters, refer to the\nTerraform configuration language documentation\n.\nEdit this page on GitHub", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://developer.hashicorp.com/terraform/cli"}}
{"text": "Overview | Prometheus\nPrometheus\nDocs\nDownload\nCommunity\nSupport & Training\nBlog\nCtrl + K\nShow nav\nOverview\nWhat is Prometheus?\nPrometheus\nis an open-source systems\nmonitoring and alerting toolkit originally built at\nSoundCloud\n. Since its inception in 2012, many\ncompanies and organizations have adopted Prometheus, and the project has a very\nactive developer and user\ncommunity\n. It is now a standalone open source project\nand maintained independently of any company. To emphasize this, and to clarify\nthe project's governance structure, Prometheus joined the\nCloud Native Computing Foundation\nin 2016\nas the second hosted project, after\nKubernetes\n.\nPrometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.\nFor more elaborate overviews of Prometheus, see the resources linked from the\nmedia\nsection.\nFeatures\nPrometheus's main features are:\na multi-dimensional\ndata model\nwith time series data identified by metric name and key/value pairs\nPromQL, a\nflexible query language\nto leverage this dimensionality\nno reliance on distributed storage; single server nodes are autonomous\ntime series collection happens via a pull model over HTTP\npushing time series\nis supported via an intermediary gateway\ntargets are discovered via service discovery or static configuration\nmultiple modes of graphing and dashboarding support\nWhat are metrics?\nMetrics are numerical measurements in layperson terms. The term time series refers to the recording of changes over time. What users want to measure differs from application to application. For a web server, it could be request times; for a database, it could be the number of active connections or active queries, and so on.\nMetrics play an important role in understanding why your application is working in a certain way. Let's assume you are running a web application and discover that it is slow. To learn what is happening with your application, you will need some information. For example, when the number of requests is high, the application may become slow. If you have the request count metric, you can determine the cause and increase the number of servers to handle the load.\nComponents\nThe Prometheus ecosystem consists of multiple components, many of which are\noptional:\nthe main\nPrometheus server\nwhich scrapes and stores time series data\nclient libraries\nfor instrumenting application code\na\npush gateway\nfor supporting short-lived jobs\nspecial-purpose\nexporters\nfor services like HAProxy, StatsD, Graphite, etc.\nan\nalertmanager\nto handle alerts\nvarious support tools\nMost Prometheus components are written in\nGo\n, making\nthem easy to build and deploy as static binaries.\nArchitecture\nThis diagram illustrates the architecture of Prometheus and some of\nits ecosystem components:\nPrometheus scrapes metrics from instrumented jobs, either directly or via an\nintermediary push gateway for short-lived jobs. It stores all scraped samples\nlocally and runs rules over this data to either aggregate and record new time\nseries from existing data or generate alerts.\nGrafana\nor\nother API consumers can be used to visualize the collected data.\nWhen does it fit?\nPrometheus works well for recording any purely numeric time series. It fits\nboth machine-centric monitoring as well as monitoring of highly dynamic\nservice-oriented architectures. In a world of microservices, its support for\nmulti-dimensional data collection and querying is a particular strength.\nPrometheus is designed for reliability, to be the system you go to\nduring an outage to allow you to quickly diagnose problems. Each Prometheus\nserver is standalone, not depending on network storage or other remote services.\nYou can rely on it when other parts of your infrastructure are broken, and\nyou do not need to setup extensive infrastructure to use it.\nWhen does it not fit?\nPrometheus values reliability. You can always view what statistics are\navailable about your system, even under failure conditions. If you need 100%\naccuracy, such as for per-request billing, Prometheus is not a good choice as\nthe collected data will likely not be detailed and complete enough. In such a\ncase you would be best off using some other system to collect and analyze the\ndata for billing, and Prometheus for the rest of your monitoring.\nOn this page", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://prometheus.io/docs/introduction/overview/"}}
{"text": "Querying basics | Prometheus\nPrometheus\nDocs\nDownload\nCommunity\nSupport & Training\nBlog\nCtrl + K\nShow nav\nQuerying basics\nPrometheus provides a functional query language called PromQL (Prometheus Query\nLanguage) that lets the user select and aggregate time series data in real\ntime.\nWhen you send a query request to Prometheus, it can be an\ninstant query\n, evaluated at one point in time,\nor a\nrange query\nat equally-spaced steps between a start and an end time. PromQL works exactly the same\nin each case; the range query is just like an instant query run multiple times at different timestamps.\nIn the Prometheus UI, the \"Table\" tab is for instant queries and the \"Graph\" tab is for range queries.\nOther programs can fetch the result of a PromQL expression via the\nHTTP API\n.\nExamples\nThis document is a Prometheus basic language reference. For learning, it may be easier to\nstart with a couple of\nexamples\n.\nExpression language data types\nIn Prometheus's expression language, an expression or sub-expression can\nevaluate to one of four types:\nInstant vector\n- a set of time series containing a single sample for each time series, all sharing the same timestamp\nRange vector\n- a set of time series containing a range of data points over time for each time series\nScalar\n- a simple numeric floating point value\nString\n- a simple string value; currently unused\nDepending on the use case (e.g. when graphing vs. displaying the output of an\nexpression), only some of these types are legal as the result of a\nuser-specified expression.\nFor\ninstant queries\n, any of the above data types are allowed as the root of the expression.\nRange queries\nonly support scalar-typed and instant-vector-typed expressions.\nNotes about the experimental native histograms:\nIngesting native histograms has to be enabled via a\nfeature\nflag\n.\nOnce native histograms have been ingested into the TSDB (and even after\ndisabling the feature flag again), both instant vectors and range vectors may\nnow contain samples that aren't simple floating point numbers (float samples)\nbut complete histograms (histogram samples). A vector may contain a mix of\nfloat samples and histogram samples. Note that the term “histogram sample” in\nthe PromQL documentation always refers to a native histogram. Classic\nhistograms are broken up into a number of series of float samples. From the\nperspective of PromQL, there are no “classic histogram samples”.\nLike float samples, histogram samples can have a counter or a gauge “flavor”,\nmarking them as counter histograms or gauge histograms, respectively. In\ncontrast to float samples, histogram samples “know” their flavor, allowing\nreliable warnings about mismatched operations (e.g. applying the\nrate\nfunction to a range vector of gauge histograms).\nNative histograms can have different bucket layouts, but they are generally\nconvertible to compatible versions to apply binary and aggregation operations\nto them. This is not true for all bucketing schemas. If incompatible\nhistograms are encountered in an operation, the corresponding output vector\nelement is removed from the result, flagged with a warn-level annotation.\nMore details can be found in the\nnative histogram\nspecification\n.\nLiterals\nString literals\nString literals are designated by single quotes, double quotes or backticks.\nPromQL follows the same\nescaping rules as\nGo\n. For string literals in single or double quotes, a\nbackslash begins an escape sequence, which may be followed by\na\n,\nb\n,\nf\n,\nn\n,\nr\n,\nt\n,\nv\nor\n\\\n.  Specific characters can be provided using octal\n(\n\\nnn\n) or hexadecimal (\n\\xnn\n,\n\\unnnn\nand\n\\Unnnnnnnn\n) notations.\nConversely, escape characters are not parsed in string literals designated by backticks. It is important to note that, unlike Go, Prometheus does not discard newlines inside backticks.\nExample:\n\"this is a string\"\n'these are unescaped: \\n \\\\ \\t'\n`these are not unescaped: \\n ' \" \\t`\nFloat literals and time durations\nScalar float values can be written as literal integer or floating-point numbers\nin the format (whitespace only included for better readability):\n[-+]?(\n[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\n| 0[xX][0-9a-fA-F]+\n| [nN][aA][nN]\n| [iI][nN][fF]\n)\nExamples:\n23\n-2.43\n3.4e-9\n0x8f\n-Inf\nNaN\nAdditionally, underscores (\n_\n) can be used in between decimal or hexadecimal\ndigits to improve readability.\nExamples:\n1_000_000\n.123_456_789\n0x_53_AB_F3_82\nFloat literals are also used to specify durations in seconds. For convenience,\ndecimal integer numbers may be combined with the following\ntime units:\nms\n– milliseconds\ns\n– seconds – 1s equals 1000ms\nm\n– minutes – 1m equals 60s (ignoring leap seconds)\nh\n– hours – 1h equals 60m\nd\n– days – 1d equals 24h (ignoring so-called daylight saving time)\nw\n– weeks – 1w equals 7d\ny\n– years – 1y equals 365d (ignoring leap days)\nSuffixing a decimal integer number with one of the units above is a different\nrepresentation of the equivalent number of seconds as a bare float literal.\nExamples:\n1s # Equivalent to 1.\n2m # Equivalent to 120.\n1ms # Equivalent to 0.001.\n-2h # Equivalent to -7200.\nThe following examples do\nnot\nwork:\n0xABm # No suffixing of hexadecimal numbers.\n1.5h # Time units cannot be combined with a floating point.\n+Infd # No suffixing of ±Inf or NaN.\nMultiple units can be combined by concatenation of suffixed integers. Units\nmust be ordered from the longest to the shortest. A given unit must only appear\nonce per float literal.\nExamples:\n1h30m # Equivalent to 5400s and thus 5400.\n12h34m56s # Equivalent to 45296s and thus 45296.\n54s321ms # Equivalent to 54.321.\nTime series selectors\nThese are the basic building-blocks that instruct PromQL what data to fetch.\nInstant vector selectors\nInstant vector selectors allow the selection of a set of time series and a\nsingle sample value for each at a given timestamp (point in time).  In the simplest\nform, only a metric name is specified, which results in an instant vector\ncontaining elements for all time series that have this metric name.\nThe value returned will be that of the most recent sample at or before the\nquery's evaluation timestamp (in the case of an\ninstant query\n)\nor the current step within the query (in the case of a\nrange query\n).\nThe\n@\nmodifier\nallows overriding the timestamp relative to which\nthe selection takes place. Time series are only returned if their most recent sample is less than the\nlookback period\nago.\nThis example selects all time series that have the\nhttp_requests_total\nmetric\nname, returning the most recent sample for each:\nhttp_requests_total\nIt is possible to filter these time series further by appending a comma-separated list of label\nmatchers in curly braces (\n{}\n).\nThis example selects only those time series with the\nhttp_requests_total\nmetric name that also have the\njob\nlabel set to\nprometheus\nand their\ngroup\nlabel set to\ncanary\n:\nhttp_requests_total{job=\"prometheus\",group=\"canary\"}\nIt is also possible to negatively match a label value, or to match label values\nagainst regular expressions. The following label matching operators exist:\n=\n: Select labels that are exactly equal to the provided string.\n!=\n: Select labels that are not equal to the provided string.\n=~\n: Select labels that regex-match the provided string.\n!~\n: Select labels that do not regex-match the provided string.\nRegex\nmatches are fully anchored. A match of\nenv=~\"foo\"\nis treated as\nenv=~\"^foo$\"\n.\nFor example, this selects all\nhttp_requests_total\ntime series for\nstaging\n,\ntesting\n, and\ndevelopment\nenvironments and HTTP methods other than\nGET\n.\nhttp_requests_total{environment=~\"staging|testing|development\",method!=\"GET\"}\nLabel matchers that match empty label values also select all time series that\ndo not have the specific label set at all. It is possible to have multiple matchers for the same label name.\nFor example, given the dataset:\nhttp_requests_total\nhttp_requests_total{replica=\"rep-a\"}\nhttp_requests_total{replica=\"rep-b\"}\nhttp_requests_total{environment=\"development\"}\nThe query\nhttp_requests_total{environment=\"\"}\nwould match and return:\nhttp_requests_total\nhttp_requests_total{replica=\"rep-a\"}\nhttp_requests_total{replica=\"rep-b\"}\nand would exclude:\nhttp_requests_total{environment=\"development\"}\nMultiple matchers can be used for the same label name; they all must pass for a result to be returned.\nThe query:\nhttp_requests_total{replica!=\"rep-a\",replica=~\"rep.*\"}\nWould then match:\nhttp_requests_total{replica=\"rep-b\"}\nVector selectors must either specify a name or at least one label matcher\nthat does not match the empty string. The following expression is illegal:\n{job=~\".*\"} # Bad!\nIn contrast, these expressions are valid as they both have a selector that does not\nmatch empty label values.\n{job=~\".+\"}              # Good!\n{job=~\".*\",method=\"get\"} # Good!\nLabel matchers can also be applied to metric names by matching against the internal\n__name__\nlabel. For example, the expression\nhttp_requests_total\nis equivalent to\n{__name__=\"http_requests_total\"}\n. Matchers other than\n=\n(\n!=\n,\n=~\n,\n!~\n) may also be used.\nThe following expression selects all metrics that have a name starting with\njob:\n:\n{__name__=~\"job:.*\"}\nThe metric name must not be one of the keywords\nbool\n,\non\n,\nignoring\n,\ngroup_left\nand\ngroup_right\n. The following expression is illegal:\non{} # Bad!\nA workaround for this restriction is to use the\n__name__\nlabel:\n{__name__=\"on\"} # Good!\nRange Vector Selectors\nRange vector literals work like instant vector literals, except that they\nselect a range of samples back from the current instant. Syntactically, a\nfloat literal\nis appended in square\nbrackets (\n[]\n) at the end of a vector selector to specify for how many seconds\nback in time values should be fetched for each resulting range vector element.\nCommonly, the float literal uses the syntax with one or more time units, e.g.\n[5m]\n. The range is a left-open and right-closed interval, i.e. samples with\ntimestamps coinciding with the left boundary of the range are excluded from the\nselection, while samples coinciding with the right boundary of the range are\nincluded in the selection.\nIn this example, we select all the values recorded less than 5m ago for all\ntime series that have the metric name\nhttp_requests_total\nand a\njob\nlabel\nset to\nprometheus\n:\nhttp_requests_total{job=\"prometheus\"}[5m]\nOffset modifier\nThe\noffset\nmodifier allows changing the time offset for individual\ninstant and range vectors in a query.\nFor example, the following expression returns the value of\nhttp_requests_total\n5 minutes in the past relative to the current\nquery evaluation time:\nhttp_requests_total offset 5m\nNote that the\noffset\nmodifier always needs to follow the selector\nimmediately, i.e. the following would be correct:\nsum(http_requests_total{method=\"GET\"} offset 5m) // GOOD.\nWhile the following would be\nincorrect\n:\nsum(http_requests_total{method=\"GET\"}) offset 5m // INVALID.\nThe same works for range vectors. This returns the 5-minute\nrate\nthat\nhttp_requests_total\nhad a week ago:\nrate(http_requests_total[5m] offset 1w)\nWhen querying for samples in the past, a negative offset will enable temporal comparisons forward in time:\nrate(http_requests_total[5m] offset -1w)\nNote that this allows a query to look ahead of its evaluation time.\n@ modifier\nThe\n@\nmodifier allows changing the evaluation time for individual instant\nand range vectors in a query. The time supplied to the\n@\nmodifier\nis a Unix timestamp and described with a float literal.\nFor example, the following expression returns the value of\nhttp_requests_total\nat\n2021-01-04T07:40:00+00:00\n:\nhttp_requests_total @ 1609746000\nNote that the\n@\nmodifier always needs to follow the selector\nimmediately, i.e. the following would be correct:\nsum(http_requests_total{method=\"GET\"} @ 1609746000) // GOOD.\nWhile the following would be\nincorrect\n:\nsum(http_requests_total{method=\"GET\"}) @ 1609746000 // INVALID.\nThe same works for range vectors. This returns the 5-minute rate that\nhttp_requests_total\nhad at\n2021-01-04T07:40:00+00:00\n:\nrate(http_requests_total[5m] @ 1609746000)\nThe\n@\nmodifier supports all representations of numeric literals described above.\nIt works with the\noffset\nmodifier where the offset is applied relative to the\n@\nmodifier time.  The results are the same irrespective of the order of the modifiers.\nFor example, these two queries will produce the same result:\n# offset after @\nhttp_requests_total @ 1609746000 offset 5m\n# offset before @\nhttp_requests_total offset 5m @ 1609746000\nAdditionally,\nstart()\nand\nend()\ncan also be used as values for the\n@\nmodifier as special values.\nFor a range query, they resolve to the start and end of the range query respectively and remain the same for all steps.\nFor an instant query,\nstart()\nand\nend()\nboth resolve to the evaluation time.\nhttp_requests_total @ start()\nrate(http_requests_total[5m] @ end())\nNote that the\n@\nmodifier allows a query to look ahead of its evaluation time.\nSubquery\nSubquery allows you to run an instant query for a given range and resolution. The result of a subquery is a range vector.\nSyntax:\n<instant_query> '[' <range> ':' [<resolution>] ']' [ @ <float_literal> ] [ offset <float_literal> ]\n<resolution>\nis optional. Default is the global evaluation interval.\nOperators\nPrometheus supports many binary and aggregation operators. These are described\nin detail in the\nexpression language operators\npage.\nFunctions\nPrometheus supports several functions to operate on data. These are described\nin detail in the\nexpression language functions\npage.\nComments\nPromQL supports line comments that start with\n#\n. Example:\n# This is a comment\nRegular expressions\nAll regular expressions in Prometheus use\nRE2 syntax\n.\nRegex matches are always fully anchored.\nGotchas\nStaleness\nThe timestamps at which to sample data, during a query, are selected\nindependently of the actual present time series data. This is mainly to support\ncases like aggregation (\nsum\n,\navg\n, and so on), where multiple aggregated\ntime series do not precisely align in time. Because of their independence,\nPrometheus needs to assign a value at those timestamps for each relevant time\nseries. It does so by taking the newest sample that is less than the lookback period ago.\nThe lookback period is 5 minutes by default, but can be\nset with the\n--query.lookback-delta\nflag\nor overridden on an individual query via the\nlookback_delta\nparameter.\nIf a target scrape or rule evaluation no longer returns a sample for a time\nseries that was previously present, this time series will be marked as stale.\nIf a target is removed, the previously retrieved time series will be marked as\nstale soon after removal.\nIf a query is evaluated at a sampling timestamp after a time series is marked\nas stale, then no value is returned for that time series. If new samples are\nsubsequently ingested for that time series, they will be returned as expected.\nA time series will go stale when it is no longer exported, or the target no\nlonger exists. Such time series will disappear from graphs\nat the times of their latest collected sample, and they will not be returned\nin queries after they are marked stale.\nSome exporters, which put their own timestamps on samples, get a different behaviour:\nseries that stop being exported take the last value for (by default) 5 minutes before\ndisappearing. The\ntrack_timestamps_staleness\nsetting can change this.\nAvoiding slow queries and overloads\nIf a query needs to operate on a substantial amount of data, graphing it might\ntime out or overload the server or browser. Thus, when constructing queries\nover unknown data, always start building the query in the tabular view of\nPrometheus's expression browser until the result set seems reasonable\n(hundreds, not thousands, of time series at most).  Only when you have filtered\nor aggregated your data sufficiently, switch to graph mode. If the expression\nstill takes too long to graph ad-hoc, pre-record it via a\nrecording\nrule\n.\nThis is especially relevant for Prometheus's query language, where a bare\nmetric name selector like\napi_http_requests_total\ncould expand to thousands\nof time series with different labels. Also, keep in mind that expressions that\naggregate over many time series will generate load on the server even if the\noutput is only a small number of time series. This is similar to how it would\nbe slow to sum all values of a column in a relational database, even if the\noutput value is only a single number.\nOn this page", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://prometheus.io/docs/prometheus/latest/querying/basics/"}}
{"text": "Get started with Grafana Open Source | Grafana documentation\nProducts\nOpen Source\nSolutions\nLearn\nDocs\nPricing\nDownloads\nContact us\nSign in\nCreate free account\nContact us\nProducts\nAll\nProducts\nGrafana Cloud\nMonitor, analyze, and act faster with AI-powered observability.\nGrafana Cloud overview\nLGTM+ Stack\nLogs\npowered by Grafana Loki\nGrafana\nfor visualization\nTraces\npowered by Grafana Tempo\nMetrics\npowered by Grafana Mimir and Prometheus\nKey Capabilities\nAI/ML insights\nIdentify anomalies and reduce toil\nContextual root cause analysis\nAutomated anomaly correlation\nSLO management\nCreate SLOs and error budget alerts\nAlerting\nTrigger alerts from any data source\nPlugins\nConnect Grafana to data sources, apps, and more\nObservability Solutions\nApplication Observability\nMonitor application performance\nInfrastructure observability\nEnsure infrastructure health and performance\nTesting\nPerformance testing\nPowered by Grafana k6\nSynthetic Monitoring\npowered by Grafana k6\nIRM\nIncident response\nRoutine task automation for incidents\nOn-call management\nFlexible on-call management\nOpen Source\nAll\nOpen Source\nGrafana Loki\nMulti-tenant log aggregation system\nGrafana\nQuery, visualize, and alert on data\nGrafana Tempo\nHigh-scale distributed tracing backend\nGrafana Mimir\nScalable and performant metrics backend\nGrafana Pyroscope\nScalable continuous profiling backend\nGrafana Beyla\neBPF auto-instrumentation\nGrafana Faro\nFrontend application observability web SDK\nGrafana Alloy\nOpenTelemetry Collector distribution with Prometheus pipelines\nGrafana k6\nLoad testing for engineering teams\nPrometheus\nMonitor Kubernetes and cloud native\nOpenTelemetry\nInstrument and collect telemetry data\nGraphite\nScalable monitoring for time series data\nAll\nCommunity resources\nDashboard templates\nTry out and share prebuilt visualizations\nPrometheus exporters\nGet your metrics into Prometheus quickly\nSolutions\nAll\nend-to-end solutions\nOpinionated solutions that help you get there easier and faster\nKubernetes Monitoring\nGet K8s health, performance, and cost monitoring from cluster to container\nApplication Observability\nMonitor application performance\nFrontend Observability\nGain real user monitoring insights\nIncident Response & Management\nDetect and respond to incidents with a simplified workflow\nAll monitoring and visualization solutions\nmonitor infrastructure\nOut-of-the-box KPIs, dashboards, and alerts for observability\nLinux\nWindows\nDocker\nPostgres\nMySQL\nAWS\nKafka\nJenkins\nRabbitMQ\nMongoDB\nMicrosoft Azure\nGoogle Cloud\nAll monitoring solutions\nvisualize any data\nInstantly connect all your data sources to Grafana\nMongoDB\nAppDynamics\nOracle\nGitLab\nJira\nSalesforce\nSplunk\nDatadog\nNew Relic\nSnowflake\nAll visualization solutions\nLearn\nAll\nLearn\nCommunity and events\nEvents\nUpcoming in-person and virtual events\nObservabilityCON 2025\nOur flagship observability event\nGrafanaCON 2026\nOur annual OSS community conference\nObservabilityCON on the Road\nOur observability conference on the road\nCommunity\nJoin the Grafana community\nCommunity forums\nAsk the community for help\nResources\nBlog\nNews, releases, cool stories, and more\n4th annual Observability Survey\nShare your thoughts on the state of observability\nBenefits of Observability\nNew research, reports, and insights\nSuccess stories\nBy use case, product, and industry\nHow-to\nDocumentation\nAll the docs\nWebinars and videos\nDemos, webinars, and feature tours\nTutorials\nStep-by-step guides\nWorkshops\nFree, in-person or online\nLearning Journeys\nExpert guidance for mastering our platform\nProfessional Services\nExpert guidance and training\nDocs\nPricing\nHelp build the future of open source observability software\nOpen positions\nCheck out the open source projects we support\nDownloads\nSign in\nGrafana Cloud\nMonitor, analyze, and act faster with AI-powered observability.\nGrafana Cloud overview\nLGTM+ Stack\nLogs\nGrafana\nTraces\nMetrics\nKey Capabilities\nAI/ML insights\nContextual root cause analysis\nSLO management\nAlerting\nPlugins\nObservability Solutions\nApplication Observability\nInfrastructure observability\nTesting\nPerformance testing\nSynthetic Monitoring\nIRM\nIncident response\nOn-call management\nThe actually useful free plan\nGrafana Cloud\nFree Tier\n10k series Prometheus metrics\n50GB logs, 50GB traces, 50GB profiles\n500VUk k6 testing\n20+ Enterprise data source plugins\n100+ pre-built solutions\nCreate account\nGrafana Loki\nMulti-tenant log aggregation system\nGrafana\nQuery, visualize, and alert on data\nGrafana Tempo\nHigh-scale distributed tracing backend\nGrafana Mimir\nScalable and performant metrics backend\nGrafana Pyroscope\nScalable continuous profiling backend\nGrafana Beyla\neBPF auto-instrumentation\nGrafana Faro\nFrontend application observability web SDK\nGrafana Alloy\nOpenTelemetry Collector distribution with Prometheus pipelines\nGrafana k6\nLoad testing for engineering teams\nPrometheus\nMonitor Kubernetes and cloud native\nOpenTelemetry\nInstrument and collect telemetry data\nGraphite\nScalable monitoring for time series data\nAll\nCommunity resources\nDashboard templates\nTry out and share prebuilt visualizations\nPrometheus exporters\nGet your metrics into Prometheus quickly\nend-to-end solutions\nOpinionated solutions that help you get there easier and faster\nKubernetes Monitoring\nGet K8s health, performance, and cost monitoring from cluster to container\nApplication Observability\nMonitor application performance\nFrontend Observability\nGain real user monitoring insights\nIncident Response & Management\nDetect and respond to incidents with a simplified workflow\nmonitor infrastructure\nOut-of-the-box KPIs, dashboards, and alerts for observability\nLinux\nWindows\nDocker\nPostgres\nMySQL\nAWS\nKafka\nJenkins\nRabbitMQ\nMongoDB\nMicrosoft Azure\nGoogle Cloud\nvisualize any data\nInstantly connect all your data sources to Grafana\nMongoDB\nAppDynamics\nOracle\nGitLab\nJira\nSalesforce\nSplunk\nDatadog\nNew Relic\nSnowflake\nAll monitoring and visualization solutions\nCommunity and events\nEvents\nObservabilityCON 2025\nGrafanaCON 2026\nObservabilityCON on the Road\nCommunity\nCommunity forums\nResources\nBlog\n4th annual Observability Survey\nBenefits of Observability\nSuccess stories\nHow-to\nDocumentation\nWebinars and videos\nTutorials\nWorkshops\nLearning Journeys\nProfessional Services\nFeatured webinar\nGetting started with managing your metrics, logs, and traces using Grafana\nLearn how to unify, correlate, and visualize data with dashboards using Grafana.\nLearn more â\nSite search\nAsk Grot AI\nTry using\nGrot AI\nfor this query ->\nMenu\nDocumentation\nGrafana documentation\nIntroduction\nGet started with Grafana Open Source\nEnterprise\nOpen source\nGet started with Grafana Open Source\nYou can use Grafana Cloud to avoid installing, maintaining, and scaling your own instance of Grafana.\nCreate a free account\nto get started, which includes free forever access to 10k metrics, 50GB logs, 50GB traces, 500VUh k6 testing & more.\nGrafana helps you collect, correlate, and visualize data with beautiful dashboards â the open source data visualization and monitoring solution that drives informed decisions, enhances system performance, and streamlines troubleshooting.\nThis section provides guidance to our open source community about how to build your first dashboard after you have installed Grafana. It also provides step-by-step instructions on how to add a Prometheus, InfluxDB, or an MS SQL Server data source. If you are connecting a different data source, please refer to our complete list of supported\nData sources\n. If you would like to learn how to get started with Grafana Cloud, our fully managed observability stack, visit the\nGrafana Cloud documentation\nfor more information.\nBuild your first dashboard\nWas this page helpful?\nYes\nNo\nSuggest an edit in GitHub\nCreate a GitHub issue\nEmail docs@grafana.com\nHelp and support\nCommunity\nRelated resources from Grafana Labs\nAdditional helpful documentation, links, and articles:\nVideo\nGetting started with managing your metrics, logs, and traces using Grafana\nIn this webinar, weâll demo how to get started using the LGTM Stack: Loki for logs, Grafana for visualization, Tempo for traces, and Mimir for metrics.\nVideo\nGetting started with Grafana dashboard design\nIn this webinar, you'll learn how to design stylish and easily accessible Grafana dashboards that tell a story.\nVideo\nBuilding advanced Grafana dashboards\nIn this webinar, weâll demo how to build and format Grafana dashboards.\nIs this page helpful?\nYes\nNo", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://grafana.com/docs/grafana/latest/getting-started/"}}
