{"text": "AMD ROCm documentation — ROCm Documentation\nSkip to main content\nBack to top\nCtrl\n+\nK\nThe ROCm 7.9.0 technology preview release documentation is available at\nROCm Preview documentation\n. For production use, continue to use ROCm 7.1.1 documentation.\nROCm Documentation\nSearch\nCtrl\n+\nK\nAMD ROCm documentation\nAMD ROCm documentation\n#\n2025-11-19\n3 min read time\nApplies to Linux and Windows\nROCm is an open-source software platform optimized to extract HPC and AI workload\nperformance from AMD Instinct GPUs and AMD Radeon GPUs while maintaining\ncompatibility with industry software frameworks. For more information, see\nWhat is ROCm?\nROCm supports multiple programming languages and programming interfaces such as\nHIP (Heterogeneous-Compute Interface for Portability)\n, OpenCL,\nand OpenMP, as explained in the\nProgramming guide\n.\nIf you’re using AMD Radeon GPUs or Ryzen APUs in a workstation setting with a display connected, review\nROCm on Radeon and Ryzen documentation\n.\nROCm documentation is organized into the following categories:\nInstall\nROCm on Linux\nHIP SDK on Windows\nROCm on Radeon and Ryzen\nDeep learning frameworks\nBuild from source\nHow to\nUse ROCm for AI\nAI tutorials\nUse ROCm for HPC\nSystem optimization\nAMD Instinct MI300X performance validation and tuning\nSystem debugging\nUse advanced compiler features\nSet the number of CUs\nTroubleshoot BAR access limitation\nROCm examples\nConceptual\nGPU architecture overview\nFile structure (Linux FHS)\nGPU isolation techniques\nUsing CMake\nInception v3 with PyTorch\nReference\nROCm libraries\nROCm tools, compilers, and runtimes\nGPU hardware specifications\nHardware atomics operation support\nEnvironment variables\nData types and precision support\nGraph safe support", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://rocm.docs.amd.com/en/latest/"}}
{"text": "GPU architecture documentation — ROCm Documentation\nSkip to main content\nBack to top\nCtrl\n+\nK\nThe ROCm 7.9.0 technology preview release documentation is available at\nROCm Preview documentation\n. For production use, continue to use ROCm 7.1.1 documentation.\nROCm Documentation\nSearch\nCtrl\n+\nK\nGPU architecture documentation\nGPU architecture documentation\n#\n2025-10-20\n3 min read time\nApplies to Linux and Windows\nAMD Instinct MI300 Series\nReview hardware aspects of the AMD Instinct™ MI300 Series GPUs and the CDNA™ 3\narchitecture.\nAMD Instinct™ MI300 microarchitecture\nAMD Instinct MI300/CDNA3 ISA\nWhite paper\nMI300 performance counters\nMI350 Series performance counters\nAMD Instinct MI200 Series\nReview hardware aspects of the AMD Instinct™ MI200 Series GPUs and the CDNA™ 2\narchitecture.\nAMD Instinct™ MI250 microarchitecture\nAMD Instinct MI200/CDNA2 ISA\nWhite paper\nPerformance counters\nAMD Instinct MI100\nReview hardware aspects of the AMD Instinct™ MI100 Series GPUs and the CDNA™ 1\narchitecture.\nAMD Instinct™ MI100 microarchitecture\nAMD Instinct MI100/CDNA1 ISA\nWhite paper\nRDNA\nAMD RDNA4 ISA\nAMD RDNA3 ISA\nAMD RDNA2 ISA\nAMD RDNA ISA\nOlder architectures\nAMD Instinct MI50/Vega 7nm ISA\nAMD Instinct MI25/Vega ISA\nAMD GCN3 ISA\nAMD Vega Architecture White Paper", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://rocm.docs.amd.com/en/latest/conceptual/gpu-arch.html"}}
{"text": "ROCm tools, compilers, and runtimes — ROCm Documentation\nSkip to main content\nBack to top\nCtrl\n+\nK\nThe ROCm 7.9.0 technology preview release documentation is available at\nROCm Preview documentation\n. For production use, continue to use ROCm 7.1.1 documentation.\nROCm Documentation\nSearch\nCtrl\n+\nK\nROCm tools, compilers, and runtimes\nROCm tools, compilers, and runtimes\n#\n2025-10-20\n2 min read time\nApplies to Linux and Windows\nSystem Management\nAMD SMI\nROCm Data Center Tool\nrocminfo\nROCm SMI\nROCm Validation Suite\nPerformance\nROCm Bandwidth Test\nROCm Compute Profiler\nROCm Systems Profiler\nROCProfiler\nROCprofiler-SDK\nROCTracer\nDevelopment\nROCm CMake\nHIPIFY\nROCdbgapi\nROCm Debugger (ROCgdb)\nROCr Debug Agent\nCompilers\nROCm Compilers\nHIPCC\nFLANG\nRuntimes\nAMD Compute Language Runtime (CLR)\nHIP\nROCR-Runtime", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://rocm.docs.amd.com/en/latest/reference/rocm-tools.html"}}
{"text": "HIP documentation — HIP 7.1.52802 Documentation\nSkip to main content\nBack to top\nCtrl\n+\nK\nThe ROCm 7.9.0 technology preview release documentation is available at\nROCm Preview documentation\n. For production use, continue to use ROCm 7.1.1 documentation.\nROCm documentation\nHIP 7.1.52802 Documentation\nSearch\nCtrl\n+\nK\nHIP documentation\nHIP documentation\n#\nThe Heterogeneous-computing Interface for Portability (HIP) is a C++ runtime API\nand kernel language that lets you create portable applications for AMD and\nNVIDIA GPUs from a single source code. For more information, see\nWhat is HIP?\nNote\nHIP API 7.0 introduces changes to make it align more closely with NVIDIA CUDA.\nThese changes are incompatible with prior releases, and might require recompiling\nexisting HIP applications for use with the ROCm 7.0 release. For more information,\nsee\nHIP API 7.0 changes\n.\nInstallation instructions are available from:\nInstalling HIP\nBuilding HIP from source\nThe HIP documentation is organized into the following categories:\nProgramming guide\nIntroduction to the HIP programming model\nHardware implementation\nHIP compilers\nPerformance guidelines\nDebugging with HIP\nLogging HIP activity\nUsing HIP runtime API\nHIP C++ language extensions\nKernel language C++ support\nPorting NVIDIA CUDA code to HIP\nProgramming for HIP runtime compiler (RTC)\nAMD compute language runtimes (CLR)\nReference\nHIP runtime API\nHIP math API\nHIP complex math API\nHIP environment variables\nHIP error codes\nCUDA to HIP API Function Comparison\nList of deprecated APIs\nLow Precision Floating Point Types\nHardware features\nTutorial\nHIP basic examples\nHIP examples\nSAXPY tutorial\nGPU programming patterns\nReduction tutorial\nCooperative groups tutorial\nHIP Graph API tutorial\nKnown issues are listed on the\nHIP GitHub repository\n.\nTo contribute features or functions to the HIP project, refer to\nContributing to HIP\n.\nTo contribute to the documentation, refer to\nContributing to ROCm docs\npage.\nYou can find licensing information on the\nLicensing\npage.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://rocm.docs.amd.com/projects/HIP/en/latest/"}}
{"text": "Introduction to the HIP programming model — HIP 7.1.52802 Documentation\nSkip to main content\nBack to top\nCtrl\n+\nK\nThe ROCm 7.9.0 technology preview release documentation is available at\nROCm Preview documentation\n. For production use, continue to use ROCm 7.1.1 documentation.\nROCm documentation\nHIP 7.1.52802 Documentation\nSearch\nCtrl\n+\nK\nIntroduction to the HIP programming model\nContents\nIntroduction to the HIP programming model\n#\nThe HIP programming model enables mapping data-parallel C/C++ algorithms to massively\nparallel SIMD (Single Instruction, Multiple Data) architectures like GPUs. HIP\nsupports many imperative languages, such as Python via PyHIP, but this document\nfocuses on the original C/C++ API of HIP.\nWhile GPUs may be capable of running applications written for CPUs if properly ported\nand compiled, it would not be an efficient use of GPU resources. GPUs fundamentally differ\nfrom CPUs and should be used accordingly to achieve optimum\nperformance. A basic understanding of the underlying device architecture helps you\nmake efficient use of HIP and general purpose graphics processing unit (GPGPU)\nprogramming in general. The following topics introduce you to the key concepts of\nGPU-based programming and the HIP programming model.\nHardware differences: CPU vs GPU\n#\nCPUs and GPUs have been designed for different purposes. CPUs quickly execute a single thread, decreasing the time for a single operation while increasing the number of sequential instructions that can be executed. This includes fetching data and reducing pipeline stalls where the ALU has to wait for previous instructions to finish.\nDifferences in CPUs and GPUs\n#\nWith CPUs, the goal is to quickly process operations. CPUs provide low-latency processing for\nserial instructions. On the other hand, GPUs have been designed to execute many similar commands, or threads,\nin parallel, achieving higher throughput. Latency is the time between starting an\noperation and receiving its result, such as 2 ns, while throughput is the rate of\ncompleted operations, for example, operations per second.\nFor the GPU, the objective is to process as many operations in parallel, rather\nthan to finish a single instruction quickly. GPUs in general are made up of basic\nbuilding blocks called compute units (CUs), that execute the threads of a kernel.\nAs described in\nHardware implementation\n, these CUs provide the necessary\nresources for the threads: the Arithmetic Logical Units (ALUs), register files,\ncaches and shared memory for efficient communication between the threads.\nThe following describes a few hardware differences between CPUs and GPUs:\nCPU:\nOptimized for sequential processing with a few powerful cores (4-64 typically)\nHigh clock speeds (3-5 GHz)\nOne register file per thread. On modern CPUs you have at most 2 register files per core, called hyperthreading.\nOne ALU executing the thread.\nDesigned to quickly execute instructions of the same thread.\nComplex branch prediction.\nLarge L1/L2 cache per core, shared by fewer threads (maximum of 2 when hyperthreading is available).\nA disadvantage is switching execution from one thread to another (or context switching) takes a considerable amount of time: the ALU pipeline needs to be emptied, the register file has to be written to memory to free the register for another thread.\nGPU:\nDesigned for parallel processing with many simpler cores (hundreds/thousands)\nLower clock speeds (1-2 GHz)\nStreamlined control logic\nSmall caches, more registers\nRegister files are shared among threads. The number of threads that can be run in parallel depends on the registers needed per thread.\nMultiple ALUs execute a collection of threads having the same operations, also known as a wavefront or warp. This is called single-instruction, multiple threads (SIMT) operation as described in\nSingle instruction multiple threads (SIMT)\n.\nThe collection of ALUs is called SIMD. SIMDs are an extension to the hardware architecture that allows a\nsingle instruction\nto concurrently operate on\nmultiple data\ninputs.\nFor branching threads where conditional instructions lead to thread divergence, ALUs still process the full wavefront, but the result for divergent threads is masked out. This leads to wasted ALU cycles and should be a consideration in your programming. Keep instructions consistent and leave conditionals out of threads.\nThe advantage for GPUs is that context switching is easy. All threads that run on a core/compute unit have their registers on the compute unit, so they don’t need to be stored to global memory, and each cycle one instruction from any wavefront that resides on the compute unit can be issued.\nWhen programming for a heterogeneous system, which incorporates CPUs and GPUs, you must\nwrite your program to take advantage of the strengths of the available hardware.\nUse the CPU for tasks that require complex logic with conditional branching, to reduce the\ntime to reach a decision. Use the GPU for parallel operations of the same instruction\nacross large datasets, with little branching, where the volume of operations is the key.\nHeterogeneous programming\n#\nThe HIP programming model has two execution contexts. The main application starts on the CPU, or\nthe\nhost\nprocessor, and compute kernels are launched on the\ndevice\nsuch as\nInstinct\naccelerators\nor AMD GPUs.\nThe host execution is defined by the C++ abstract machine, while device execution\nfollows the\nSIMT model\nof HIP. These two execution contexts\nare signified by the\n__host__\nand\n__global__\n(or\n__device__\n) decorators\nin HIP program code. There are a few key differences between the two contexts:\nThe C++ abstract machine assumes a unified memory address space, meaning that\none can always access any given address in memory (assuming the absence of\ndata races). HIP however introduces several memory namespaces, an address\nfrom one means nothing in another. Moreover, not all address spaces are\naccessible from all contexts.\nLooking at the\nAMD Graphics Core Next (GCN) CU\nfigure, you can see that every CU has an instance of storage\nbacking the namespace\n__shared__\n. Even if the host were to have access to these\nregions of memory, the performance benefits of the segmented memory subsystem are\nsupported by the inability of asynchronous access from the host.\nNot all C++ language features map cleanly to typical GPU device architectures.\nSome C++ features have poor latency when implemented on GPU devices, therefore\nthey are forbidden in device contexts to avoid using features that unexpectedly\ndecimate the program’s performance. Offload devices targeted by HIP aren’t general\npurpose devices, at least not in the sense that a CPU is. HIP focuses on data\nparallel computations and as such caters to throughput optimized architectures,\nsuch as GPUs or accelerators derived from GPU architectures.\nAsynchronicity is at the forefront of the HIP API. Computations launched on the device\nexecute asynchronously with respect to the host, and it is the user’s responsibility to\nsynchronize their data dispatch/fetch with computations on the device.\nNote\nHIP performs implicit synchronization on occasions, unlike some\nAPIs where the responsibility for synchronization is left to the user.\nHost programming\n#\nIn heterogeneous programming, the CPU is available for processing operations but the host application has the additional task of managing data and computation exchanges between the CPU (host) and GPU (device). The host acts as the application manager, coordinating the overall workflow and directing operations to the appropriate context, handles data preparation and data transfers, and manages GPU tasks and synchronization. Here is a typical sequence of operations:\nInitialize the HIP runtime and select the GPU: As described in\nInitialization\n, refers to identifying and selecting a target GPU, setting up a context to let the CPU interact with the GPU.\nData preparation: As discussed in\nMemory management\n, this includes allocating the required memory on the host and device, preparing input data and transferring it from the host to the device. The data is both transferred to the device, and passed as an input parameter when launching the kernel.\nConfigure and launch the kernel on the GPU: As described in\nDevice programming\n, this defines kernel configurations and arguments, launches kernel to run on the GPU device using the triple chevron syntax or appropriate API call (for example\nhipLaunchKernelGGL\n). On the GPU, multiple kernels can run on streams, with a queue of operations. Within the same stream, operations run in the order they were issued, but on multiple streams operations are independent and can execute concurrently. In the HIP runtime, kernels run on the default stream when one is not specified, but specifying a stream for the kernel lets you increase concurrency in task scheduling and resource utilization, and launch and manage multiple kernels from the host program.\nSynchronization: As described in\nAsynchronous concurrent execution\n, kernel execution occurs in the context of device streams, specifically the default (\n0\n) stream. You can use streams and events to manage task dependencies, overlap computation with data transfers, and manage asynchronous processes to ensure proper sequencing of operations. Wait for events or streams to finish execution and transfer results from the GPU back to the host.\nError handling: As described in\nError handling\n, you should catch and handle potential errors from API calls, kernel launches, or memory operations. For example, use\nhipGetErrorString\nto retrieve error messages.\nCleanup and resource management: Validate results, clean up GPU contexts and resources, and free allocated memory on the host and devices.\nThis structure allows for efficient use of GPU resources and facilitates the acceleration of compute-intensive tasks while keeping the host CPU available for other tasks.\nInteraction of Host and Device in a GPU application\n#\nDevice programming\n#\nThe device or kernel program acts as workers on the GPU application, distributing operations to be handled quickly and efficiently. Launching a kernel in the host application starts the kernel program running on the GPU, defining the parallel operations to repeat the same instructions across many datasets. Understanding how the kernel works and the processes involved is essential to writing efficient GPU applications. Threads, blocks, and grids provide a hierarchical approach to parallel operations. Understanding the thread hierarchy is critical to distributing work across the available CUs, managing parallel operations, and optimizing memory access. The general flow of the kernel program looks like this:\nThread Grouping: As described in\nHierarchical thread model\n, threads are organized into a hierarchy consisting of threads, which are individual instances of parallel operations, blocks that group the threads, and grids that group blocks into the kernel. Each thread runs an instance of the kernel in parallel with other threads in the block.\nIndexing: The kernel computes the unique index for each thread to access the relevant data to be processed by the thread.\nData Fetch: Threads fetch input data from memory previously transferred from the host to the device. As described in\nMemory model\n, the hierarchy of threads is influenced by the memory subsystem of GPUs. The memory hierarchy includes local memory per-thread with very fast access, shared memory for the block of threads which also supports quick access, and larger amounts of global memory visible to the whole kernel,but accesses are expensive due to high latency. Understanding the memory model is a key concept for kernel programming.\nComputation: Threads perform the required computations on the input data, and generate any needed output. Each thread of the kernel runs the same instruction simultaneously on the different datasets. This sometimes require multiple iterations when the number of operations exceeds the resources of the CU.\nSynchronization: When needed, threads synchronize within their block to ensure correct results when working with shared memory.\nKernels are parallel programs that execute the same instruction set across multiple threads, organized in wavefronts, as described below and as demonstrated in the\nHello World tutorial\nor\nSAXPY - Hello, HIP\n. However, heterogeneous GPU applications can also become quite complex, managing hundreds, thousands, or hundreds of thousands of operations with repeated data transfers between host and device to support massive parallelization, using multiple streams to manage concurrent asynchronous operations, using rich libraries of functions optimized for GPU hardware as described in the\nROCm documentation\n.\nSingle instruction multiple threads (SIMT)\n#\nThe HIP kernel code, written as a series of scalar instructions for multiple\nthreads with different thread indices, gets mapped to the SIMD units of the GPUs.\nEvery single instruction, which is executed for every participating thread of a\nkernel, gets mapped to the SIMD.\nThis is done by grouping threads into warps, which contain as many threads as there\nare physical lanes in a SIMD, and issuing that instruction to the SIMD for every\nwarp of a kernel. Ideally, the SIMD is always fully utilized. However, if the number of threads\ncan’t be evenly divided by the warpSize, then the unused lanes are masked out\nfrom the corresponding SIMD execution.\nInstruction flow of a sample SIMT program\n#\nA kernel follows the same C++ rules as the functions on the host, but it has a special\n__global__\nlabel to mark it for execution on the device, as shown in the following example:\n__global__\nvoid\nAddKernel\n(\nfloat\n*\na\n,\nconst\nfloat\n*\nb\n)\n{\nint\nglobal_idx\n=\nthreadIdx\n.\nx\n+\nblockIdx\n.\nx\n*\nblockDim\n.\nx\n;\na\n[\nglobal_idx\n]\n+=\nb\n[\nglobal_idx\n];\n}\nOne of the first things you might notice is the usage of the special\nthreadIdx\n,\nblockIdx\nand\nblockDim\nvariables. Unlike normal C++ host functions, a kernel\nis not launched once, but as often as specified by the user. Each of these instances\nis a separate thread, with its own values for\nthreadIdx\n,\nblockIdx\nand\nblockDim\n.\nThe kernel program is launched from the host application using a language extension\ncalled the triple chevron syntax, which looks like the following:\nAddKernel\n<<<\nnumber_of_blocks\n,\nthreads_per_block\n>>>\n(\na\n,\nb\n);\nInside the angle brackets, provide the following:\nThe number of blocks to launch, which defines the grid size (relating to blockDim).\nThe number of threads in a block, which defines the block size (relating to blockIdx).\nThe amount of shared memory to allocate by the host, not specified above.\nThe device stream to enqueue the operation on, not specified above so the default stream is used.\nNote\nThe kernel can also be launched through other methods, such as the\nhipLaunchKernel()\nfunction.\nHere, the total number of threads launched for the\nAddKernel\nprogram is defined by\nnumber_of_blocks\n*\nthreads_per_block\n. You define these values when launching the\nkernel program to address the problem to be solved with the available resources within\nthe system. In other words, the thread configuration is customized to the needs of the\noperations and the available hardware.\nFor comparison, the\nAddKernel\nprogram could be written in plain C++ as a\nFOR\nloop:\nfor\n(\nint\ni\n=\n0\n;\ni\n<\n(\nnumber_of_blocks\n*\nthreads_per_block\n);\n++\ni\n){\na\n[\ni\n]\n+=\nb\n[\ni\n];\n}\nIn HIP, lanes of the SIMD architecture are fed by mapping threads of a SIMT\nexecution, one thread down each lane of an SIMD engine. Execution parallelism\nusually isn’t exploited from the width of the built-in vector types, but across\nmultiple threads via the thread ID constants\nthreadIdx.x\n,\nblockIdx.x\n, etc.\nHierarchical thread model\n#\nAs previously discussed, all threads of a kernel are uniquely identified by a set\nof integral values called thread IDs. The hierarchy consists of three levels: thread,\nblocks, and grids.\nThreads are single instances of kernel operations, running concurrently across warps\nBlocks group threads together and enable cooperation and shared memory\nGrids define the number of thread blocks for a single kernel launch\nBlocks and grids can be defined in 3 dimensions (\nx\n,\ny\n,\nz\n)\nBy default, the Y and Z dimensions are set to 1\nThe combined values represent the thread index, and relate to the sequence that the\nthreads execute. The thread hierarchy is integral to how AMD GPUs operate, and is\ndepicted in the following figure.\nHierarchy of thread groups.\n#\nWarp (or Wavefront)\nThe innermost grouping of threads is called a warp. A warp is the most tightly\ncoupled groups of threads, both physically and logically. Threads inside a warp\nare executed in lockstep, with each thread executing the same instruction. Threads\nin a warp are also called lanes, and the value identifying them is the lane ID.\nTip\nLane IDs aren’t queried like other thread IDs, but are user-calculated. As a\nconsequence, they are only as multidimensional as the user interprets the\ncalculated values to be.\nThe size of a warp is architecture dependent and always fixed. For AMD GPUs\nthe warp is typically 64 threads, though sometimes 32 threads. Warps are\nsignified by the set of communication primitives at their disposal, as\ndiscussed in\nWarp cross-lane functions\n.\nBlock\nThe next level of the thread hierarchy is called a thread block, or block. The\ndefining feature of a block is that all threads in the block have shared memory\nthat they can use to share data or synchronize with one another, as described in\nMemory model\n.\nThe size of a block, or the block dimension, is the user-configurable number of\nthreads per block, but is limited by the queryable capabilities of the executing\nhardware. The unique ID of the thread within a block can be 1, 2, or 3-dimensional\nas provided by the HIP API. You can configure the thread block to best represent\nthe data associated with the kernel instruction set.\nNote\nWhen linearizing thread IDs within a block, assume the\nfast index\nis the\nx\ndimension, followed by the\ny\nand\nz\ndimensions.\nGrid\nThe top-most level of the thread hierarchy is a grid. A grid is the number of blocks\nneeded for a single launch of the kernel. The unique ID of each block within\na grid can be 1, 2, or 3-dimensional, as provided by the API and is queryable\nby every thread within the block.\nThe three-dimensional thread hierarchy available to a kernel program lends itself to solutions\nthat align closely to the computational problem. The following are some examples:\n1-dimensional: array processing, linear data structures, or sequential data transformation\n2-dimensional: Image processing, matrix operations, 2 dimensional simulations\n3-dimensional: Volume rendering, 3D scientific simulations, spatial algorithms\nCooperative groups thread model\n#\nThe Cooperative groups API introduces new functions to launch, group, subdivide,\nsynchronize and identify threads, as well as some predefined group-collective\nalgorithms.  Cooperative groups let you define your own set of thread groups which\nmay fit your use-cases better than those defined by the hardware. It relaxes some\nrestrictions of the\nHierarchical thread model\nimposed by the strict 1:1 mapping\nof architectural details to the programming model.\nNote\nThe implicit groups defined by kernel launch parameters are still available\nwhen working with cooperative groups.\nFor further information, see\nCooperative groups\n.\nMemory model\n#\nThe GPU memory architecture is designed to support parallel execution across the\nthread hierarchy. Understanding the following memory spaces and their relationships\nto thread groupings is crucial for efficient GPU programming. The choice of memory\ntype and access patterns significantly impacts kernel performance. The following figure\nsummarizes the memory namespaces and how they relate to the various levels of the\nthreading model.\nMemory hierarchy.\n#\nLocal or per-thread memory\nRead-write storage only visible to the threads defining the given variables,\nalso called per-thread memory. This is the default memory namespace.\nThe size of the blocks for a given kernel, and thereby the number of concurrent\nwarps, are limited by local memory usage. This relates to the\noccupancy\nof the\nCU as described in\nCompute Units\n,\nan important concept in resource usage and performance optimization.\nUse local memory when the data is specific to a thread, to store variables generated\nby the thread, or to provide register pressure relief for the thread.\nShared memory\nRead-write storage visible to all the threads in a given block. Use shared memory\nwhen the data is reused within a thread block, when cross-thread communication\nis needed, or to minimize global memory transactions by using device memory\nwhenever possible.\nGlobal\nRead-write storage visible to all threads in a given grid. There are\nspecialized versions of global memory with different usage semantics which\nare typically backed by the same hardware storing global.\nUse global memory when you have large datasets, are transferring memory between\nthe host and the device, and when you are sharing data between thread blocks.\nConstant\nRead-only storage visible to all threads in a given grid. It is a limited\nsegment of global with queryable size. Use constant memory for read-only data\nthat is shared across multiple threads, and that has a small data size.\nTexture\nRead-only storage visible to all threads in a given grid and accessible\nthrough additional APIs.\nSurface\nA read-write version of texture memory.\nMemory optimizations and best practices\n#\nCoalesced memory accesses\n#\nThe following are a few memory access patterns and best practices to improve performance. You can find additional information in\nMemory management\nand\nPerformance guidelines\n.\nGlobal memory\n: Coalescing reduces the number of memory transactions.\nCoalesced memory access in HIP refers to the optimization of memory transactions to maximize throughput when accessing global memory. When a kernel accesses global memory, the memory transactions typically occur in chunks of 32, 64, or 128 bytes, which must be naturally aligned. Coalescing memory accesses means aligning and organizing these accesses so that multiple threads in a warp can combine their memory requests into the fewest possible transactions. If threads access memory in a coalesced manner, meaning consecutive threads read or write consecutive memory locations, the memory controller can merge these accesses into a single transaction. This is crucial because global memory bandwidth is relatively low compared to on-chip bandwidths, and non-optimal memory accesses can significantly impact performance. If all the threads in a warp can access consecutive memory locations, memory access is fully coalesced.\nTo achieve coalesced memory access in HIP, you should:\nAlign Data\n: Use data types that are naturally aligned and ensure that structures and arrays are aligned properly.\nOptimize Access Patterns\n: Arrange memory accesses so that consecutive threads in a warp access consecutive memory locations. For example, if threads access a 2D array, the array and thread block widths should be multiples of the warp size.\nAvoid strided access\n: For example array[i * stride] can lead to memory bank conflicts and inefficient access.\nPad Data\n: If necessary, pad data structures to ensure alignment and coalescing.\nShared memory\n: Avoiding bank conflicts reduces the serialization of memory transactions.\nShared memory is a small, fast memory region inside the CU. Unlike global memory, shared memory accesses do not require coalescing, but they can suffer from bank conflicts, which are another form of inefficient memory access. Shared memory is divided into multiple memory banks (usually 32 banks on modern GPUs). If multiple threads within a warp try to access different addresses that map to the same memory bank, accesses get serialized, leading to poor performance. To optimize shared memory usage, ensure that consecutive threads access different memory banks. Use padding if necessary to avoid conflicts.\nTexture memory\n: Spatial locality improves caching performance.\nTexture memory is read-only memory optimized for spatial locality and caching rather than coalescing. Texture memory is cached, unlike standard global memory, and it provides optimized access patterns for 2D and spatially local data. Accessing neighboring values results in cache hits, improving performance. Therefore, instead of worrying about coalescing, optimal memory access patterns involve ensuring that threads access spatially adjacent texture elements, and the memory layout aligns well with the 2D caching mechanism.\nUnified memory\n: Structured access reduces the overhead of page migrations.\nUnified memory allows the CPU and GPU to share memory seamlessly, but performance depends on access patterns. Unified memory enables automatic page migration between CPU and GPU memory. However, if different threads access different pages, it can lead to expensive page migrations and slow throughput performance. Accessing unified memory in a structured, warp-friendly manner reduces unnecessary page transfers. Ensure threads access memory in a structured, consecutive manner, minimizing page faults. Prefetch data to the GPU before computation by using\nhipMemPrefetchAsync()\n. In addition, using small batch transfers as described below, can reduce unexpected page migrations when using unified memory.\nSmall batch transfers\n: Enable pipelining and improve PCIe bandwidth use.\nMemory transfers between the host and the device can become a major bottleneck if not optimized. One method is to use small batch memory transfers where data is transferred in smaller chunks instead of dealing with large datasets to avoid long blocking operations. Small batch transfers offer better PCIe bandwidth utilization over large data transfers. Small batch transfers offer performance improvement by offering reduced latency with small batches that run asynchronously using\nhipMemcpyAsync()\nas described in\nAsynchronous concurrent execution\n, pipelining data transfers and kernel execution using separate streams. Finally, using pinned memory with small batch transfers enables faster DMA transfers without CPU involvement, greatly improving memory transfer performance.\nExecution model\n#\nAs previously discussed in\nHeterogeneous programming\n, HIP programs consist of two distinct scopes:\nThe host-side API running on the host processor.\nThe device-side kernels running on GPUs.\nBoth the host and the device-side APIs have synchronous and asynchronous functions.\nHost-side execution\n#\nThe host-side API dealing with device management and their queries are synchronous.\nAll asynchronous APIs, such as kernel execution, data movement and potentially data\nallocation/freeing all happen in the context of device streams, as described in\nManaging streams\n.\nStreams are FIFO buffers of commands to execute relating to a given device.\nOperations that enqueue tasks on a stream all return promptly, and the command is\nexecuted asynchronously. All side effects of a command on a stream are visible\nto all subsequent commands on the same stream. Multiple streams may point to\nthe same device and those streams may be fed from multiple concurrent host-side\nthreads. Execution on multiple streams may be concurrent but isn’t required to\nbe.\nAsynchronous APIs involving a stream all return a stream event, which can be\nused to synchronize the execution of multiple streams. A user may enqueue a\nbarrier onto a stream referencing an event. The barrier will block activity on the\nstream until the operation related to the event completes. After the event completes, all\nside effects of the operation will be visible to subsequent commands even if those\nside effects manifest on different devices.\nMultiple stream workflow\n#\nStreams also support executing user-defined functions as callbacks on the host.\nThe stream will not launch subsequent commands until the callback completes.\nDevice-side execution\n#\nKernels may be launched in multiple ways, all with different syntaxes and\nintended use cases.\nUsing the triple-chevron\n<<<...>>>\noperator on a\n__global__\nannotated\nfunction.\nUsing\nhipLaunchKernelGGL()\non a\n__global__\nannotated function.\nTip\nThis name, by default, is a macro expanding to the triple-chevron syntax. In cases where\nlanguage syntax extensions are undesirable, or where launching templated\nand/or overloaded kernel functions define the\nHIP_TEMPLATE_KERNEL_LAUNCH\npreprocessor macro before including the HIP\nheaders to turn it into a templated function.\nAsynchronous execution\n#\nAsynchronous operations between the host and the kernel provide a variety of opportunities,\nor challenges, for managing synchronization, as described in\nAsynchronous concurrent execution\n.\nFor instance, a basic model would be to launch an asynchronous operation on a kernel\nin a stream, create an event to track the operation, continue operations in the host\nprogram, and when the event shows that the asynchronous operation is complete,  synchronize the kernel to return the results.\nHowever, one of the opportunities of asynchronous operation is the pipelining of operations\nbetween launching kernels and transferring memory. In this case, you would be working\nwith multiple streams running concurrently, or at least overlapping in some regard,\nand managing any dependencies between the streams in the host application.\nThe producer-consumer paradigm can be used to convert a sequential program\ninto parallel operations to improve performance. This process can employ multiple\nstreams to kick off asynchronous kernels, provide data to the kernels, perform operations,\nand return the results for further processing in the host application.\nThese asynchronous activities call for stream management strategies. In the case\nof the single stream, the only management would be the stream synchronization\nwhen the work was complete. However, with multiple streams you have\noverlapping execution of operations and synchronization becomes more complex, as shown\nin the variations of the example in\nProgrammatic dependent launch and synchronization\n.\nYou need to manage each stream’s activities, evaluate the availability of results, evaluate the critical path of the tasks, allocate resources on the hardware, and manage the execution order.\nMulti-GPU and load balancing\n#\nFor applications requiring additional computational power beyond a single device,\nHIP supports utilizing multiple GPUs within a system. Large-scale applications\nthat need more compute power can use multiple GPUs in the system. This enables\nthe runtime to distribute workloads across multiple GPUs to balance the load and prevent some GPUs\nfrom being over-utilized while others are idle.\nFor more information, see\nMulti-device management\n.\nContents", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://rocm.docs.amd.com/projects/HIP/en/latest/user_guide/programming_manual.html"}}
