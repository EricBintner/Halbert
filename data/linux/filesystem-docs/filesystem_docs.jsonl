{"text": "bcachefs\nbcachefs\nRecentChanges\nOther useful links\nRoadmap\nWishlist\nUser manual\nDeveloper documentation\nArchitecture\nBtreeIterators\nBtreeNodes\nBtreeWhiteouts\nEncryption\nTransactions\nSnapshots\nAllocator\nFsck\nTestServerSetup\nStablePages\nOctober 2022 talk for Redhat\n\"The COW filesystem for Linux that won't eat your data\".\nBcachefs is an advanced new filesystem for Linux, with an emphasis on\nreliability and robustness and the complete set of features one would expect\nfrom a modern filesystem.\nCopy on write (COW) - like zfs\nFull data and metadata checksumming, for full data integrity: the filesystem\nshould always detect (and where possible, recover from) damage; it should\nnever return incorrect data.\nMultiple devices\nReplication\nErasure coding\n(incomplete)\nHigh performance: doesn't fragment your writes (like ZFS), no RAID hole\nCaching, data placement\nCompression\nEncryption\nSnapshots\nNocow mode\nReflink\nExtended attributes, ACLs, quotas\nPetabyte scalability\nFull online fsck, check and repair (in progress)\nRobustness and rock solid repair. Damage and breakage are a fact of life,\nit's not a matter of if, but when. It doesn't matter what happened to the\nfilesystem: bad hardware, lightning strikes, an errant dd, you can expect\nthat bcachefs will repair the damage and keep going, usually with no user\nintervention required.\nIt's the job of the filesystem to never lose your data: anything that can be\nrepaired, will be.\nDocumentation\nGetting Started\nUser manual\nFAQ\nDistribution packages, installation instructions, DKMS:\nAs of 6.18, bcachefs is no longer being distributed with the kernel, for\nreasons too complicated to go into here.\n(But it might have something to do with QA).\nSo we're shipping as a DKMS module now. (Like ZFS!). The DKMS package supports\nLinux 6.16 and later.\nNixos, Arch: your distribution bcachefs-tools package has everything you need.\nYou're good to go!\nDebian, Ubuntu:\nhttps://apt.bcachefs.org/\nFedora:\nhttps://copr.fedorainfracloud.org/coprs/ngompa/bcachefs/builds/\nOpenSUSE:\nhttps://build.opensuse.org/package/show/filesystems:bcachefs:release/bcachefs\nRepositories\nhttps://evilpiepirate.org/git/bcachefs.git\nhttps://evilpiepirate.org/git/bcachefs-tools.git\nRelease tarballs\nhttps://evilpiepirate.org/bcachefs-tools/\nDebugging tools\nbcachefs has extensive debugging tools and facilities for inspecting the state\nof the system while running.\nDevelopment tools\nbcachefs development is done with\nktest\n, which is used for both\ninteractive and automated testing (including code coverage analysis), with a\nlarge test suite:\ndashboard\n.\nAbout\nThere are two main themes or ideas that make up bcachefs: copy on write, and\nfilesystem-as-database. These are the two primary tools for achieving\nreliability: a filesystem that won't corrupt or lose your data, and that can\nalways be repaired even when things go horribly wrong.\nHistory - ZFS\nThe history of modern filesystems starts with ZFS. Their primary goal was a\nfilesystem that absolutely guarantees data integrity, and COW is primary\nmechanism by which ZFS achieves that goal: it enables full data and metadata\nchecksumming, with a chain of trust up to a single root, the superblock.\nThis came out of real world experience with failure modes of block storage that\nconventional filesystems weren't able to protect against: bit corruption, lost\nwrites, misdirected writes, and others. COW, coupled with full data and\nmetadata checksumming, makes it possible for the filesystem to definitively\nguard againts all of them: a properly implemented COW filesystem should never\nreturn incorrect data.\nThe push towards copy on write goes back even further in academic research. It\nhad long been realized that it was fundamentally a safer technique than\nupdate-in-place, but ZFS was the first to realize COW in a filesystem and\ndemonstrate the benefits to data integrity.\nFilesystem as a database\nThe other half of bcachefs is \"filesystem as a database\".\nThe original Unix filesystem (and other filesystems of the era are broadly\nsimilar in concept, though the ideas were different) was implented with various\nspecial purpose (but simple - originally) on disk data structures: inodes (file\nmetadata) of fixed size that live in fixed locations on disk, file data stored\nin blocks (not extents) indexed by radix trees hanging off the inode, and\nsimple bitmaps for free space allocation.\nThis worked well when filesystems were 1k loc, or 10k loc. But every additional\nfeature needed new on disk data structures and pushed the limits of this\ndesign, with transactional correctness (filesystems that don't need a fsck\nafter a crash) being a particularly big jump in complexity. In this model,\nevery additional feature has needed new on disk data structures - each unique\nand special purpose. High performance directory indexing, xattrs, reflink, and\na great many features not visible to users but necessary for scalability to\nmodern filesystem sizes.\nNotably, ZFS has more in common with the original Unix filesystem in the design\nof its data structures than other modern filesystems, for what were good and\npragmatic reasons at the time. Their primary goal was robustness, and that has\nbeen the strength of filesystems in that lineage (as with ext2/3/4, also based\noff the original Unix filesystem): those simple, often fixed-location on disk\ndata structures make for reliable repair.\nBut as research into b-trees and databases advanced, a great many filesystems\nlooked at that and said: \"Wouldn't life be simpler if all metadata was just\nkeys in a key/value store?\", with that ideally being a single core\nb-tree/database implementation, with scalability and performance usually being\nprimary goals.\nXFS was the earliest and most successful of the \"everything is a b-tree\"\napproach, but largely predating the push towards COW filesystems, and didn't\npush the \"unified data model\" very far in comparison to later filesystems.\nReiserfs and btrfs are also notable for pushing this approach.\nHistorically, the primary disadvantages of this approach has been:\nRobustness. \"Everything is a b-tree\" introduces new single points of failure\n(b-tree roots), and high performance, production quality b-trees are\nnotoriously complicated beasts.\nPerformance. The original Unix filesystem design had a major scalability\nadvantage, with a lot of built in sharding at the inode level. Fully realizing\nthe benefits of the \"filesystem as a database\" approach requires tossing that\nout: instead of many b-trees, one per inode, having internals that work like\na normal-ish database requires the filesystem to be implemented with a few\nmassive b-trees: then we can have something that starts to look like a\nrelational database, with tables corresponding to data types, and simple\nunified transactional interface.\nThese have been the achille's heels, and the reason why other more modern COW\nfilesystems have not lived up to ZFS's robustness.\nHistory of bcachefs, and how these were solved\nbcache:\nEnter bcache, the prototype for bcachefs. When SSDs were first being\nintroduced, they were from the beginning orders of magnitude faster than\nrotating disk - but expensive, so block layer caching was an obvious approach\nfor using them effectively.\nWhereas other approaches towards SSD caching at the time were based on hash\ntables (which have major disadvantages as persistent data structures, and don't\nsupport range lookups - making indexing extents instead of blocks extremely\nproblematic), bcache went with a b-tree approach, with several novel\ninnovations to make the b-tree performant enough to work for indexing every IO\nlookup, without sharding. The primary innovation was log structured btree\nnodes: this enabled eytzinger search trees, which eliminates binary search.\nThis was huge: binary search is O(log n), but from a CPU cache perspective is\nthe worst lookup algorithm possible.\nThis made bcache's b-tree the fastest ordered, persistent, production quality\nkey value store in existence; perhaps bcachefs's biggest claim to fame, but\nmore importantly, making the rest of the problems in \"modern COW filesystem\nwith real database underpinnings\" tractable.\nRobustness and reliability: making \"restore from backup\" a thing of the past\nUnlike the rest, there is no silver bullet to producing truly reliable code.\nReliability comes from making it a priority and sticking to it, at every stage\nof development; learning from every accident and mistake, discovering all the\nfailure modes that can happen in the wild, and continually building on those\nlessons and experience.\nSome approaches pioneered in other filesystems:\nbtree node scan, for disaster recovery scenarios - originally done in\nreiserfs. The reiserfs implementation had issues; notably, it would pick up\nbtree nodes from unrelated filesystems on the same device. bcachefs corrects\nall of these with improved per-btree-node metadata, and makes it practical\nfor the huge filesystems of today by segregating data and metadata (and\nnoting which regions are which in the superblock).\nExtensive runtime verification of metadata, any time metadata is read - or\nwritten; XFS also makes heavy use of this approach. This drastically reduces\nthe impact of bugs: bugs that would otherwise result in filesystem corruption\ntypically result in emergency shutdown - restart and keep going, and bugs are\ncaught much more quickly (and are therefore much easier to debug).\nFull filesystem rollback, as in log structured filesystems (like nilfs2), and\nZFS in limited fashion. This falls out of the \"filesystem as a database\"\napproach: with all metadata as keys in b-trees, with all updates (and\noverwrites) logged in a single unified journal, anything can be rolled back.\nOther areas of emphasis:\nComprehensive and thorough logging, introspection, and debugging tools.\nYou can't debug what you can't see. Any time something goes wrong, the system\nshould tell you clearly what went wrong, and tell you clearly and directly\neverything you, as an end user or engineer, need to understand what went\nwrong and debug the issue.\nComprehensive and robust fsck\nNotably, filesystem repair is considerably simplified with the \"filesystem as\na database\" approach, but it's still a major topic, worth its own chapter in\nthe user manual.\nThe main point is: repair is not, and cannot be, an afterthought. It doesn't\nmatter what the cause of the damage was, it's the filesystem's job to repair\nit, without causing additional damage or loss.\nIf you're running bcachefs: you won't lose data. If you find a way to break it:\nfirst, I'll be impressed, and secondly - get in contact. We've got the\ndebugging tools to make short work of anything that may come up, and real world\ntesting (the crazier, the better!) is how we make this filesystem more robust\nand rock solid for everyone.\nA few other things worth mentioning\nUnified codebase\nThe entire bcachefs codebase can be built and used either inside the kernel, or\nin userspace - notably, fsck is not a from-scratch implementation, it's just a\nsmall module in the larger bcachefs codebase.\nRust\nWe've got some initial work done on transitioning to Rust, with plans for much\nmore: here's an example of walking the btree, from Rust:\ncmd_list\nContact and support\nDeveloping a filesystem is also not cheap, quick, or easy; we need funding!\nPlease chip in on\nPatreon\nWe're also now offering contracts for support and feature development -\nemail\nfor more info. Check the\nroadmap\nfor ideas on things you might like to support.\nJoin us in the bcache\nIRC\nchannel, we have a small group of bcachefs\nusers and testers there: #bcache on OFTC (irc.oftc.net).\nMailing list:\nhttps://lore.kernel.org/linux-bcachefs/\n, or\nlinux-bcachefs@vger.kernel.org.\nBug trackers:\nbcachefs\n,\nbcachefs-tools\nNews\nBlog:\nRSS\nAtom\nmembers-v2\nMembers v2, configurationless tiering\nA feature request we've had is configurationless tiering, smart tiering of\nmember devices in a filesystem based on performance. This feature will allow\neasy and simple tiering of devices within a filesystem based on the\nperformance of the device. The effect of this is that it will allow data that\nis commonly accessed, hot data, stored on the faster drives while data that is\nnot used as often, cold data, will be stored on slower drives. This will\nincrease filesystem performance greatly.\nBackground: extensible metadata\nExtensible metadata means having to ability to add new fields to metadata\nwhile still being compatible with older versions of the filesystem. To achieve\nthis structs cannot be of a fixed size and need to be able to add new fields\nusing a bounds check or filling in wth zeroes when reading data from a\nprevious version.\nNon extensible members\nEarly in the process of implementing the tiering feature we ran into an\ninteresting problem.  Members within the superblock were not large enough to\nproperly store this data. While we could have just resized the member, this\nwould have caused further issues regarding compatability. Instead we opted to\nimplement resizable members, members v2 if you will. The effect of adding\nresizable members allows us to add new fields to the members while still\nensuring backwards compatability.\nMembers v2\nThe superblock of a filesystem is the start of that filesystem and requires\nextensible fields which contain important data such as a list of member\ndevices. A suberblock needs extensible fields in cases such as a new device\nbeing added to the filesystem, in which case the members field needs to be\nextended.\nIn the case of members v1, the members array itself was extensible, the\nmembers themselves were a fixed size. Due to their fixed size it was quite easy\nto index and retrieve members from the list. However, when members can be\ndynamically resizable it is not that easy. The location of each member can not\nbe known before runtime and therefore has to be found and accessed manually within the\narray of members. This was at times a complicated process for me to implement\nbut will make future expansions of the members much simpler.\nConfigurationless tiering\nConfigurationless tiering is a feature that has been commonly requested.\nInstead the user specifying foreground and background targets, foreground\nallocations will go to the fastest device(s) and cold data will be moved to the\nslower device(s) in the background. To implement this the filesystem will\nrequire some idea of device performance which needs to be stored in the\nsuperblock.\nStoring device performance\nThe devices within the filesytem will now store IOPS measurements for randread,\nrandwrite, seq-read, and seq-write. In the future the new IOPS field can also be\nuseful in other features such as monitoring device health.\nAddendum: Cap'n proto\nSome of the ideas in bcachefs about how to handle metadata were inspired by\nCap'n Proto\n, which is highly recommended reading -\nit's a library that does everything we have to do by hand in C, exactly the\nway we want it.\nPosted\nWed Sep 27 18:58:27 2023\nArchive:\nArchive\nLast edited\nFri Oct 17 09:10:55 2025", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://bcachefs.org/"}}
{"text": "FAQ\nbcachefs\n/\nFAQ\nRecentChanges\nOther useful links\nRoadmap\nWishlist\nUser manual\nDeveloper documentation\nArchitecture\nBtreeIterators\nBtreeNodes\nBtreeWhiteouts\nEncryption\nTransactions\nSnapshots\nAllocator\nFsck\nTestServerSetup\nStablePages\nOctober 2022 talk for Redhat\nFrequently Asked Questions\nWhat is bcachefs\nBcachefs is an advanced new filesystem for Linux, with an emphasis on reliability\nand robustness and the complete set of features one would expect from a modern filesystem.\nHow does bcachefs compare to other Linux filesystems like ext4, btrfs, xfs, and zfs?\nThe bcachefs filesystem has various advantages over other Linux filesystems.\nBcachefs is a feature complete filesystem while also containing extra features such as\nchecksumming and multi-device functionality within a filesystem. These are both features\nthat are absent from the ext4 and xfs filesystems. Other benefits presented by bcachefs\ninclude a focus on reliability, robustness, and performance. Bcachefs is safer to use than\nbtrfs and is also shown to outperform zfs in terms of speed and reliability.\nIs bcachefs stable for production use?\nBcachefs can currently be considered beta quality. It has a small pool of\noutside users and has been stable for quite some time now; there's no reason\nto expect issues as long as you stick to the currently supported feature set.\nHow do I install and use bcachefs on my Linux system?\nInstructions for installation can be found here:\nGetting Started\nThe user manual can be found here:\nbcachefs-principles-of-operation.pdf\nDoes bcachefs support data deduplication and compression?\nBcachefs currently does not support data deduplication however compression\nis supported, more information can be found here:\nCompression\nAre there any limitations or known issues with bcachefs?\nInformation on current bugs within bcachefs and related tooling can be found here:\nbcachefs bugs\nbcachefs-tools bugs\nOur Todo list can be found here:\nTodo\nWhat is the development status of bcachefs?\nCurrently we are working on getting bcachefs merged into the linux kernel.\nA roadmap and features list can be found here:\nRoadmap\nCan I migrate my existing filesystem to bcachefs?\nYes, users can migrate their existing filesystems into a bcachefs filesystem\nusing the 'bcachefs migrate' subcommand\nAre there any recommended use cases for bcachefs?\nBcachefs is primarily for multi-device filesystems.\nHow do I contribute to the bcachefs project?\nInformation on contributions can be found here:\nContributing\nJoin the\nIRC\nchannel and chat with other contributers\nDo I have to reformat my bcache drive to use the new bcachefs ?\nTo use the caching device (cdev) with the new bcachefs, you will need to\nreformat it. In principle this should not be a problem, as they can be removed\nfrom a backing device (bdev/bcache).\nIf you wish to reuse the backing device (bdev/bcache), you can't this is\nnolonger an option.\nIf you wish to use bcachefs as a filesystem you will have to format the\nbacking device (bdev/bcache) with the new filesystem.\nDo I have to reformat my bcache drive to use the new bcachefs ?\nTo use the caching device (cdev) with the new bcachefs, you will need to\nreformat it. In principle this should not be a problem, as they can be removed\nfrom a backing device (bdev/bcache).\nIf you wish to reuse the backing device (bdev/bcache), you can't this is\nnolonger an option.\nIf you wish to use bcachefs as a filesystem you will have to format the\nbacking device (bdev/bcache) with the new filesystem.\nDoes bcachefs still have the bcache caching functionality of block devices ?\nNo.\nWhere do I obtain statically linked bcachefs-tool (for use in initramfs) ?\nYou can either compile them yourself from source, or refer to your\ndistribution on how to obtain a statically linked set.\nStatically linked programs have all the required dependencies compiled in. As\nsuch, they require no external libraries at runtime. This does however\nincrease the size of the binary.\nWhat is liburcu ?\nuserspace RCU (read-copy-update) library\nhttp://lttng.org/urcu\nI get the error \"tools-util.c:16:19: fatal error: blkid.h: No such file or directory\":\nYou might need to add\nEXTRA_CFLAGS=\"-l/usr/include/blkid/\"\nor where-ever your OS stores the blkid.h file.\nI get the error \"mount(2) system call failed: Required key not available.\"\nYou need to unlock the filesystem before mounting. As a superuser, run\n(substituting your drive for\n/dev/sdc1\n):\nbcachefs unlock /dev/sdc1\nIf the error persists, it might be because you're using\nsudo\nand root's\nkeyring is not connected to your session. To verify, run these two commands:\nsudo keyctl list @u\nsudo keyctl list @s\nThe first one displays the keys in root's keyring, and should mention external\nUUID of your bcachefs filesystem (which you can find with\nbcachefs show-super\n/dev/scd1\n). The second one displays your session's keyring, and should mention\nuid.0 (root).\nIf you don't see uid.0 in the output of the second command, connect root's\nkeyring to your session:\nsudo keyctl link @u @s\nLinks:\nindex\nLast edited\nWed Jul 30 10:28:35 2025", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://bcachefs.org/FAQ/"}}
{"text": "OpenZFS Documentation — OpenZFS  documentation\nOpenZFS Documentation\nEdit on GitHub\nOpenZFS Documentation\n\nWelcome to the OpenZFS Documentation. This resource provides documentation for\nusers and developers working with (or contributing to) the OpenZFS\nproject. New users or system administrators should refer to the\ndocumentation for their favorite platform to get started.\nGetting Started\nProject and\nCommunity\nDeveloper\nResources\nHow to get started\nwith OpenZFS on your\nfavorite platform\nAbout the project\nand how to\ncontribute\nTechnical\ndocumentation\ndiscussing the\nOpenZFS\nimplementation\nTable of Contents:\n\nGetting Started\nAlpine Linux\nArch Linux\nDebian\nFedora\nFreeBSD\nGentoo\nNixOS\nopenSUSE\nRHEL-based distro\nSlackware\nUbuntu\nProject and Community\nAdmin Documentation\nDonate\nFAQ\nMailing Lists\nSigning Keys\nIssue Tracker\nReleases\nRoadmap\nDeveloper Resources\nCustom Packages\nBuilding ZFS\nOpenZFS Documentation\nGit and GitHub for beginners (ZoL edition)\nPerformance and Tuning\nAsync Writes\nHardware\nModule Parameters\nWorkload Tuning\nZFS Transaction Delay\nZFS I/O (ZIO) Scheduler\nBasic Concepts\nChecksums and Their Use in ZFS\nFeature Flags\nRAIDZ\nTroubleshooting\ndRAID\nMan Pages\nmaster\nv2.4\nv2.3\nv2.2\nv2.1\nv2.0\nv0.8\nv0.7\nv0.6\nZFS Messages\nMessage ID: ZFS-8000-14\nMessage ID: ZFS-8000-2Q\nMessage ID: ZFS-8000-3C\nMessage ID: ZFS-8000-4J\nMessage ID: ZFS-8000-5E\nMessage ID: ZFS-8000-6X\nMessage ID: ZFS-8000-72\nMessage ID: ZFS-8000-8A\nMessage ID: ZFS-8000-9P\nMessage ID: ZFS-8000-A5\nMessage ID: ZFS-8000-ER\nMessage ID: ZFS-8000-EY\nMessage ID: ZFS-8000-HC\nMessage ID: ZFS-8000-JQ\nMessage ID: ZFS-8000-K4\nLicense", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://openzfs.github.io/openzfs-docs/"}}
{"text": "Getting Started — OpenZFS  documentation\nGetting Started\nEdit on GitHub\nGetting Started\n\nTo get started with OpenZFS refer to the provided documentation for your\ndistribution. It will cover the recommended installation method and any\ndistribution specific information. First time OpenZFS users are\nencouraged to check out Aaron Toponce’s\nexcellent\ndocumentation\n.\nAlpine Linux\nContents\nAlpine Linux Root on ZFS\nInstallation\nAutomatic zpool importing and mount\nRoot on ZFS\nAlpine Linux Root on ZFS\nArch Linux\nContents\nArch Linux Root on ZFS\nSupport\nOverview\nInstallation\nRoot on ZFS\nArch Linux Root on ZFS\nContribute\nDebian\nInstallation\nRoot on ZFS\nDebian Bookworm Root on ZFS\nDebian Bullseye Root on ZFS\nDebian Buster Root on ZFS\nDebian Stretch Root on ZFS\nDebian Trixie Root on ZFS\nRelated topics\nDebian GNU Linux initrd documentation\nFedora\nContents\nFedora Root on ZFS\nInstallation\nTesting Repo\nRoot on ZFS\nFedora Root on ZFS\nFreeBSD\nInstallation on FreeBSD\nDevelopment on FreeBSD\nGentoo\nNixOS\nContents\nNixOS Root on ZFS\nSupport\nInstallation\nRoot on ZFS\nNixOS Root on ZFS\nContribute\nopenSUSE\nInstallation\nExternal Links\nRoot on ZFS\nopenSUSE Leap Root on ZFS\nopenSUSE Tumbleweed Root on ZFS\nRHEL-based distro\nContents\nRocky Linux Root on ZFS\nDKMS\nkABI-tracking kmod\nPrevious minor EL releases\nTesting Repositories\nRoot on ZFS\nRocky Linux Root on ZFS\nSlackware\nInstallation\nRoot on ZFS\nSlackware Root on ZFS\nUbuntu\nInstallation\nRoot on ZFS\nUbuntu 18.04 Root on ZFS\nUbuntu 20.04 Root on ZFS\nUbuntu 20.04 Root on ZFS for Raspberry Pi\nUbuntu 22.04 Root on ZFS\nUbuntu 22.04 Root on ZFS for Raspberry Pi", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://openzfs.github.io/openzfs-docs/Getting%20Started/index.html"}}
{"text": "You should have been redirected.\nIf not, click here to continue.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://openzfs.github.io/openzfs-docs/man/8/zpool.8.html"}}
{"text": "You should have been redirected.\nIf not, click here to continue.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://openzfs.github.io/openzfs-docs/man/8/zfs.8.html"}}
{"text": "Basic Concepts — OpenZFS  documentation\nBasic Concepts\nEdit on GitHub\nBasic Concepts\n\nContents:\nChecksums and Their Use in ZFS\nChecksum Algorithms\nChecksum Accelerators\nChecksum Microbenchmarks\nDisabling Checksums\nFeature Flags\nCompatibility\nReference materials\nFeature flags implementation per OS\nRAIDZ\nIntroduction\nSpace efficiency\nPerformance considerations\nTroubleshooting\nAbout Log Files\nUnkillable Process\nZFS Events\ndRAID\nIntroduction\nCreate a dRAID vdev\nRebuilding to a Distributed Spare\nRebalancing", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://openzfs.github.io/openzfs-docs/Basic%20Concepts/index.html"}}
{"text": "Welcome to BTRFS documentation! — BTRFS  documentation\nWelcome to BTRFS documentation!\nView page source\nWelcome to BTRFS documentation!\n\nBTRFS is a modern copy on write (COW) filesystem for Linux aimed at\nimplementing advanced features while also focusing on fault tolerance, repair\nand easy administration. You can read more about the features in the\nintroduction\nor choose from the pages below. Documentation\nfor command line tools\nbtrfs(8)\n,\nmkfs.btrfs(8)\nand others\nis in the\nmanual pages\n.\nOverview\nIntroduction\nStatus\nManual pages\nAdministration\nHardware considerations\nChanges (feature/version)\nChanges (kernel/version)\nChanges (btrfs-progs)\nContributors\nGlossary\nInstallation instructions\nSource repositories\nInteroperability\nFeatures\nCommon Linux features\nCustom ioctls\nAuto-repair on read\nBalance\nCompression\nChecksumming\nConvert\nDeduplication\nDefragmentation\nInline files\nQuota groups\nReflink\nResize\nScrub\nSeeding device\nSend/receive\nSubpage support\nSubvolumes\nSwapfile\nTree checker\nTrim/discard\nVolume management\nZoned mode\nDeveloper documentation\nDevelopment notes\nDeveloper’s FAQ\nConventions and style for documentation\nExperimental features\nBtrfs design\nBtrees\nOn-disk Format\nSend stream format\nJSON output\nInternal APIs\nRelease checklist\nPull request review workflow\nCommand line, formatting, UI guidelines\nbtrfs-ioctl(2)\nNeed help?\n\nAssistance is available from the\n#btrfs channel on Libera Chat\nor the\nlinux-btrfs mailing list\n. Issues with the\nuserspace btrfs tools can be reported to the\nbtrfs-progs issue tracker on\nGitHub\n.\nThis documentation is still work in progress, not everything from the original\nwiki\nhttps://btrfs.wiki.kernel.org\nhas been moved here. Below are starting points\nfor missing contents.\nTODO\nTroubleshooting pages", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://btrfs.readthedocs.io/en/latest/"}}
{"text": "Introduction — BTRFS  documentation\nIntroduction\nView page source\nIntroduction\n\nBTRFS is a modern copy on write (COW) filesystem for Linux aimed at\nimplementing advanced features while also focusing on fault tolerance, repair\nand easy administration. Its main features and benefits are:\nSnapshots which do not make a full copy of the files\nBuilt-in volume management, support for software-based RAID 0, RAID 1, RAID 10 and others\nSelf-healing - checksums for data and metadata, automatic detection of silent data corruptions\nData compression\nReflinks, fast and efficient file copies\nFeature overview\n\nExtent based file storage\n2\n64\nbyte (16 EiB)\nmaximum file size\n(practical limit is 8 EiB due to Linux VFS)\nSpace-efficient packing of small files\nSpace-efficient indexed directories\nDynamic inode allocation\nWritable snapshots, read-only snapshots, subvolumes (separate internal filesystem roots)\nChecksums on data and metadata\n(crc32c, xxhash, sha256, blake2b)\nCompression (ZLIB, LZO, ZSTD), heuristics\nIntegrated multiple device support\n:\nFile Striping (like RAID0)\nFile Mirroring (like RAID1 up to 4 copies)\nFile Striping+Mirroring (like RAID10)\nSingle and Dual Parity implementations (like RAID5/6, experimental, not production-ready)\nSSD/NVMe (flash storage) awareness,\nTRIM/Discard\nfor reporting free blocks for\nreuse and optimizations (e.g. avoiding unnecessary seek optimizations,\nsending writes in clusters.\nBackground scrub\nprocess for finding and repairing errors of files with redundant copies\nOnline filesystem defragmentation\nOffline filesystem check\nIn-place conversion\nof existing ext2/3/4 and reiserfs filesystems\nSeeding device.\nCreate a (readonly) filesystem that\nacts as a template to seed other Btrfs filesystems. The original filesystem\nand devices are included as a readonly starting point for the new filesystem.\nUsing copy on write, all modifications are stored on different devices; the\noriginal is unchanged.\nSubvolume-aware quota\nsupport\nSend/receive of subvolume changes\n, efficient\nincremental filesystem mirroring and backup\nBatch, or out-of-band deduplication\n(happens after writes, not during)\nSwapfile support\nTree-checker\n, post-read and pre-write metadata verification\nZoned mode support\n(SMR/ZBC/ZNS friendly allocation, emulated on non-zoned devices)\nA more detailed list of features and compatibility is on the\nstatus page\n.\nQuick start\n\nFor a really quick start you can simply create and mount the filesystem. Make\nsure that the block device you’d like to use is suitable so you don’t overwrite existing data.\n# mkfs.btrfs /dev/sdx\n# mount /dev/sdx /mnt/test\nThe default options should be acceptable for most users and sometimes can be\nchanged later. The example above is for a single device filesystem, creating a\nsingle\nprofile for data (no redundant copies of the blocks), and\nDUP\nfor metadata (each block is duplicated).\nRead more about:\ncreating a filesystem at\nmkfs.btrfs(8)\nmount options at\nAdministration", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://btrfs.readthedocs.io/en/latest/Introduction.html"}}
{"text": "Administration — BTRFS  documentation\nAdministration\nView page source\nAdministration\n\nThe main administration tool for BTRFS filesystems is\nbtrfs(8)\n.\nPlease refer to the manual pages of the subcommands for further documentation.\nOther topics explaining features or concepts can be found in\nbtrfs(5)\n.\nMount options\n\nBTRFS SPECIFIC MOUNT OPTIONS\n\nThis section describes mount options specific to BTRFS.  For the generic mount\noptions please refer to\nmount(8)\nmanual page and also see the section\nwith BTRFS specifics\nbelow (in btrfs-man5)\n. The options are\nsorted alphabetically (discarding the\nno\nprefix).\nNote\nMost mount options apply to the whole filesystem and only options in the\nfirst mounted subvolume will take effect. This is due to lack of implementation\nand may change in the future. This means that (for example) you can’t set\nper-subvolume\nnodatacow\n,\nnodatasum\n, or\ncompress\nusing mount options. This\nshould eventually be fixed, but it has proved to be difficult to implement\ncorrectly within the Linux VFS framework.\nMount options are processed in order, only the last occurrence of an option\ntakes effect and may disable other options due to constraints (see e.g.\nnodatacow\nand\ncompress\n). The output of\nmount\ncommand shows which options\nhave been applied.\nacl, noacl\n(default: on)\nEnable/disable support for POSIX Access Control Lists (ACLs).  See the\nacl(5)\nmanual page for more information about ACLs.\nThe support for ACL is build-time configurable (BTRFS_FS_POSIX_ACL) and\nmount fails if\nacl\nis requested but the feature is not compiled in.\nautodefrag, noautodefrag\n(since: 3.0, default: off)\nEnable automatic file defragmentation.\nWhen enabled, small random writes into files (in a range of tens of kilobytes,\ncurrently it’s 64KiB) are detected and queued up for the defragmentation process.\nMay not be well suited for large database workloads.\nThe read latency may increase due to reading the adjacent blocks that make up the\nrange for defragmentation, successive write will merge the blocks in the new\nlocation.\nWarning\nDefragmenting with Linux kernel versions < 3.9 or ≥ 3.14-rc2 as\nwell as with Linux stable kernel versions ≥ 3.10.31, ≥ 3.12.12 or\n≥ 3.13.4 will break up the reflinks of COW data (for example files\ncopied with\ncp --reflink\n, snapshots or de-duplicated data).\nThis may cause considerable increase of space usage depending on the\nbroken up reflinks.\nbarrier, nobarrier\n(default: on)\nEnsure that all IO write operations make it through the device cache and are stored\npermanently when the filesystem is at its consistency checkpoint. This\ntypically means that a flush command is sent to the device that will\nsynchronize all pending data and ordinary metadata blocks, then writes the\nsuperblock and issues another flush.\nThe write flushes incur a slight hit and also prevent the IO block\nscheduler to reorder requests in a more effective way. Disabling barriers gets\nrid of that penalty but will most certainly lead to a corrupted filesystem in\ncase of a crash or power loss. The ordinary metadata blocks could be yet\nunwritten at the time the new superblock is stored permanently, expecting that\nthe block pointers to metadata were stored permanently before.\nOn a device with a volatile battery-backed write-back cache, the\nnobarrier\noption will not lead to filesystem corruption as the pending blocks are\nsupposed to make it to the permanent storage.\nclear_cache\nForce clearing and rebuilding of the free space cache if something\nhas gone wrong.\nFor free space cache\nv1\n, this only clears (and, unless\nnospace_cache\nis\nused, rebuilds) the free space cache for block groups that are modified while\nthe filesystem is mounted with that option. To actually clear an entire free\nspace cache\nv1\n, see\nbtrfs check --clear-space-cache v1\n.\nFor free space cache\nv2\n, this clears the entire free space cache.\nTo do so without requiring to mounting the filesystem, see\nbtrfs check --clear-space-cache v2\n.\nSee also:\nspace_cache\n.\ncommit=<seconds>\n(since: 3.12, default: 30)\nSet the interval of periodic transaction commit when data are synchronized\nto permanent storage. Higher interval values lead to larger amount of unwritten\ndata to accumulate in memory, which has obvious consequences when the\nsystem crashes.  The upper bound is not forced, but a warning is\nprinted if it’s more than 300 seconds (5 minutes). Use with care.\nThe periodic commit is not the only mechanism to do the transaction commit,\nthis can also happen by explicit\nsync\nor indirectly by other\ncommands that affect the global filesystem state or internal kernel\nmechanisms that flush based on various thresholds or policies (e.g. cgroups).\ncompress, compress=<type[:level]>, compress-force, compress-force=<type[:level]>\n(default: off, level support since: 5.1)\nControl BTRFS file data compression.  Type may be specified as\nzlib\n,\nlzo\n,\nzstd\nor\nno\n(for no compression, used for remounting).  If no type\nis specified,\nzlib\nis used.  If\ncompress-force\nis specified,\nthen compression will always be attempted, but the data may end up uncompressed\nif the compression would make them larger.\nBoth\nzlib\nand\nzstd\n(since version 5.1) expose the compression level as a\ntunable knob with higher levels trading speed and memory (\nzstd\n) for higher\ncompression ratios. This can be set by appending a colon and the desired level.\nZLIB accepts the range [1, 9] and ZSTD accepts [1, 15]. If no level is set,\nboth currently use a default level of 3. The value 0 is an alias for the\ndefault level.\nOtherwise some simple heuristics are applied to detect an incompressible file.\nIf the first blocks written to a file are not compressible, the whole file is\npermanently marked to skip compression. As this is too simple, the\ncompress-force\nis a workaround that will compress most of the files at the\ncost of some wasted CPU cycles on failed attempts.\nSince kernel 4.15, a set of heuristic algorithms have been improved by using\nfrequency sampling, repeated pattern detection and Shannon entropy calculation\nto avoid that.\nNote\nIf compression is enabled,\nnodatacow\nand\nnodatasum\nare disabled.\ndatacow, nodatacow\n(default: on)\nEnable data copy-on-write for newly created files.\nNodatacow\nimplies\nnodatasum\n, and disables\ncompression\n. All files created\nunder\nnodatacow\nare also set the NOCOW file attribute (see\nchattr(1)\n).\nNote\nIf\nnodatacow\nor\nnodatasum\nare enabled, compression is disabled.\nUpdates in-place improve performance for workloads that do frequent overwrites,\nat the cost of potential partial writes, in case the write is interrupted\n(system crash, device failure).\ndatasum, nodatasum\n(default: on)\nEnable data checksumming for newly created files.\nDatasum\nimplies\ndatacow\n, i.e. the normal mode of operation. All files created\nunder\nnodatasum\ninherit the “no checksums” property, however there’s no\ncorresponding file attribute (see\nchattr(1)\n).\nNote\nIf\nnodatacow\nor\nnodatasum\nare enabled, compression is disabled.\nThere is a slight performance gain when checksums are turned off, the\ncorresponding metadata blocks holding the checksums do not need to updated.\nThe cost of checksumming of the blocks in memory is much lower than the IO,\nmodern CPUs feature hardware support of the checksumming algorithm.\ndegraded\n(default: off)\nAllow mounts with fewer devices than the RAID profile constraints\nrequire.  A read-write mount (or remount) may fail when there are too many devices\nmissing, for example if a stripe member is completely missing from RAID0.\nSince 4.14, the constraint checks have been improved and are verified on the\nchunk level, not at the device level. This allows degraded mounts of\nfilesystems with mixed RAID profiles for data and metadata, even if the\ndevice number constraints would not be satisfied for some of the profiles.\nExample: metadata -- raid1, data -- single, devices --\n/dev/sda\n,\n/dev/sdb\nSuppose the data are completely stored on\nsda\n, then missing\nsdb\nwill not\nprevent the mount, even if 1 missing device would normally prevent (any)\nsingle\nprofile to mount. In case some of the data chunks are stored on\nsdb\n,\nthen the constraint of single/data is not satisfied and the filesystem\ncannot be mounted.\ndevice=<devicepath>\nSpecify a path to a device that will be scanned for BTRFS filesystem during\nmount. This is usually done automatically by a device manager (like udev) or\nusing the\nbtrfs device scan\ncommand (e.g. run from the initial ramdisk). In\ncases where this is not possible the\ndevice\nmount option can help.\nNote\nBooting e.g. a RAID1 system may fail even if all filesystem’s\ndevice\npaths are provided as the actual device nodes may not be discovered by the\nsystem at that point.\ndiscard, discard=sync, discard=async, nodiscard\n(default: async when devices support it since 6.2, async support since: 5.6)\nEnable discarding of freed file blocks.  This is useful for SSD/NVMe\ndevices, thinly provisioned LUNs, or virtual machine images; however,\nevery storage layer must support discard for it to work.\nIn the synchronous mode (\nsync\nor without option value), lack of asynchronous\nqueued TRIM on the backing device TRIM can severely degrade performance,\nbecause a synchronous TRIM operation will be attempted instead. Queued TRIM\nrequires SATA devices with chipsets revision newer than 3.1 and devices.\nThe asynchronous mode (\nasync\n) gathers extents in larger chunks before sending\nthem to the devices for TRIM. The overhead and performance impact should be\nnegligible compared to the previous mode and it’s supposed to be the preferred\nmode if needed.\nIf it is not necessary to immediately discard freed blocks, then the\nfstrim\ntool can be used to discard all free blocks in a batch. Scheduling a TRIM\nduring a period of low system activity will prevent latent interference with\nthe performance of other operations. Also, a device may ignore the TRIM command\nif the range is too small, so running a batch discard has a greater probability\nof actually discarding the blocks.\nenospc_debug, noenospc_debug\n(default: off)\nEnable verbose output for some ENOSPC conditions. It’s safe to use but can\nbe noisy if the system reaches near-full state.\nfatal_errors=<action>\n(since: 3.4, default: bug)\nAction to take when encountering a fatal error.\nbug\nBUG()\non a fatal error, the system will stay in the crashed state and may be\nstill partially usable, but reboot is required for full operation\npanic\npanic()\non a fatal error, depending on other system configuration, this may\nbe followed by a reboot. Please refer to the documentation of kernel boot\nparameters, e.g.\npanic\n,\noops\nor\ncrashkernel\n.\nflushoncommit, noflushoncommit\n(default: off)\nThis option forces any data dirtied by a write in a prior transaction to commit\nas part of the current commit, effectively a full filesystem sync.\nThis makes the committed state a fully consistent view of the file system from\nthe application’s perspective (i.e. it includes all completed file system\noperations). This was previously the behavior only when a snapshot was\ncreated.\nWhen off, the filesystem is consistent but buffered writes may last more than\none transaction commit.\nfragment=<type>\n(depends on compile-time option CONFIG_BTRFS_DEBUG, since: 4.4, default: off)\nA debugging helper to intentionally fragment given\ntype\nof block groups. The\ntype can be\ndata\n,\nmetadata\nor\nall\n. This mount option should not be used\noutside of debugging environments and is not recognized if the kernel config\noption\nCONFIG_BTRFS_DEBUG\nis not enabled.\nnologreplay\n(default: off, even read-only)\nThe tree-log contains pending updates to the filesystem until the full commit.\nThe log is replayed on next mount, this can be disabled by this option.  See\nalso\ntreelog\n.  Note that\nnologreplay\nis the same as\nnorecovery\n.\nWarning\nCurrently, the tree log is replayed even with a read-only mount! To\ndisable that behaviour, mount also with\nnologreplay\n.\nmax_inline=<bytes>\n(default: min(2048, page size) )\nSpecify the maximum amount of space, that can be inlined in\na metadata b-tree leaf.  The value is specified in bytes, optionally\nwith a K suffix (case insensitive).  In practice, this value\nis limited by the filesystem block size (named\nsectorsize\nat mkfs time),\nand memory page size of the system. In case of sectorsize limit, there’s\nsome space unavailable due to b-tree leaf headers.  For example, a 4KiB\nsectorsize, maximum size of inline data is about 3900 bytes.\nInlining can be completely turned off by specifying 0. This will increase data\nblock slack if file sizes are much smaller than block size but will reduce\nmetadata consumption in return.\nNote\nThe default value has changed to 2048 in kernel 4.6.\nmetadata_ratio=<value>\n(default: 0, internal logic)\nSpecifies that 1 metadata chunk should be allocated after every\nvalue\ndata\nchunks. Default behaviour depends on internal logic, some percent of unused\nmetadata space is attempted to be maintained but is not always possible if\nthere’s not enough space left for chunk allocation. The option could be useful to\noverride the internal logic in favor of the metadata allocation if the expected\nworkload is supposed to be metadata intense (snapshots, reflinks, xattrs,\ninlined files).\nnorecovery\n(since: 4.5, default: off)\nDo not attempt any data recovery at mount time. This will disable\nlogreplay\nand avoids other write operations. Note that this option is the same as\nnologreplay\n.\nNote\nThe opposite option\nrecovery\nused to have different meaning but was\nchanged for consistency with other filesystems, where\nnorecovery\nis used for\nskipping log replay. BTRFS does the same and in general will try to avoid any\nwrite operations.\nrescan_uuid_tree\n(since: 3.12, default: off)\nForce check and rebuild procedure of the UUID tree. This should not\nnormally be needed. Alternatively the tree can be cleared from\nuserspace by command\nbtrfs rescue clear-uuid-tree\nand then it will be automatically rebuilt in kernel (the mount option\nis not needed in that case).\nrescue\n(since: 5.9)\nModes allowing mount with damaged filesystem structures, all requires\nthe filesystem to be mounted read-only and doesn’t allow remount to read-write.\nThis is supposed to provide unified and more fine grained tuning of\nerrors that affect filesystem operation.\nusebackuproot\n(since 5.9)\nTry to use backup root slots inside super block.\nReplaces standalone option\nusebackuproot\nnologreplay\n(since 5.9)\nDo not replay any dirty logs.\nReplaces standalone option\nnologreplay\nignorebadroots\n,\nibadroots\n(since: 5.11)\nIgnore bad tree roots, greatly improve the chance for data salvage.\nignoredatacsums\n,\nidatacsums\n(since: 5.11)\nIgnore data checksum verification.\nignoremetacsums\n,\nimetacsums\n(since 6.12)\nIgnore metadata checksum verification, useful for interrupted checksum conversion.\nall\n(since: 5.9)\nEnable all supported rescue options.\nskip_balance\n(since: 3.3, default: off)\nSkip automatic resume of an interrupted balance operation. The operation can\nlater be resumed with\nbtrfs balance resume\n, or the paused state can be\nremoved with\nbtrfs balance cancel\n. The default behaviour is to resume an\ninterrupted balance immediately after the filesystem is mounted.\nspace_cache, space_cache=<version>, nospace_cache\n(\nnospace_cache\nsince: 3.2,\nspace_cache=v1\nand\nspace_cache=v2\nsince 4.5, default:\nspace_cache=v2\n)\nOptions to control the free space cache. The free space cache greatly improves\nperformance when reading block group free space into memory. However, managing\nthe space cache consumes some resources, including a small amount of disk\nspace.\nThere are two implementations of the free space cache. The original\none, referred to as\nv1\n, used to be a safe default but has been\nsuperseded by\nv2\n.  The\nv1\nspace cache can be disabled at mount time\nwith\nnospace_cache\nwithout clearing.\nOn very large filesystems (many terabytes) and certain workloads, the\nperformance of the\nv1\nspace cache may degrade drastically. The\nv2\nimplementation, which adds a new b-tree called the free space tree, addresses\nthis issue. Once enabled, the\nv2\nspace cache will always be used and cannot\nbe disabled unless it is cleared. Use\nclear_cache,space_cache=v1\nor\nclear_cache,nospace_cache\nto do so. If\nv2\nis enabled, and\nv1\nspace\ncache will be cleared (at the first mount) and kernels without\nv2\nsupport will only be able to mount the filesystem in read-only mode.\nOn an unmounted filesystem the caches (both versions) can be cleared by\n“btrfs check --clear-space-cache”.\nThe\nbtrfs-check(8)\nand\n:doc:`mkfs.btrfs\ncommands have full\nv2\nfree space\ncache support since v4.19.\nIf a version is not explicitly specified, the default implementation will be\nchosen, which is\nv2\n.\nssd, ssd_spread, nossd, nossd_spread\n(default: SSD autodetected)\nOptions to control SSD allocation schemes.  By default, BTRFS will\nenable or disable SSD optimizations depending on status of a device with\nrespect to rotational or non-rotational type. This is determined by the\ncontents of\n/sys/block/DEV/queue/rotational\n). If it is 0, the\nssd\noption is turned on.  The option\nnossd\nwill disable the\nautodetection.\nThe optimizations make use of the absence of the seek penalty that’s inherent\nfor the rotational devices. The blocks can be typically written faster and\nare not offloaded to separate threads.\nNote\nSince 4.14, the block layout optimizations have been dropped. This used\nto help with first generations of SSD devices. Their FTL (flash translation\nlayer) was not effective and the optimization was supposed to improve the wear\nby better aligning blocks. This is no longer true with modern SSD devices and\nthe optimization had no real benefit. Furthermore it caused increased\nfragmentation. The layout tuning has been kept intact for the option\nssd_spread\n.\nThe\nssd_spread\nmount option attempts to allocate into bigger and aligned\nchunks of unused space, and may perform better on low-end SSDs.\nssd_spread\nimplies\nssd\n, enabling all other SSD heuristics as well. The option\nnossd\nwill disable all SSD options while\nnossd_spread\nonly disables\nssd_spread\n.\nsubvol=<path>\nMount subvolume from\npath\nrather than the toplevel subvolume. The\npath\nis always treated as relative to the toplevel subvolume.\nThis mount option overrides the default subvolume set for the given filesystem.\nsubvolid=<subvolid>\nMount subvolume specified by a\nsubvolid\nnumber rather than the toplevel\nsubvolume.  You can use\nbtrfs subvolume list\nof\nbtrfs subvolume show\nto see\nsubvolume ID numbers.\nThis mount option overrides the default subvolume set for the given filesystem.\nNote\nIf both\nsubvolid\nand\nsubvol\nare specified, they must point at the\nsame subvolume, otherwise the mount will fail.\nthread_pool=<number>\n(default: min(NRCPUS + 2, 8) )\nThe number of worker threads to start. NRCPUS is number of on-line CPUs\ndetected at the time of mount. Small number leads to less parallelism in\nprocessing data and metadata, higher numbers could lead to a performance hit\ndue to increased locking contention, process scheduling, cache-line bouncing or\ncostly data transfers between local CPU memories.\ntreelog, notreelog\n(default: on)\nEnable the tree logging used for\nfsync\nand\nO_SYNC\nwrites. The tree log\nstores changes without the need of a full filesystem sync. The log operations\nare flushed at sync and transaction commit. If the system crashes between two\nsuch syncs, the pending tree log operations are replayed during mount.\nWarning\nCurrently, the tree log is replayed even with a read-only mount! To\ndisable that behaviour, also mount with\nnologreplay\n.\nThe tree log could contain new files/directories, these would not exist on\na mounted filesystem if the log is not replayed.\nusebackuproot\n(since: 4.6, default: off)\nEnable autorecovery attempts if a bad tree root is found at mount time.\nCurrently this scans a backup list of several previous tree roots and tries to\nuse the first readable. This can be used with read-only mounts as well.\nNote\nThis option has replaced\nrecovery\nwhich has been deprecated.\nuser_subvol_rm_allowed\n(default: off)\nAllow subvolumes to be deleted by their respective owner. Otherwise, only the\nroot user can do that.\nNote\nHistorically, any user could create a snapshot even if he was not owner\nof the source subvolume, the subvolume deletion has been restricted for that\nreason. The subvolume creation has been restricted but this mount option is\nstill required. This is a usability issue.\nSince 4.18, the\nrmdir(2)\nsyscall can delete an empty subvolume just like an\nordinary directory. Whether this is possible can be detected at runtime, see\nrmdir_subvol\nfeature in\nFILESYSTEM FEATURES\n.\nDEPRECATED MOUNT OPTIONS\n\nList of mount options that have been removed, kept for backward compatibility.\nrecovery\n(since: 3.2, default: off, deprecated since: 4.5)\nNote\nThis option has been replaced by\nusebackuproot\nand should not be used\nbut will work on 4.5+ kernels.\ninode_cache, noinode_cache\n(removed in: 5.11, since: 3.0, default: off)\nNote\nThe functionality has been removed in 5.11, any stale data created by\nprevious use of the\ninode_cache\noption can be removed by\nbtrfs rescue clear-ino-cache\n.\ncheck_int, check_int_data, check_int_print_mask=<value>\n(removed in: 6.7, since: 3.0, default: off)\nThese debugging options control the behavior of the integrity checking\nmodule (the BTRFS_FS_CHECK_INTEGRITY config option required). The main goal is\nto verify that all blocks from a given transaction period are properly linked.\ncheck_int\nenables the integrity checker module, which examines all\nblock write requests to ensure on-disk consistency, at a large\nmemory and CPU cost.\ncheck_int_data\nincludes extent data in the integrity checks, and\nimplies the\ncheck_int\noption.\ncheck_int_print_mask\ntakes a bit mask of BTRFSIC_PRINT_MASK_* values\nas defined in\nfs/btrfs/check-integrity.c\n, to control the integrity\nchecker module behavior.\nSee comments at the top of\nfs/btrfs/check-integrity.c\nfor more information.\nNOTES ON GENERIC MOUNT OPTIONS\n\nSome of the general mount options from\nmount(8)\nthat affect BTRFS and are\nworth mentioning.\ncontext\nThe context refers to the SELinux contexts and policy definitions passed\nas mount options. This works properly since version v6.8 (because the\nmount option parser of BTRFS was ported to new API that also understood\nthe options).\nnoatime\nunder read intensive work-loads, specifying\nnoatime\nsignificantly improves\nperformance because no new access time information needs to be written. Without\nthis option, the default is\nrelatime\n, which only reduces the number of\ninode atime updates in comparison to the traditional\nstrictatime\n. The worst\ncase for atime updates under\nrelatime\noccurs when many files are read whose\natime is older than 24 h and which are freshly snapshotted. In that case the\natime is updated and COW happens - for each file - in bulk. See also\nhttps://lwn.net/Articles/499293/\n-\nAtime and btrfs: a bad combination? (LWN, 2012-05-31)\n.\nNote that\nnoatime\nmay break applications that rely on atime uptimes like\nthe venerable Mutt (unless you use\nmaildir\nmailboxes).\nBootloaders\n\nGRUB2 (\nhttps://www.gnu.org/software/grub\n) has the most advanced support of\nbooting from BTRFS with respect to features.\nU-Boot (\nhttps://www.denx.de/wiki/U-Boot/\n) has decent support for booting but\nnot all BTRFS features are implemented, check the documentation.\nIn general, the first 1MiB on each device is unused with the exception of\nprimary superblock that is on the offset 64KiB and spans 4KiB. The rest can be\nfreely used by bootloaders or for other system information. Note that booting\nfrom a filesystem on\nzoned device\nis not supported.\nFilesystem limits\n\nmaximum file name length\n255\nThis limit is imposed by Linux VFS, the structures of BTRFS could store\nlarger file names.\nmaximum symlink target length\ndepends on the\nnodesize\nvalue, for 4KiB it’s 3949 bytes, for larger nodesize\nit’s 4095 due to the system limit PATH_MAX\nThe symlink target may not be a valid path, i.e. the path name components\ncan exceed the limits (NAME_MAX), there’s no content validation at\nsymlink(3)\ncreation.\nmaximum number of inodes\n2\n64\nbut depends on the available metadata space as the inodes are created\ndynamically\nEach subvolume is an independent namespace of inodes and thus their\nnumbers, so the limit is per subvolume, not for the whole filesystem.\ninode numbers\nminimum number: 256 (for subvolumes), regular files and directories: 257,\nmaximum number: (2\n64\n- 256)\nThe inode numbers that can be assigned to user created files are from\nthe whole 64bit space except first 256 and last 256 in that range that\nare reserved for internal b-tree identifiers.\nmaximum file length\ninherent limit of BTRFS is 2\n64\n(16 EiB) but the practical\nlimit of Linux VFS is 2\n63\n(8 EiB)\nmaximum number of subvolumes\nthe subvolume ids can go up to 2\n48\nbut the number of actual subvolumes\ndepends on the available metadata space\nThe space consumed by all subvolume metadata includes bookkeeping of\nshared extents can be large (MiB, GiB). The range is not the full 64bit\nrange because of qgroups that use the upper 16 bits for another\npurposes.\nmaximum number of hardlinks of a file in a directory\n65536 when the\nextref\nfeature is turned on during mkfs (default), roughly\n100 otherwise and depends on file name length that fits into one metadata node\nminimum filesystem size\nthe minimal size of each device depends on the\nmixed-bg\nfeature, without that\n(the default) it’s about 109MiB, with mixed-bg it’s is 16MiB\nFlexibility\n\nThe underlying design of BTRFS data structures allows a lot of flexibility and\nmaking changes after filesystem creation, like resizing, adding/removing space\nor enabling some features on-the-fly.\ndynamic inode creation\n-- there’s no fixed space or tables for tracking\ninodes so the number of inodes that can be created is bounded by the metadata\nspace and its utilization\nblock group profile change on-the-fly\n-- the block group profiles can be\nchanged on a mounted filesystem by running the balance operation and\nspecifying the conversion filters (see\nbalance\n)\nresize\n-- the space occupied by the filesystem on each device can be\nresized up (grow) or down (shrink) as long as the amount of data can be still\ncontained on the device\ndevice management\n-- devices can be added, removed or replaced without\nrequiring recreating the filesystem (mkfs), new redundancy options available\non more devices can be also utilized by rebalancing\nSysfs\n\nBtrfs has a sysfs interface to provide extra knobs.\nThe top level path is\n/sys/fs/btrfs/\n, and the main directory layout is the following:\nRelative Path\nDescription\nVersion\nfeatures/\nAll supported features\n3.14\n<UUID>/\nMounted fs UUID\n3.14\n<UUID>/allocation/\nSpace allocation info\n3.14\n<UUID>/bdi/\nBacking device info (writeback)\n5.9\n<UUID>/devices/<DEVID>/\nSymlink to each block device sysfs\n5.6\n<UUID>/devinfo/<DEVID>/\nBtrfs specific info for each device\n5.6\n<UUID>/discard/\nDiscard stats and tunables\n6.1\n<UUID>/features/\nFeatures of the filesystem\n3.14\n<UUID>/qgroups/\nGlobal qgroup info\n5.9\n<UUID>/qgroups/<LEVEL>_<ID>/\nInfo for each qgroup\n5.9\nFor\n/sys/fs/btrfs/features/\ndirectory, each file means a supported feature\nof the current kernel. Most files have value 0. Otherwise it depends on the file,\nvalue\n1\ntypically means the feature can be turned on a mounted filesystem.\nFor\n/sys/fs/btrfs/<UUID>/features/\ndirectory, each file means an enabled\nfeature on the mounted filesystem.\nThe features share the same name in section\nFILESYSTEM FEATURES\n.\nUUID\n\nFiles in\n/sys/fs/btrfs/<UUID>/\ndirectory are:\nbg_reclaim_threshold\n(RW, since: 5.19)\nUsed space percentage of total device space to start auto block group claim.\nMostly for zoned devices.\nchecksum\n(RO, since: 5.5)\nThe checksum used for the mounted filesystem.\nThis includes both the checksum type (see section\nCHECKSUM ALGORITHMS\n)\nand the implemented driver (mostly shows if it’s hardware accelerated).\nclone_alignment\n(RO, since: 3.16)\nThe bytes alignment for\nclone\nand\ndedupe\nioctls.\ncommit_stats\n(RW, since: 6.0)\nThe performance statistics for btrfs transaction commit since the first\nmount. Mostly for debugging purposes.\nWriting into this file will reset the maximum commit duration\n(\nmax_commit_ms\n) to 0. The file looks like:\ncommits 70649\nlast_commit_ms 2\nmax_commit_ms 131\ntotal_commit_ms 170840\ncommits\n- number of transaction commits since the first mount\nlast_commit_ms\n- duration in milliseconds of the last commit\nmax_commit_ms\n- maximum time a transaction commit took since first mount or last reset\ntotal_commit_ms\n- sum of all transaction commit times\nexclusive_operation\n(RO, since: 5.10)\nShows the running exclusive operation.\nCheck section\nFILESYSTEM EXCLUSIVE OPERATIONS\nfor details.\ngeneration\n(RO, since: 5.11)\nShow the generation of the mounted filesystem.\nlabel\n(RW, since: 3.14)\nShow the current label of the mounted filesystem.\nmetadata_uuid\n(RO, since: 5.0)\nShows the metadata UUID of the mounted filesystem.\nCheck\nmetadata_uuid\nfeature for more details.\nnodesize\n(RO, since: 3.14)\nShow the nodesize of the mounted filesystem.\nquota_override\n(RW, since: 4.13)\nShows the current quota override status.\n0 means no quota override.\n1 means quota override, quota can ignore the existing limit settings.\nread_policy\n(RW, since: 5.11)\nShows the current balance policy for reads.\nCurrently only\npid\n(balance using the process id (pid) value) is\nsupported. More balancing policies are available in experimental\nbuild, namely round-robin.\nsectorsize\n(RO, since: 3.14)\nShows the sectorsize of the mounted filesystem.\ntemp_fsid\n(RO, since 6.7)\nIndicate that this filesystem got assigned a temporary FSID at mount time,\nmaking possible to mount devices with the same FSID.\nUUID/allocations\n\nFiles and directories in\n/sys/fs/btrfs/<UUID>/allocations\ndirectory are:\nglobal_rsv_reserved\n(RO, since: 3.14)\nThe used bytes of the global reservation.\nglobal_rsv_size\n(RO, since: 3.14)\nThe total size of the global reservation.\ndata/\n,\nmetadata/\nand\nsystem/\ndirectories\n(RO, since: 5.14)\nSpace info accounting for the 3 block group types.\nUUID/allocations/{data,metadata,system}\n\nFiles in\n/sys/fs/btrfs/<UUID>/allocations/\ndata,metadata,system\ndirectory are:\nbg_reclaim_threshold\n(RW, since: 5.19)\nReclaimable space percentage of block group’s size (excluding\npermanently unusable space) to reclaim the block group.\nCan be used on regular or zoned devices.\nbytes_*\n(RO)\nValues of the corresponding data structures for the given block group\ntype and profile that are used internally and may change rapidly depending\non the load.\nComplete list: bytes_may_use, bytes_pinned, bytes_readonly,\nbytes_reserved, bytes_used, bytes_zone_unusable\nchunk_size\n(RW, since: 6.0)\nShows the chunk size. Can be changed for data and metadata (independently)\nand cannot be set for system block group type.\nCannot be set for zoned devices as it depends on the fixed device zone size.\nUpper bound is 10% of the filesystem size, the value must be multiple of 256MiB\nand greater than 0.\nsize_classes\n(RO, since: 6.3)\nNumbers of block groups of a given classes based on heuristics that\nmeasure extent length, age and fragmentation.\nnone 136\nsmall 374\nmedium 282\nlarge 93\nUUID/bdi\n\nSymlink to the sysfs directory of the backing device info (BDI), which is\nrelated to writeback process and infrastructure.\nUUID/devices\n\nFiles in\n/sys/fs/btrfs/<UUID>/devices\ndirectory are symlinks named\nafter device nodes (e.g. sda, dm-0) and pointing to their sysfs directory.\nUUID/devinfo\n\nThe directory contains subdirectories named after device ids (numeric values). Each\nsubdirectory has information about the device of the given\ndevid\n.\nUUID/devinfo/DEVID\n\nFiles in\n/sys/fs/btrfs/<UUID>/devinfo/<DEVID>\ndirectory are:\nerror_stats:\n(RO, since: 5.14)\nShows device stats of this device, same as\nbtrfs device stats\n(\nbtrfs-device(8)\n).\nwrite_errs 0\nread_errs 0\nflush_errs 0\ncorruption_errs 0\ngeneration_errs 0\nfsid:\n(RO, since: 5.17)\nShows the fsid which the device belongs to.\nIt can be different than the\nUUID\nif it’s a seed device.\nin_fs_metadata\n(RO, since: 5.6)\nShows whether we have found the device.\nShould always be 1, as if this turns to 0, the\nDEVID\ndirectory\nwould get removed automatically.\nmissing\n(RO, since: 5.6)\nShows whether the device is considered missing by the kernel module.\nreplace_target\n(RO, since: 5.6)\nShows whether the device is the replace target.\nIf no device replace is running, this value is 0.\nscrub_speed_max\n(RW, since: 5.14)\nShows the scrub speed limit for this device. The unit is Bytes/s.\n0 means no limit. The value can be set but is not persistent.\nwriteable\n(RO, since: 5.6)\nShow if the device is writeable.\nUUID/qgroups\n\nFiles in\n/sys/fs/btrfs/<UUID>/qgroups/\ndirectory are:\nenabled\n(RO, since: 6.1)\nShows if qgroup is enabled.\nAlso, if qgroup is disabled, the\nqgroups\ndirectory will\nbe removed automatically.\ninconsistent\n(RO, since: 6.1)\nShows if the qgroup numbers are inconsistent.\nIf 1, it’s recommended to do a qgroup rescan.\ndrop_subtree_threshold\n(RW, since: 6.1)\nShows the subtree drop threshold to automatically mark qgroup inconsistent.\nWhen dropping large subvolumes with qgroup enabled, there would be a huge\nload for qgroup accounting.\nIf we have a subtree whose level is larger than or equal to this value,\nwe will not trigger qgroup account at all, but mark qgroup inconsistent to\navoid the huge workload.\nDefault value is 3, which means that trees of low height will be accounted\nproperly as this is sufficiently fast. The value was 8 until 6.13 where\nno subtree drop can trigger qgroup rescan making it less useful.\nLower value can reduce qgroup workload, at the cost of extra qgroup rescan\nto re-calculate the numbers.\nUUID/qgroups/LEVEL_ID\n\nFiles in each\n/sys/fs/btrfs/<UUID>/qgroups/<LEVEL>_<ID>/\ndirectory are:\nexclusive\n(RO, since: 5.9)\nShows the exclusively owned bytes of the qgroup.\nlimit_flags\n(RO, since: 5.9)\nShows the numeric value of the limit flags.\nIf 0, means no limit implied.\nmax_exclusive\n(RO, since: 5.9)\nShows the limits on exclusively owned bytes.\nmax_referenced\n(RO, since: 5.9)\nShows the limits on referenced bytes.\nreferenced\n(RO, since: 5.9)\nShows the referenced bytes of the qgroup.\nrsv_data\n(RO, since: 5.9)\nShows the reserved bytes for data.\nrsv_meta_pertrans\n(RO, since: 5.9)\nShows the reserved bytes for per transaction metadata.\nrsv_meta_prealloc\n(RO, since: 5.9)\nShows the reserved bytes for preallocated metadata.\nUUID/discard\n\nFiles in\n/sys/fs/btrfs/<UUID>/discard/\ndirectory are:\ndiscardable_bytes\n(RO, since: 6.1)\nShows amount of bytes that can be discarded in the async discard and\nnodiscard mode.\ndiscardable_extents\n(RO, since: 6.1)\nShows number of extents to be discarded in the async discard and\nnodiscard mode.\ndiscard_bitmap_bytes\n(RO, since: 6.1)\nShows amount of discarded bytes from data tracked as bitmaps.\ndiscard_extent_bytes\n(RO, since: 6.1)\nShows amount of discarded extents from data tracked as bitmaps.\ndiscard_bytes_saved\n(RO, since: 6.1)\nShows the amount of bytes that were reallocated without being discarded.\nkbps_limit\n(RW, since: 6.1)\nTunable limit of kilobytes per second issued as discard IO in the async\ndiscard mode.\niops_limit\n(RW, since: 6.1)\nTunable limit of number of discard IO operations to be issued in the\nasync discard mode.\nmax_discard_size\n(RW, since: 6.1)\nTunable limit for size of one IO discard request.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://btrfs.readthedocs.io/en/latest/Administration.html"}}
{"text": "btrfs(5) — BTRFS  documentation\nManual pages\nbtrfs(5)\nView page source\nbtrfs(5)\n\nDESCRIPTION\n\nThis document describes topics related to BTRFS that are not specific to the\ntools.  Currently covers:\nmount options\nfilesystem features\nswapfile support\nchecksum algorithms\ncompression\nsysfs interface\nfilesystem exclusive operations\nfilesystem limits\nbootloader support\nfile attributes\nzoned mode\ncontrol device\nfilesystems with multiple block group profiles\nseeding device\nRAID56 status and recommended practices\nglossary\nstorage model, hardware considerations\nMOUNT OPTIONS\n\nBTRFS SPECIFIC MOUNT OPTIONS\n\nThis section describes mount options specific to BTRFS.  For the generic mount\noptions please refer to\nmount(8)\nmanual page and also see the section\nwith BTRFS specifics\nbelow\n. The options are\nsorted alphabetically (discarding the\nno\nprefix).\nNote\nMost mount options apply to the whole filesystem and only options in the\nfirst mounted subvolume will take effect. This is due to lack of implementation\nand may change in the future. This means that (for example) you can’t set\nper-subvolume\nnodatacow\n,\nnodatasum\n, or\ncompress\nusing mount options. This\nshould eventually be fixed, but it has proved to be difficult to implement\ncorrectly within the Linux VFS framework.\nMount options are processed in order, only the last occurrence of an option\ntakes effect and may disable other options due to constraints (see e.g.\nnodatacow\nand\ncompress\n). The output of\nmount\ncommand shows which options\nhave been applied.\nacl, noacl\n(default: on)\nEnable/disable support for POSIX Access Control Lists (ACLs).  See the\nacl(5)\nmanual page for more information about ACLs.\nThe support for ACL is build-time configurable (BTRFS_FS_POSIX_ACL) and\nmount fails if\nacl\nis requested but the feature is not compiled in.\nautodefrag, noautodefrag\n(since: 3.0, default: off)\nEnable automatic file defragmentation.\nWhen enabled, small random writes into files (in a range of tens of kilobytes,\ncurrently it’s 64KiB) are detected and queued up for the defragmentation process.\nMay not be well suited for large database workloads.\nThe read latency may increase due to reading the adjacent blocks that make up the\nrange for defragmentation, successive write will merge the blocks in the new\nlocation.\nWarning\nDefragmenting with Linux kernel versions < 3.9 or ≥ 3.14-rc2 as\nwell as with Linux stable kernel versions ≥ 3.10.31, ≥ 3.12.12 or\n≥ 3.13.4 will break up the reflinks of COW data (for example files\ncopied with\ncp --reflink\n, snapshots or de-duplicated data).\nThis may cause considerable increase of space usage depending on the\nbroken up reflinks.\nbarrier, nobarrier\n(default: on)\nEnsure that all IO write operations make it through the device cache and are stored\npermanently when the filesystem is at its consistency checkpoint. This\ntypically means that a flush command is sent to the device that will\nsynchronize all pending data and ordinary metadata blocks, then writes the\nsuperblock and issues another flush.\nThe write flushes incur a slight hit and also prevent the IO block\nscheduler to reorder requests in a more effective way. Disabling barriers gets\nrid of that penalty but will most certainly lead to a corrupted filesystem in\ncase of a crash or power loss. The ordinary metadata blocks could be yet\nunwritten at the time the new superblock is stored permanently, expecting that\nthe block pointers to metadata were stored permanently before.\nOn a device with a volatile battery-backed write-back cache, the\nnobarrier\noption will not lead to filesystem corruption as the pending blocks are\nsupposed to make it to the permanent storage.\nclear_cache\nForce clearing and rebuilding of the free space cache if something\nhas gone wrong.\nFor free space cache\nv1\n, this only clears (and, unless\nnospace_cache\nis\nused, rebuilds) the free space cache for block groups that are modified while\nthe filesystem is mounted with that option. To actually clear an entire free\nspace cache\nv1\n, see\nbtrfs check --clear-space-cache v1\n.\nFor free space cache\nv2\n, this clears the entire free space cache.\nTo do so without requiring to mounting the filesystem, see\nbtrfs check --clear-space-cache v2\n.\nSee also:\nspace_cache\n.\ncommit=<seconds>\n(since: 3.12, default: 30)\nSet the interval of periodic transaction commit when data are synchronized\nto permanent storage. Higher interval values lead to larger amount of unwritten\ndata to accumulate in memory, which has obvious consequences when the\nsystem crashes.  The upper bound is not forced, but a warning is\nprinted if it’s more than 300 seconds (5 minutes). Use with care.\nThe periodic commit is not the only mechanism to do the transaction commit,\nthis can also happen by explicit\nsync\nor indirectly by other\ncommands that affect the global filesystem state or internal kernel\nmechanisms that flush based on various thresholds or policies (e.g. cgroups).\ncompress, compress=<type[:level]>, compress-force, compress-force=<type[:level]>\n(default: off, level support since: 5.1)\nControl BTRFS file data compression.  Type may be specified as\nzlib\n,\nlzo\n,\nzstd\nor\nno\n(for no compression, used for remounting).  If no type\nis specified,\nzlib\nis used.  If\ncompress-force\nis specified,\nthen compression will always be attempted, but the data may end up uncompressed\nif the compression would make them larger.\nBoth\nzlib\nand\nzstd\n(since version 5.1) expose the compression level as a\ntunable knob with higher levels trading speed and memory (\nzstd\n) for higher\ncompression ratios. This can be set by appending a colon and the desired level.\nZLIB accepts the range [1, 9] and ZSTD accepts [1, 15]. If no level is set,\nboth currently use a default level of 3. The value 0 is an alias for the\ndefault level.\nOtherwise some simple heuristics are applied to detect an incompressible file.\nIf the first blocks written to a file are not compressible, the whole file is\npermanently marked to skip compression. As this is too simple, the\ncompress-force\nis a workaround that will compress most of the files at the\ncost of some wasted CPU cycles on failed attempts.\nSince kernel 4.15, a set of heuristic algorithms have been improved by using\nfrequency sampling, repeated pattern detection and Shannon entropy calculation\nto avoid that.\nNote\nIf compression is enabled,\nnodatacow\nand\nnodatasum\nare disabled.\ndatacow, nodatacow\n(default: on)\nEnable data copy-on-write for newly created files.\nNodatacow\nimplies\nnodatasum\n, and disables\ncompression\n. All files created\nunder\nnodatacow\nare also set the NOCOW file attribute (see\nchattr(1)\n).\nNote\nIf\nnodatacow\nor\nnodatasum\nare enabled, compression is disabled.\nUpdates in-place improve performance for workloads that do frequent overwrites,\nat the cost of potential partial writes, in case the write is interrupted\n(system crash, device failure).\ndatasum, nodatasum\n(default: on)\nEnable data checksumming for newly created files.\nDatasum\nimplies\ndatacow\n, i.e. the normal mode of operation. All files created\nunder\nnodatasum\ninherit the “no checksums” property, however there’s no\ncorresponding file attribute (see\nchattr(1)\n).\nNote\nIf\nnodatacow\nor\nnodatasum\nare enabled, compression is disabled.\nThere is a slight performance gain when checksums are turned off, the\ncorresponding metadata blocks holding the checksums do not need to updated.\nThe cost of checksumming of the blocks in memory is much lower than the IO,\nmodern CPUs feature hardware support of the checksumming algorithm.\ndegraded\n(default: off)\nAllow mounts with fewer devices than the RAID profile constraints\nrequire.  A read-write mount (or remount) may fail when there are too many devices\nmissing, for example if a stripe member is completely missing from RAID0.\nSince 4.14, the constraint checks have been improved and are verified on the\nchunk level, not at the device level. This allows degraded mounts of\nfilesystems with mixed RAID profiles for data and metadata, even if the\ndevice number constraints would not be satisfied for some of the profiles.\nExample: metadata -- raid1, data -- single, devices --\n/dev/sda\n,\n/dev/sdb\nSuppose the data are completely stored on\nsda\n, then missing\nsdb\nwill not\nprevent the mount, even if 1 missing device would normally prevent (any)\nsingle\nprofile to mount. In case some of the data chunks are stored on\nsdb\n,\nthen the constraint of single/data is not satisfied and the filesystem\ncannot be mounted.\ndevice=<devicepath>\nSpecify a path to a device that will be scanned for BTRFS filesystem during\nmount. This is usually done automatically by a device manager (like udev) or\nusing the\nbtrfs device scan\ncommand (e.g. run from the initial ramdisk). In\ncases where this is not possible the\ndevice\nmount option can help.\nNote\nBooting e.g. a RAID1 system may fail even if all filesystem’s\ndevice\npaths are provided as the actual device nodes may not be discovered by the\nsystem at that point.\ndiscard, discard=sync, discard=async, nodiscard\n(default: async when devices support it since 6.2, async support since: 5.6)\nEnable discarding of freed file blocks.  This is useful for SSD/NVMe\ndevices, thinly provisioned LUNs, or virtual machine images; however,\nevery storage layer must support discard for it to work.\nIn the synchronous mode (\nsync\nor without option value), lack of asynchronous\nqueued TRIM on the backing device TRIM can severely degrade performance,\nbecause a synchronous TRIM operation will be attempted instead. Queued TRIM\nrequires SATA devices with chipsets revision newer than 3.1 and devices.\nThe asynchronous mode (\nasync\n) gathers extents in larger chunks before sending\nthem to the devices for TRIM. The overhead and performance impact should be\nnegligible compared to the previous mode and it’s supposed to be the preferred\nmode if needed.\nIf it is not necessary to immediately discard freed blocks, then the\nfstrim\ntool can be used to discard all free blocks in a batch. Scheduling a TRIM\nduring a period of low system activity will prevent latent interference with\nthe performance of other operations. Also, a device may ignore the TRIM command\nif the range is too small, so running a batch discard has a greater probability\nof actually discarding the blocks.\nenospc_debug, noenospc_debug\n(default: off)\nEnable verbose output for some ENOSPC conditions. It’s safe to use but can\nbe noisy if the system reaches near-full state.\nfatal_errors=<action>\n(since: 3.4, default: bug)\nAction to take when encountering a fatal error.\nbug\nBUG()\non a fatal error, the system will stay in the crashed state and may be\nstill partially usable, but reboot is required for full operation\npanic\npanic()\non a fatal error, depending on other system configuration, this may\nbe followed by a reboot. Please refer to the documentation of kernel boot\nparameters, e.g.\npanic\n,\noops\nor\ncrashkernel\n.\nflushoncommit, noflushoncommit\n(default: off)\nThis option forces any data dirtied by a write in a prior transaction to commit\nas part of the current commit, effectively a full filesystem sync.\nThis makes the committed state a fully consistent view of the file system from\nthe application’s perspective (i.e. it includes all completed file system\noperations). This was previously the behavior only when a snapshot was\ncreated.\nWhen off, the filesystem is consistent but buffered writes may last more than\none transaction commit.\nfragment=<type>\n(depends on compile-time option CONFIG_BTRFS_DEBUG, since: 4.4, default: off)\nA debugging helper to intentionally fragment given\ntype\nof block groups. The\ntype can be\ndata\n,\nmetadata\nor\nall\n. This mount option should not be used\noutside of debugging environments and is not recognized if the kernel config\noption\nCONFIG_BTRFS_DEBUG\nis not enabled.\nnologreplay\n(default: off, even read-only)\nThe tree-log contains pending updates to the filesystem until the full commit.\nThe log is replayed on next mount, this can be disabled by this option.  See\nalso\ntreelog\n.  Note that\nnologreplay\nis the same as\nnorecovery\n.\nWarning\nCurrently, the tree log is replayed even with a read-only mount! To\ndisable that behaviour, mount also with\nnologreplay\n.\nmax_inline=<bytes>\n(default: min(2048, page size) )\nSpecify the maximum amount of space, that can be inlined in\na metadata b-tree leaf.  The value is specified in bytes, optionally\nwith a K suffix (case insensitive).  In practice, this value\nis limited by the filesystem block size (named\nsectorsize\nat mkfs time),\nand memory page size of the system. In case of sectorsize limit, there’s\nsome space unavailable due to b-tree leaf headers.  For example, a 4KiB\nsectorsize, maximum size of inline data is about 3900 bytes.\nInlining can be completely turned off by specifying 0. This will increase data\nblock slack if file sizes are much smaller than block size but will reduce\nmetadata consumption in return.\nNote\nThe default value has changed to 2048 in kernel 4.6.\nmetadata_ratio=<value>\n(default: 0, internal logic)\nSpecifies that 1 metadata chunk should be allocated after every\nvalue\ndata\nchunks. Default behaviour depends on internal logic, some percent of unused\nmetadata space is attempted to be maintained but is not always possible if\nthere’s not enough space left for chunk allocation. The option could be useful to\noverride the internal logic in favor of the metadata allocation if the expected\nworkload is supposed to be metadata intense (snapshots, reflinks, xattrs,\ninlined files).\nnorecovery\n(since: 4.5, default: off)\nDo not attempt any data recovery at mount time. This will disable\nlogreplay\nand avoids other write operations. Note that this option is the same as\nnologreplay\n.\nNote\nThe opposite option\nrecovery\nused to have different meaning but was\nchanged for consistency with other filesystems, where\nnorecovery\nis used for\nskipping log replay. BTRFS does the same and in general will try to avoid any\nwrite operations.\nrescan_uuid_tree\n(since: 3.12, default: off)\nForce check and rebuild procedure of the UUID tree. This should not\nnormally be needed. Alternatively the tree can be cleared from\nuserspace by command\nbtrfs rescue clear-uuid-tree\nand then it will be automatically rebuilt in kernel (the mount option\nis not needed in that case).\nrescue\n(since: 5.9)\nModes allowing mount with damaged filesystem structures, all requires\nthe filesystem to be mounted read-only and doesn’t allow remount to read-write.\nThis is supposed to provide unified and more fine grained tuning of\nerrors that affect filesystem operation.\nusebackuproot\n(since 5.9)\nTry to use backup root slots inside super block.\nReplaces standalone option\nusebackuproot\nnologreplay\n(since 5.9)\nDo not replay any dirty logs.\nReplaces standalone option\nnologreplay\nignorebadroots\n,\nibadroots\n(since: 5.11)\nIgnore bad tree roots, greatly improve the chance for data salvage.\nignoredatacsums\n,\nidatacsums\n(since: 5.11)\nIgnore data checksum verification.\nignoremetacsums\n,\nimetacsums\n(since 6.12)\nIgnore metadata checksum verification, useful for interrupted checksum conversion.\nall\n(since: 5.9)\nEnable all supported rescue options.\nskip_balance\n(since: 3.3, default: off)\nSkip automatic resume of an interrupted balance operation. The operation can\nlater be resumed with\nbtrfs balance resume\n, or the paused state can be\nremoved with\nbtrfs balance cancel\n. The default behaviour is to resume an\ninterrupted balance immediately after the filesystem is mounted.\nspace_cache, space_cache=<version>, nospace_cache\n(\nnospace_cache\nsince: 3.2,\nspace_cache=v1\nand\nspace_cache=v2\nsince 4.5, default:\nspace_cache=v2\n)\nOptions to control the free space cache. The free space cache greatly improves\nperformance when reading block group free space into memory. However, managing\nthe space cache consumes some resources, including a small amount of disk\nspace.\nThere are two implementations of the free space cache. The original\none, referred to as\nv1\n, used to be a safe default but has been\nsuperseded by\nv2\n.  The\nv1\nspace cache can be disabled at mount time\nwith\nnospace_cache\nwithout clearing.\nOn very large filesystems (many terabytes) and certain workloads, the\nperformance of the\nv1\nspace cache may degrade drastically. The\nv2\nimplementation, which adds a new b-tree called the free space tree, addresses\nthis issue. Once enabled, the\nv2\nspace cache will always be used and cannot\nbe disabled unless it is cleared. Use\nclear_cache,space_cache=v1\nor\nclear_cache,nospace_cache\nto do so. If\nv2\nis enabled, and\nv1\nspace\ncache will be cleared (at the first mount) and kernels without\nv2\nsupport will only be able to mount the filesystem in read-only mode.\nOn an unmounted filesystem the caches (both versions) can be cleared by\n“btrfs check --clear-space-cache”.\nThe\nbtrfs-check(8)\nand\n:doc:`mkfs.btrfs\ncommands have full\nv2\nfree space\ncache support since v4.19.\nIf a version is not explicitly specified, the default implementation will be\nchosen, which is\nv2\n.\nssd, ssd_spread, nossd, nossd_spread\n(default: SSD autodetected)\nOptions to control SSD allocation schemes.  By default, BTRFS will\nenable or disable SSD optimizations depending on status of a device with\nrespect to rotational or non-rotational type. This is determined by the\ncontents of\n/sys/block/DEV/queue/rotational\n). If it is 0, the\nssd\noption is turned on.  The option\nnossd\nwill disable the\nautodetection.\nThe optimizations make use of the absence of the seek penalty that’s inherent\nfor the rotational devices. The blocks can be typically written faster and\nare not offloaded to separate threads.\nNote\nSince 4.14, the block layout optimizations have been dropped. This used\nto help with first generations of SSD devices. Their FTL (flash translation\nlayer) was not effective and the optimization was supposed to improve the wear\nby better aligning blocks. This is no longer true with modern SSD devices and\nthe optimization had no real benefit. Furthermore it caused increased\nfragmentation. The layout tuning has been kept intact for the option\nssd_spread\n.\nThe\nssd_spread\nmount option attempts to allocate into bigger and aligned\nchunks of unused space, and may perform better on low-end SSDs.\nssd_spread\nimplies\nssd\n, enabling all other SSD heuristics as well. The option\nnossd\nwill disable all SSD options while\nnossd_spread\nonly disables\nssd_spread\n.\nsubvol=<path>\nMount subvolume from\npath\nrather than the toplevel subvolume. The\npath\nis always treated as relative to the toplevel subvolume.\nThis mount option overrides the default subvolume set for the given filesystem.\nsubvolid=<subvolid>\nMount subvolume specified by a\nsubvolid\nnumber rather than the toplevel\nsubvolume.  You can use\nbtrfs subvolume list\nof\nbtrfs subvolume show\nto see\nsubvolume ID numbers.\nThis mount option overrides the default subvolume set for the given filesystem.\nNote\nIf both\nsubvolid\nand\nsubvol\nare specified, they must point at the\nsame subvolume, otherwise the mount will fail.\nthread_pool=<number>\n(default: min(NRCPUS + 2, 8) )\nThe number of worker threads to start. NRCPUS is number of on-line CPUs\ndetected at the time of mount. Small number leads to less parallelism in\nprocessing data and metadata, higher numbers could lead to a performance hit\ndue to increased locking contention, process scheduling, cache-line bouncing or\ncostly data transfers between local CPU memories.\ntreelog, notreelog\n(default: on)\nEnable the tree logging used for\nfsync\nand\nO_SYNC\nwrites. The tree log\nstores changes without the need of a full filesystem sync. The log operations\nare flushed at sync and transaction commit. If the system crashes between two\nsuch syncs, the pending tree log operations are replayed during mount.\nWarning\nCurrently, the tree log is replayed even with a read-only mount! To\ndisable that behaviour, also mount with\nnologreplay\n.\nThe tree log could contain new files/directories, these would not exist on\na mounted filesystem if the log is not replayed.\nusebackuproot\n(since: 4.6, default: off)\nEnable autorecovery attempts if a bad tree root is found at mount time.\nCurrently this scans a backup list of several previous tree roots and tries to\nuse the first readable. This can be used with read-only mounts as well.\nNote\nThis option has replaced\nrecovery\nwhich has been deprecated.\nuser_subvol_rm_allowed\n(default: off)\nAllow subvolumes to be deleted by their respective owner. Otherwise, only the\nroot user can do that.\nNote\nHistorically, any user could create a snapshot even if he was not owner\nof the source subvolume, the subvolume deletion has been restricted for that\nreason. The subvolume creation has been restricted but this mount option is\nstill required. This is a usability issue.\nSince 4.18, the\nrmdir(2)\nsyscall can delete an empty subvolume just like an\nordinary directory. Whether this is possible can be detected at runtime, see\nrmdir_subvol\nfeature in\nFILESYSTEM FEATURES\n.\nDEPRECATED MOUNT OPTIONS\n\nList of mount options that have been removed, kept for backward compatibility.\nrecovery\n(since: 3.2, default: off, deprecated since: 4.5)\nNote\nThis option has been replaced by\nusebackuproot\nand should not be used\nbut will work on 4.5+ kernels.\ninode_cache, noinode_cache\n(removed in: 5.11, since: 3.0, default: off)\nNote\nThe functionality has been removed in 5.11, any stale data created by\nprevious use of the\ninode_cache\noption can be removed by\nbtrfs rescue clear-ino-cache\n.\ncheck_int, check_int_data, check_int_print_mask=<value>\n(removed in: 6.7, since: 3.0, default: off)\nThese debugging options control the behavior of the integrity checking\nmodule (the BTRFS_FS_CHECK_INTEGRITY config option required). The main goal is\nto verify that all blocks from a given transaction period are properly linked.\ncheck_int\nenables the integrity checker module, which examines all\nblock write requests to ensure on-disk consistency, at a large\nmemory and CPU cost.\ncheck_int_data\nincludes extent data in the integrity checks, and\nimplies the\ncheck_int\noption.\ncheck_int_print_mask\ntakes a bit mask of BTRFSIC_PRINT_MASK_* values\nas defined in\nfs/btrfs/check-integrity.c\n, to control the integrity\nchecker module behavior.\nSee comments at the top of\nfs/btrfs/check-integrity.c\nfor more information.\nNOTES ON GENERIC MOUNT OPTIONS\n\nSome of the general mount options from\nmount(8)\nthat affect BTRFS and are\nworth mentioning.\ncontext\nThe context refers to the SELinux contexts and policy definitions passed\nas mount options. This works properly since version v6.8 (because the\nmount option parser of BTRFS was ported to new API that also understood\nthe options).\nnoatime\nunder read intensive work-loads, specifying\nnoatime\nsignificantly improves\nperformance because no new access time information needs to be written. Without\nthis option, the default is\nrelatime\n, which only reduces the number of\ninode atime updates in comparison to the traditional\nstrictatime\n. The worst\ncase for atime updates under\nrelatime\noccurs when many files are read whose\natime is older than 24 h and which are freshly snapshotted. In that case the\natime is updated and COW happens - for each file - in bulk. See also\nhttps://lwn.net/Articles/499293/\n-\nAtime and btrfs: a bad combination? (LWN, 2012-05-31)\n.\nNote that\nnoatime\nmay break applications that rely on atime uptimes like\nthe venerable Mutt (unless you use\nmaildir\nmailboxes).\nFILESYSTEM FEATURES\n\nThe basic set of filesystem features gets extended over time. The backward\ncompatibility is maintained and the features are optional, need to be\nexplicitly asked for so accidental use will not create incompatibilities.\nThere are several classes and the respective tools to manage the features:\nat mkfs time only\nThis is namely for core structures, like the b-tree nodesize or checksum\nalgorithm, see\nmkfs.btrfs(8)\nfor more details.\nafter mkfs, on an unmounted filesystem\nFeatures that may optimize internal structures or add new structures to support\nnew functionality, see\nbtrfstune(8)\n. The command\nbtrfs inspect-internal dump-super /dev/sdx\nwill dump a superblock, you can map the value of\nincompat_flags\nto the features listed below\nafter mkfs, on a mounted filesystem\nThe features of a filesystem (with a given UUID) are listed in\n/sys/fs/btrfs/UUID/features/\n, one file per feature. The status is stored\ninside the file. The value\n1\nis for enabled and active, while\n0\nmeans the\nfeature was enabled at mount time but turned off afterwards.\nWhether a particular feature can be turned on a mounted filesystem can be found\nin the directory\n/sys/fs/btrfs/features/\n, one file per feature. The value\n1\nmeans the feature can be enabled.\nList of features (see also\nmkfs.btrfs(8)\nsection\nFILESYSTEM FEATURES\n):\nbig_metadata\n(since: 3.4)\nthe filesystem uses\nnodesize\nfor metadata blocks, this can be bigger than the\npage size\nblock_group_tree\n(since: 6.1)\nblock group item representation using a dedicated b-tree, this can greatly\nreduce mount time for large filesystems\ncompress_lzo\n(since: 2.6.38)\nthe\nlzo\ncompression has been used on the filesystem, either as a mount option\nor via\nbtrfs filesystem defrag\n.\ncompress_zstd\n(since: 4.14)\nthe\nzstd\ncompression has been used on the filesystem, either as a mount option\nor via\nbtrfs filesystem defrag\n.\ndefault_subvol\n(since: 2.6.34)\nthe default subvolume has been set on the filesystem\nextended_iref\n(since: 3.7)\nincreased hardlink limit per file in a directory to 65536, older kernels\nsupported a varying number of hardlinks depending on the sum of all file name\nsizes that can be stored into one metadata block\nfree_space_tree\n(since: 4.5)\nfree space representation using a dedicated b-tree, successor of v1 space cache\nmetadata_uuid\n(since: 5.0)\nthe main filesystem UUID is the metadata_uuid, which stores the new UUID only\nin the superblock while all metadata blocks still have the UUID set at mkfs\ntime, see\nbtrfstune(8)\nfor more\nmixed_backref\n(since: 2.6.31)\nthe last major disk format change, improved backreferences, now default\nmixed_groups\n(since: 2.6.37)\nmixed data and metadata block groups, i.e. the data and metadata are not\nseparated and occupy the same block groups, this mode is suitable for small\nvolumes as there are no constraints how the remaining space should be used\n(compared to the split mode, where empty metadata space cannot be used for data\nand vice versa)\non the other hand, the final layout is quite unpredictable and possibly highly\nfragmented, which means worse performance\nno_holes\n(since: 3.14)\nimproved representation of file extents where holes are not explicitly\nstored as an extent, saves a few percent of metadata if sparse files are used\nraid1c34\n(since: 5.5)\nextended RAID1 mode with copies on 3 or 4 devices respectively\nraid_stripe_tree\n(since: 6.7)\na separate tree for tracking file extents on RAID profiles\nRAID56\n(since: 3.9)\nthe filesystem contains or contained a RAID56 profile of block groups\nrmdir_subvol\n(since: 4.18)\nindicate that\nrmdir(2)\nsyscall can delete an empty subvolume just like an\nordinary directory. Note that this feature only depends on the kernel version.\nskinny_metadata\n(since: 3.10)\nreduced-size metadata for extent references, saves a few percent of metadata\nsend_stream_version\n(since: 5.10)\nnumber of the highest supported send stream version\nsimple_quota\n(since: 6.7)\nsimplified quota accounting\nsupported_checksums\n(since: 5.5)\nlist of checksum algorithms supported by the kernel module, the respective\nmodules or built-in implementing the algorithms need to be present to mount\nthe filesystem, see section\nCHECKSUM ALGORITHMS\n.\nsupported_sectorsizes\n(since: 5.13)\nlist of values that are accepted as sector sizes (\nmkfs.btrfs --sectorsize\n) by\nthe running kernel\nsupported_rescue_options\n(since: 5.11)\nlist of values for the mount option\nrescue\nthat are supported by the running\nkernel, see\nbtrfs(5)\nzoned\n(since: 5.12)\nzoned mode is allocation/write friendly to host-managed zoned devices,\nallocation space is partitioned into fixed-size zones that must be updated\nsequentially, see section\nZONED MODE\nSWAPFILE SUPPORT\n\nA swapfile, when active, is a file-backed swap area.  It is supported since kernel 5.0.\nUse\nswapon(8)\nto activate it, until then (respectively again after deactivating it\nwith\nswapoff(8)\n) it’s just a normal file (with NODATACOW set), for which the special\nrestrictions for active swapfiles don’t apply.\nThere are some limitations of the implementation in BTRFS and Linux swap\nsubsystem:\nfilesystem - must be only single device\nfilesystem - must have only\nsingle\ndata profile\nsubvolume - cannot be snapshotted if it contains any active swapfiles\nswapfile - must be preallocated (i.e. no holes)\nswapfile - must be NODATACOW (i.e. also NODATASUM, no compression)\nThe limitations come namely from the COW-based design and mapping layer of\nblocks that allows the advanced features like relocation and multi-device\nfilesystems. However, the swap subsystem expects simpler mapping and no\nbackground changes of the file block location once they’ve been assigned to\nswap. The constraints mentioned above (single device and single profile) are\nrelated to the swapfile itself, i.e. the extents and their placement. It is\npossible to create swapfile on multi-device filesystem as long as the extents\nare on one device but this cannot be affected by user and depends on free space\nfragmentation and available unused space for new chunks.\nWith active swapfiles, the following whole-filesystem operations will skip\nswapfile extents or may fail:\nbalance - block groups with extents of any active swapfiles are skipped and\nreported, the rest will be processed normally\nresize grow - unaffected\nresize shrink - works as long as the extents of any active swapfiles are\noutside of the shrunk range\ndevice add - if the new devices do not interfere with any already active swapfiles\nthis operation will work, though no new swapfile can be activated\nafterwards\ndevice delete - if the device has been added as above, it can be also deleted\ndevice replace - ditto\nWhen there are no active swapfiles and a whole-filesystem exclusive operation\nis running (e.g. balance, device delete, shrink), the swapfiles cannot be\ntemporarily activated. The operation must finish first.\nTo create and activate a swapfile run the following commands:\n# truncate -s 0 swapfile\n# chattr +C swapfile\n# fallocate -l 2G swapfile\n# chmod 0600 swapfile\n# mkswap swapfile\n# swapon swapfile\nSince version 6.1 it’s possible to create the swapfile in a single command\n(except the activation):\n# btrfs filesystem mkswapfile --size 2G swapfile\n# swapon swapfile\nPlease note that the UUID returned by the\nmkswap\nutility identifies the swap\n“filesystem” and because it’s stored in a file, it’s not generally visible and\nusable as an identifier unlike if it was on a block device.\nOnce activated the file will appear in\n/proc/swaps\n:\n# cat /proc/swaps\nFilename          Type          Size           Used      Priority\n/path/swapfile    file          2097152        0         -2\nThe swapfile can be created as one-time operation or, once properly created,\nactivated on each boot by the\nswapon -a\ncommand (usually started by the\nservice manager). Add the following entry to\n/etc/fstab\n, assuming the\nfilesystem that provides the\n/path\nhas been already mounted at this point.\nAdditional mount options relevant for the swapfile can be set too (like\npriority, not the BTRFS mount options).\n/path/swapfile        none        swap        defaults      0 0\nFrom now on the subvolume with the active swapfile cannot be snapshotted until\nthe swapfile is deactivated again by\nswapoff\n. Then the swapfile is a\nregular file and the subvolume can be snapshotted again, though this would prevent\nanother activation any swapfile that has been snapshotted. New swapfiles (not\nsnapshotted) can be created and activated.\nOtherwise, an inactive swapfile does not affect the containing subvolume. Activation\ncreates a temporary in-memory status and prevents some file operations, but is\nnot stored permanently.\nHibernation\n\nA swapfile can be used for hibernation but it’s not straightforward. Before\nhibernation a resume offset must be written to file\n/sys/power/resume_offset\nor the kernel command line parameter\nresume_offset\nmust be set.\nThe value is the physical offset on the device. Note that\nthis is not the same\nvalue that\nfilefrag\nprints as physical offset!\nBtrfs filesystem uses mapping between logical and physical addresses but here\nthe physical can still map to one or more device-specific physical block\naddresses. It’s the device-specific physical offset that is suitable as resume\noffset.\nSince version 6.1 there’s a command\nbtrfs inspect-internal map-swapfile\nthat will print the device physical offset and the adjusted value for\n/sys/power/resume_offset\n.  Note that the value is divided by page size, i.e.\nit’s not the offset itself.\n# btrfs filesystem mkswapfile swapfile\n# btrfs inspect-internal map-swapfile swapfile\nPhysical\nstart:\n811511726080\nResume\noffset:\n198122980\nFor scripting and convenience the option\n-r\nwill print just the offset:\n# btrfs inspect-internal map-swapfile -r swapfile\n198122980\nThe command\nmap-swapfile\nalso verifies all the requirements, i.e. no holes,\nsingle device, etc.\nTroubleshooting\n\nIf the swapfile activation fails please verify that you followed all the steps\nabove or check the system log (e.g.\ndmesg\nor\njournalctl\n) for more\ninformation.\nNotably, the\nswapon\nutility exits with a message that does not say what\nfailed:\n# swapon /path/swapfile\nswapon: /path/swapfile: swapon failed: Invalid argument\nThe specific reason is likely to be printed to the system log by the btrfs\nmodule:\n# journalctl -t kernel | grep swapfile\nkernel: BTRFS warning (device sda): swapfile must have single data profile\nCHECKSUM ALGORITHMS\n\nData and metadata are checksummed by default. The checksum is calculated before\nwriting and verified after reading the blocks from devices. The whole metadata\nblock has an inline checksum stored in the b-tree node header. Each data block\nhas a detached checksum stored in the checksum tree.\nNote\nSince a data checksum is calculated just before submitting to the block\ndevice, btrfs has a strong requirement that the corresponding data block must\nnot be modified until the writeback is finished.\nThis requirement is met for a buffered write as btrfs has the full control on\nits page cache, but a direct write (\nO_DIRECT\n) bypasses page cache, and\nbtrfs can not control the direct IO buffer (as it can be in user space memory).\nThus it’s possible that a user space program modifies its direct write buffer\nbefore the buffer is fully written back, and this can lead to a data\nchecksum mismatch.\nTo avoid this, kernel starting with version 6.14 will force a direct\nwrite to fall back to buffered, if the inode requires a data checksum.\nThis will bring a small performance penalty. If you require true zero-copy\ndirect writes, then set the\nNODATASUM\nflag for the inode and make\nsure the direct IO buffer is fully aligned to block size.\nThere are several checksum algorithms supported. The default and backward\ncompatible algorithm is\ncrc32c\n. Since kernel 5.5 there are three more with different\ncharacteristics and trade-offs regarding speed and strength. The following list\nmay help you to decide which one to select.\nCRC32C (32 bits digest)\nDefault, best backward compatibility. Very fast, modern CPUs have\ninstruction-level support, not collision-resistant but still good error\ndetection capabilities.\nXXHASH (64 bits digest)\nCan be used as CRC32C successor. Very fast, optimized for modern CPUs utilizing\ninstruction pipelining, good collision resistance and error detection.\nSHA256 (256 bits digest)\nCryptographic-strength hash. Relatively slow but with possible CPU\ninstruction acceleration or specialized hardware cards. FIPS certified and\nin wide use.\nBLAKE2b (256 bits digest)\nCryptographic-strength hash. Relatively fast, with possible CPU acceleration\nusing SIMD extensions. Not standardized but based on BLAKE which was a SHA3\nfinalist, in wide use. The algorithm used is BLAKE2b-256 that’s optimized for\n64-bit platforms.\nThe\ndigest size\naffects overall size of data block checksums stored in the\nfilesystem.  The metadata blocks have a fixed area up to 256 bits (32 bytes), so\nthere’s no increase. Each data block has a separate checksum stored, with\nadditional overhead of the b-tree leaves.\nApproximate relative performance of the algorithms, measured against CRC32C\nusing implementations on a 11th gen 3.6GHz intel CPU:\nDigest\nCycles/4KiB\nRatio\nImplementation\nCRC32C\n470\n1.00\nCPU instruction, PCL combination\nXXHASH\n870\n1.9\nreference impl.\nSHA256\n7600\n16\nlibgcrypt\nSHA256\n8500\n18\nopenssl\nSHA256\n8700\n18\nbotan\nSHA256\n32000\n68\nbuiltin, CPU instruction\nSHA256\n37000\n78\nlibsodium\nSHA256\n78000\n166\nbuiltin, reference impl.\nBLAKE2b\n10000\n21\nbuiltin/AVX2\nBLAKE2b\n10900\n23\nlibgcrypt\nBLAKE2b\n13500\n29\nbuiltin/SSE41\nBLAKE2b\n13700\n29\nlibsodium\nBLAKE2b\n14100\n30\nopenssl\nBLAKE2b\n14500\n31\nkcapi\nBLAKE2b\n14500\n34\nbuiltin, reference impl.\nMany kernels are configured with SHA256 as built-in and not as a module.\nUp to kernel v6.15 the accelerated versions are however provided by the\nmodules and must be loaded\nexplicitly (\nmodprobe sha256\n) before mounting the filesystem to make use of\nthem. You can check in\n/sys/fs/btrfs/FSID/checksum\nwhich one is used. If you\nsee\nsha256-generic\n, then you may want to unmount and mount the filesystem\nagain. Changing that on a mounted filesystem is not possible.\nSince kernel v6.16 the accelereated implementation is always used if available.\nCheck the file\n/proc/crypto\n, when the implementation is built-in, you’d find:\nname         : sha256\ndriver       : sha256-generic\nmodule       : kernel\npriority     : 100\n...\nWhile accelerated implementation is e.g.:\nname         : sha256\ndriver       : sha256-avx2\nmodule       : sha256_ssse3\npriority     : 170\n...\nCOMPRESSION\n\nBtrfs supports transparent file compression. There are three algorithms\navailable: ZLIB, LZO and ZSTD (since v4.14), with various levels.\nThe compression happens on the level of file extents and the algorithm is\nselected by file property, mount option or by a defrag command.\nYou can have a single btrfs mount point that has some files that are\nuncompressed, some that are compressed with LZO, some with ZLIB, for instance\n(though you may not want it that way, it is supported).\nOnce the compression is set, all newly written data will be compressed, i.e.\nexisting data are untouched. Data are split into smaller chunks (128KiB) before\ncompression to make random rewrites possible without a high performance hit. Due\nto the increased number of extents the metadata consumption is higher. The\nchunks are compressed in parallel.\nThe algorithms can be characterized as follows regarding the speed/ratio\ntrade-offs:\nZLIB\nslower, higher compression ratio\nlevels: 1 to 9, mapped directly, default level is 3\ngood backward compatibility\nLZO\nfaster compression and decompression than ZLIB, worse compression ratio, designed to be fast\nno levels\ngood backward compatibility\nZSTD\ncompression comparable to ZLIB with higher compression/decompression speeds and different ratio\nlevels: -15..15, mapped directly, default is 3\nsupport since 4.14\nlevels 1..15 supported since 5.1\nlevels -15..-1 supported since 6.15\nThe differences depend on the actual data set and cannot be expressed by a\nsingle number or recommendation. Higher levels consume more CPU time and may\nnot bring a significant improvement, lower levels are close to real time.\nHow to enable compression\n\nTypically the compression can be enabled on the whole filesystem, specified for\nthe mount point. Note that the compression mount options are shared among all\nmounts of the same filesystem, either bind mounts or subvolume mounts.\nPlease refer to\nbtrfs(5)\nsection\nMOUNT OPTIONS\n.\n$\nmount\n-o\ncompress\n=\nzstd\n/dev/sdx\n/mnt\nThis will enable the\nzstd\nalgorithm on the default level (which is 3).\nThe level can be specified manually too like\nzstd:3\n. Higher levels compress\nbetter at the cost of time. This in turn may cause increased write latency, low\nlevels are suitable for real-time compression and on reasonably fast CPU don’t\ncause noticeable performance drops.\n$\nbtrfs\nfilesystem\ndefrag\n-czstd\nfile\nThe command above will start defragmentation of the whole\nfile\nand apply\nthe compression, regardless of the mount option. The compression level can be\nalso specified with the\n--level\nor\n-L\nargument as of version\n6.14\n.\nThe compression algorithm is not persistent and applies only\nto the defragmentation command, for any other writes other compression settings\napply.\nPersistent settings on a per-file basis can be set in two ways:\n$\nchattr\n+c\nfile\n$\nbtrfs\nproperty\nset\nfile\ncompression\nzstd\nThe first command is using legacy interface of file attributes inherited from\next2 filesystem and is not flexible, so by default the\nzlib\ncompression is\nset. The other command sets a property on the file with the given algorithm.\n(Note: setting level that way is not yet implemented.)\nCompression levels\n\nThe level support of ZLIB has been added in v4.14, LZO does not support levels\n(the kernel implementation provides only one), ZSTD level support has been added\nin v5.1 and the negative levels in v6.15.\nThere are 9 levels of ZLIB supported (1 to 9), mapping 1:1 from the mount option\nto the algorithm defined level. The default is level 3, which provides the\nreasonably good compression ratio and is still reasonably fast. The difference\nin compression gain of levels 7, 8 and 9 is comparable but the higher levels\ntake longer.\nThe ZSTD support includes levels -15..15, a subset of full range of what ZSTD\nprovides. Levels -15..-1 are real-time with worse compression ratio, levels\n1..3 are near real-time with good compression, 4..8 are slower with improved\ncompression and 9..15 try even harder though the resulting size may not be\nsignificantly improved. Higher levels also require more memory and as they need\nmore CPU the system performance is affected.\nLevel 0 always maps to the default. The compression level does not affect\ncompatibility.\nExceptions\n\nAny file that has been touched by the\nfallocate\nsystem call will always be\nexcepted from compression even if\nforce-compress\nmount option is used.\nThe reason for this is that a successful\nfallocate\ncall must guarantee that\nfuture writes to the allocated range will not fail because of lack of space.\nThis is difficult to guarantee in a COW filesystem. To reduce the chances of\nit happening, btrfs preallocates space and disables compression for the file.\nAs a workaround, one can trigger a compressed rewrite for such a file using the\nbtrfs defrag\ncommand. Be aware that if the file is touched again by the\nfallocate\nsystem call, it will be excepted again from compression for all the\nnew data written to it.\nIncompressible data\n\nFiles with already compressed data or with data that won’t compress well with\nthe CPU and memory constraints of the kernel implementations are using a simple\ndecision logic. If the first portion of data being compressed is not smaller\nthan the original, the compression of the whole file is disabled. Unless the\nfilesystem is mounted with\ncompress-force\nin which case btrfs will try\ncompressing every block, falling back to storing the uncompressed version for\neach block that ends up larger after compression. This is not optimal and\nsubject to optimizations and further development.\nIf a file is identified as incompressible, a flag is set (\nNOCOMPRESS\n) and it’s\nsticky. On that file compression won’t be performed unless forced. The flag\ncan be also set by\nchattr +m\n(since e2fsprogs 1.46.2) or by properties with\nvalue\nno\nor\nnone\n. Empty value will reset it to the default that’s currently\napplicable on the mounted filesystem.\nThere are two ways to detect incompressible data:\nactual compression attempt - data are compressed, if the result is not smaller,\nit’s discarded, so this depends on the algorithm and level\npre-compression heuristics - a quick statistical evaluation on the data is\nperformed and based on the result either compression is performed or skipped,\nthe NOCOMPRESS bit is not set just by the heuristic, only if the compression\nalgorithm does not make an improvement\n$\nlsattr\nfile\n---------------------m\nfile\nUsing the forcing compression is not recommended, the heuristics are\nsupposed to decide that and compression algorithms internally detect\nincompressible data too.\nPre-compression heuristics\n\nThe heuristics aim to do a few quick statistical tests on the compressed data\nin order to avoid probably costly compression that would turn out to be\ninefficient. Compression algorithms could have internal detection of\nincompressible data too but this leads to more overhead as the compression is\ndone in another thread and has to write the data anyway. The heuristic is\nread-only and can utilize cached memory.\nThe tests performed based on the following: data sampling, long repeated\npattern detection, byte frequency, Shannon entropy.\nCompatibility\n\nCompression requires both data checksums and COW, so either\nnodatasum\nor\nnodatasum\nmount option/inode flag will result in no compression.\nDirect IO reads of compressed data will always fallback to buffered reads.\nDirect IO write behavior depends on the inode flag.\nFor inodes with data checksum, direct IO writes always fallback to buffered\nwrites, thus can generate compressed data if the mount option/inode flags\nallows that.\nFor inodes without data checksums, direct IO writes will not populate page cache,\nand since the inode has no data checksums, no compressed data will be generated\nanyway.\nThe compression algorithms have been added over time so the version\ncompatibility should be also considered, together with other tools that may\naccess the compressed data like bootloaders.\nSYSFS INTERFACE\n\nBtrfs has a sysfs interface to provide extra knobs.\nThe top level path is\n/sys/fs/btrfs/\n, and the main directory layout is the following:\nRelative Path\nDescription\nVersion\nfeatures/\nAll supported features\n3.14\n<UUID>/\nMounted fs UUID\n3.14\n<UUID>/allocation/\nSpace allocation info\n3.14\n<UUID>/bdi/\nBacking device info (writeback)\n5.9\n<UUID>/devices/<DEVID>/\nSymlink to each block device sysfs\n5.6\n<UUID>/devinfo/<DEVID>/\nBtrfs specific info for each device\n5.6\n<UUID>/discard/\nDiscard stats and tunables\n6.1\n<UUID>/features/\nFeatures of the filesystem\n3.14\n<UUID>/qgroups/\nGlobal qgroup info\n5.9\n<UUID>/qgroups/<LEVEL>_<ID>/\nInfo for each qgroup\n5.9\nFor\n/sys/fs/btrfs/features/\ndirectory, each file means a supported feature\nof the current kernel. Most files have value 0. Otherwise it depends on the file,\nvalue\n1\ntypically means the feature can be turned on a mounted filesystem.\nFor\n/sys/fs/btrfs/<UUID>/features/\ndirectory, each file means an enabled\nfeature on the mounted filesystem.\nThe features share the same name in section\nFILESYSTEM FEATURES\n.\nUUID\n\nFiles in\n/sys/fs/btrfs/<UUID>/\ndirectory are:\nbg_reclaim_threshold\n(RW, since: 5.19)\nUsed space percentage of total device space to start auto block group claim.\nMostly for zoned devices.\nchecksum\n(RO, since: 5.5)\nThe checksum used for the mounted filesystem.\nThis includes both the checksum type (see section\nCHECKSUM ALGORITHMS\n)\nand the implemented driver (mostly shows if it’s hardware accelerated).\nclone_alignment\n(RO, since: 3.16)\nThe bytes alignment for\nclone\nand\ndedupe\nioctls.\ncommit_stats\n(RW, since: 6.0)\nThe performance statistics for btrfs transaction commit since the first\nmount. Mostly for debugging purposes.\nWriting into this file will reset the maximum commit duration\n(\nmax_commit_ms\n) to 0. The file looks like:\ncommits 70649\nlast_commit_ms 2\nmax_commit_ms 131\ntotal_commit_ms 170840\ncommits\n- number of transaction commits since the first mount\nlast_commit_ms\n- duration in milliseconds of the last commit\nmax_commit_ms\n- maximum time a transaction commit took since first mount or last reset\ntotal_commit_ms\n- sum of all transaction commit times\nexclusive_operation\n(RO, since: 5.10)\nShows the running exclusive operation.\nCheck section\nFILESYSTEM EXCLUSIVE OPERATIONS\nfor details.\ngeneration\n(RO, since: 5.11)\nShow the generation of the mounted filesystem.\nlabel\n(RW, since: 3.14)\nShow the current label of the mounted filesystem.\nmetadata_uuid\n(RO, since: 5.0)\nShows the metadata UUID of the mounted filesystem.\nCheck\nmetadata_uuid\nfeature for more details.\nnodesize\n(RO, since: 3.14)\nShow the nodesize of the mounted filesystem.\nquota_override\n(RW, since: 4.13)\nShows the current quota override status.\n0 means no quota override.\n1 means quota override, quota can ignore the existing limit settings.\nread_policy\n(RW, since: 5.11)\nShows the current balance policy for reads.\nCurrently only\npid\n(balance using the process id (pid) value) is\nsupported. More balancing policies are available in experimental\nbuild, namely round-robin.\nsectorsize\n(RO, since: 3.14)\nShows the sectorsize of the mounted filesystem.\ntemp_fsid\n(RO, since 6.7)\nIndicate that this filesystem got assigned a temporary FSID at mount time,\nmaking possible to mount devices with the same FSID.\nUUID/allocations\n\nFiles and directories in\n/sys/fs/btrfs/<UUID>/allocations\ndirectory are:\nglobal_rsv_reserved\n(RO, since: 3.14)\nThe used bytes of the global reservation.\nglobal_rsv_size\n(RO, since: 3.14)\nThe total size of the global reservation.\ndata/\n,\nmetadata/\nand\nsystem/\ndirectories\n(RO, since: 5.14)\nSpace info accounting for the 3 block group types.\nUUID/allocations/{data,metadata,system}\n\nFiles in\n/sys/fs/btrfs/<UUID>/allocations/\ndata,metadata,system\ndirectory are:\nbg_reclaim_threshold\n(RW, since: 5.19)\nReclaimable space percentage of block group’s size (excluding\npermanently unusable space) to reclaim the block group.\nCan be used on regular or zoned devices.\nbytes_*\n(RO)\nValues of the corresponding data structures for the given block group\ntype and profile that are used internally and may change rapidly depending\non the load.\nComplete list: bytes_may_use, bytes_pinned, bytes_readonly,\nbytes_reserved, bytes_used, bytes_zone_unusable\nchunk_size\n(RW, since: 6.0)\nShows the chunk size. Can be changed for data and metadata (independently)\nand cannot be set for system block group type.\nCannot be set for zoned devices as it depends on the fixed device zone size.\nUpper bound is 10% of the filesystem size, the value must be multiple of 256MiB\nand greater than 0.\nsize_classes\n(RO, since: 6.3)\nNumbers of block groups of a given classes based on heuristics that\nmeasure extent length, age and fragmentation.\nnone 136\nsmall 374\nmedium 282\nlarge 93\nUUID/bdi\n\nSymlink to the sysfs directory of the backing device info (BDI), which is\nrelated to writeback process and infrastructure.\nUUID/devices\n\nFiles in\n/sys/fs/btrfs/<UUID>/devices\ndirectory are symlinks named\nafter device nodes (e.g. sda, dm-0) and pointing to their sysfs directory.\nUUID/devinfo\n\nThe directory contains subdirectories named after device ids (numeric values). Each\nsubdirectory has information about the device of the given\ndevid\n.\nUUID/devinfo/DEVID\n\nFiles in\n/sys/fs/btrfs/<UUID>/devinfo/<DEVID>\ndirectory are:\nerror_stats:\n(RO, since: 5.14)\nShows device stats of this device, same as\nbtrfs device stats\n(\nbtrfs-device(8)\n).\nwrite_errs 0\nread_errs 0\nflush_errs 0\ncorruption_errs 0\ngeneration_errs 0\nfsid:\n(RO, since: 5.17)\nShows the fsid which the device belongs to.\nIt can be different than the\nUUID\nif it’s a seed device.\nin_fs_metadata\n(RO, since: 5.6)\nShows whether we have found the device.\nShould always be 1, as if this turns to 0, the\nDEVID\ndirectory\nwould get removed automatically.\nmissing\n(RO, since: 5.6)\nShows whether the device is considered missing by the kernel module.\nreplace_target\n(RO, since: 5.6)\nShows whether the device is the replace target.\nIf no device replace is running, this value is 0.\nscrub_speed_max\n(RW, since: 5.14)\nShows the scrub speed limit for this device. The unit is Bytes/s.\n0 means no limit. The value can be set but is not persistent.\nwriteable\n(RO, since: 5.6)\nShow if the device is writeable.\nUUID/qgroups\n\nFiles in\n/sys/fs/btrfs/<UUID>/qgroups/\ndirectory are:\nenabled\n(RO, since: 6.1)\nShows if qgroup is enabled.\nAlso, if qgroup is disabled, the\nqgroups\ndirectory will\nbe removed automatically.\ninconsistent\n(RO, since: 6.1)\nShows if the qgroup numbers are inconsistent.\nIf 1, it’s recommended to do a qgroup rescan.\ndrop_subtree_threshold\n(RW, since: 6.1)\nShows the subtree drop threshold to automatically mark qgroup inconsistent.\nWhen dropping large subvolumes with qgroup enabled, there would be a huge\nload for qgroup accounting.\nIf we have a subtree whose level is larger than or equal to this value,\nwe will not trigger qgroup account at all, but mark qgroup inconsistent to\navoid the huge workload.\nDefault value is 3, which means that trees of low height will be accounted\nproperly as this is sufficiently fast. The value was 8 until 6.13 where\nno subtree drop can trigger qgroup rescan making it less useful.\nLower value can reduce qgroup workload, at the cost of extra qgroup rescan\nto re-calculate the numbers.\nUUID/qgroups/LEVEL_ID\n\nFiles in each\n/sys/fs/btrfs/<UUID>/qgroups/<LEVEL>_<ID>/\ndirectory are:\nexclusive\n(RO, since: 5.9)\nShows the exclusively owned bytes of the qgroup.\nlimit_flags\n(RO, since: 5.9)\nShows the numeric value of the limit flags.\nIf 0, means no limit implied.\nmax_exclusive\n(RO, since: 5.9)\nShows the limits on exclusively owned bytes.\nmax_referenced\n(RO, since: 5.9)\nShows the limits on referenced bytes.\nreferenced\n(RO, since: 5.9)\nShows the referenced bytes of the qgroup.\nrsv_data\n(RO, since: 5.9)\nShows the reserved bytes for data.\nrsv_meta_pertrans\n(RO, since: 5.9)\nShows the reserved bytes for per transaction metadata.\nrsv_meta_prealloc\n(RO, since: 5.9)\nShows the reserved bytes for preallocated metadata.\nUUID/discard\n\nFiles in\n/sys/fs/btrfs/<UUID>/discard/\ndirectory are:\ndiscardable_bytes\n(RO, since: 6.1)\nShows amount of bytes that can be discarded in the async discard and\nnodiscard mode.\ndiscardable_extents\n(RO, since: 6.1)\nShows number of extents to be discarded in the async discard and\nnodiscard mode.\ndiscard_bitmap_bytes\n(RO, since: 6.1)\nShows amount of discarded bytes from data tracked as bitmaps.\ndiscard_extent_bytes\n(RO, since: 6.1)\nShows amount of discarded extents from data tracked as bitmaps.\ndiscard_bytes_saved\n(RO, since: 6.1)\nShows the amount of bytes that were reallocated without being discarded.\nkbps_limit\n(RW, since: 6.1)\nTunable limit of kilobytes per second issued as discard IO in the async\ndiscard mode.\niops_limit\n(RW, since: 6.1)\nTunable limit of number of discard IO operations to be issued in the\nasync discard mode.\nmax_discard_size\n(RW, since: 6.1)\nTunable limit for size of one IO discard request.\nFILESYSTEM EXCLUSIVE OPERATIONS\n\nThere are several operations that affect the whole filesystem and cannot be run\nin parallel. Attempt to start one while another is running will fail (see\nexceptions below).\nSince kernel 5.10 the currently running operation can be obtained from\n/sys/fs/UUID/exclusive_operation\nwith following values and operations:\nbalance\nbalance paused (since 5.17)\ndevice add\ndevice delete\ndevice replace\nresize\nswapfile activate\nnone\nEnqueuing is supported for several btrfs subcommands so they can be started\nat once and then serialized.\nThere’s an exception when a paused balance allows to start a device add\noperation as they don’t really collide and this can be used to add more space\nfor the balance to finish.\nFILESYSTEM LIMITS\n\nmaximum file name length\n255\nThis limit is imposed by Linux VFS, the structures of BTRFS could store\nlarger file names.\nmaximum symlink target length\ndepends on the\nnodesize\nvalue, for 4KiB it’s 3949 bytes, for larger nodesize\nit’s 4095 due to the system limit PATH_MAX\nThe symlink target may not be a valid path, i.e. the path name components\ncan exceed the limits (NAME_MAX), there’s no content validation at\nsymlink(3)\ncreation.\nmaximum number of inodes\n2\n64\nbut depends on the available metadata space as the inodes are created\ndynamically\nEach subvolume is an independent namespace of inodes and thus their\nnumbers, so the limit is per subvolume, not for the whole filesystem.\ninode numbers\nminimum number: 256 (for subvolumes), regular files and directories: 257,\nmaximum number: (2\n64\n- 256)\nThe inode numbers that can be assigned to user created files are from\nthe whole 64bit space except first 256 and last 256 in that range that\nare reserved for internal b-tree identifiers.\nmaximum file length\ninherent limit of BTRFS is 2\n64\n(16 EiB) but the practical\nlimit of Linux VFS is 2\n63\n(8 EiB)\nmaximum number of subvolumes\nthe subvolume ids can go up to 2\n48\nbut the number of actual subvolumes\ndepends on the available metadata space\nThe space consumed by all subvolume metadata includes bookkeeping of\nshared extents can be large (MiB, GiB). The range is not the full 64bit\nrange because of qgroups that use the upper 16 bits for another\npurposes.\nmaximum number of hardlinks of a file in a directory\n65536 when the\nextref\nfeature is turned on during mkfs (default), roughly\n100 otherwise and depends on file name length that fits into one metadata node\nminimum filesystem size\nthe minimal size of each device depends on the\nmixed-bg\nfeature, without that\n(the default) it’s about 109MiB, with mixed-bg it’s is 16MiB\nBOOTLOADER SUPPORT\n\nGRUB2 (\nhttps://www.gnu.org/software/grub\n) has the most advanced support of\nbooting from BTRFS with respect to features.\nU-Boot (\nhttps://www.denx.de/wiki/U-Boot/\n) has decent support for booting but\nnot all BTRFS features are implemented, check the documentation.\nIn general, the first 1MiB on each device is unused with the exception of\nprimary superblock that is on the offset 64KiB and spans 4KiB. The rest can be\nfreely used by bootloaders or for other system information. Note that booting\nfrom a filesystem on\nzoned device\nis not supported.\nFILE ATTRIBUTES\n\nThe btrfs filesystem supports setting file attributes or flags. Note there are\nold and new interfaces, with confusing names. The following list should clarify\nthat:\nattributes\n:\nchattr(1)\nor\nlsattr(1)\nutilities (the ioctls are\nFS_IOC_GETFLAGS and FS_IOC_SETFLAGS), due to the ioctl names the attributes\nare also called flags\nxflags\n: to distinguish from the previous, it’s extended flags, with tunable\nbits similar to the attributes but extensible and new bits will be added in\nthe future (the ioctls are FS_IOC_FSGETXATTR and FS_IOC_FSSETXATTR but they\nare not related to extended attributes that are also called xattrs), there’s\nno standard tool to change the bits, there’s support in\nxfs_io(8)\nas\ncommand\nxfs_io -c chattr\nAttributes have constraints associated and not all combinations can be set, the\norder of setting them also matters. Most attributes apply to files and\ndirectories but the semantics may differ. For directories the attribute may\nonly mean to set this attribute to all new files (\ninheritable\nin the list\nbelow). Some attributes need root privileges to be set.\nAttributes\n\na\n(file, dir, root)\nappend only\n, new writes are always written at the end of the file\nA\n(file, dir)\nno atime updates\nc\n(file, dir, inherited)\ncompress data\n, all data written after this attribute is set will be compressed.\nPlease note that compression is also affected by the mount options or the parent\ndirectory attributes.\nWhen set on a directory, all newly created files will inherit this attribute.\nThis attribute cannot be set with ‘m’ at the same time.\nC\n(file, dir, inherited)\nno copy-on-write\n, file data modifications are done in-place\nWhen set on a directory, all newly created files will inherit this attribute.\nNote\nDue to implementation limitations, this flag can be set/unset only on\nempty files.\nd\n(file)\nno dump\n, makes sense with 3rd party tools like\ndump(8)\n, on BTRFS the\nattribute can be set/unset but no other special handling is done\nD\n(dir)\nsynchronous directory updates\n, for more details search\nopen(2)\nfor\nO_SYNC\nand\nO_DSYNC\ni\n(file, dir, root)\nimmutable\n, no file data and metadata changes allowed even to the root user as\nlong as this attribute is set (obviously the exception is unsetting the attribute)\nm\n(file, dir)\nno compression\n, permanently turn off compression on the given file. Any\ncompression mount options will not affect this file. (\nchattr(1)\nsupport added in\n1.46.2)\nWhen set on a directory, all newly created files will inherit this attribute.\nThis attribute cannot be set with\nc\nat the same time.\nS\n(file)\nsynchronous updates\n, for more details search\nopen(2)\nfor\nO_SYNC\nand\nO_DSYNC\nV\n(file, read-only)\nfs-verity enabled\non the file\nNo other attributes are supported.  For the complete list please refer to the\nchattr(1)\nmanual page.\nXFLAGS\n\nThere’s an overlap of letters assigned to the bits with the attributes, this list\nrefers to what\nxfs_io(8)\nprovides:\ni\nimmutable\n, same as the attribute\na\nappend only\n, same as the attribute\ns\nsynchronous updates\n, same as the attribute\nS\nA\nno atime updates\n, same as the attribute\nd\nno dump\n, same as the attribute\nZONED MODE\n\nSince version 5.12 btrfs supports so called\nzoned mode\n. This is a special\non-disk format and allocation/write strategy that’s friendly to zoned devices.\nIn short, a device is partitioned into fixed-size zones and each zone can be\nupdated by append-only manner, or reset. As btrfs has no fixed data structures,\nexcept the super blocks, the zoned mode only requires block placement that\nfollows the device constraints. You can learn about the whole architecture at\nhttps://zonedstorage.io\n.\nThe devices are also called SMR/ZBC/ZNS, in\nhost-managed\nmode. Note that\nthere are devices that appear as non-zoned but actually are, this is\ndrive-managed\nand using zoned mode won’t help.\nThe zone size depends on the device, typical sizes are 256MiB or 1GiB. In\ngeneral it must be a power of two. Emulated zoned devices like\nnull_blk\nallow\nto set various zone sizes.\nRequirements, limitations\n\nall devices must have the same zone size\nmaximum zone size is 8GiB\nminimum zone size is 4MiB\nmixing zoned and non-zoned devices is possible, the zone writes are emulated,\nbut this is namely for testing\nthe super block is handled in a special way and is at different locations than on a non-zoned filesystem:\nprimary: 0B (and the next two zones)\nsecondary: 512GiB (and the next two zones)\ntertiary: 4TiB (4096GiB, and the next two zones)\nIncompatible features\n\nThe main constraint of the zoned devices is lack of in-place update of the data.\nThis is inherently incompatible with some features:\nNODATACOW - overwrite in-place, cannot create such files\nfallocate - preallocating space for in-place first write\nmixed-bg - unordered writes to data and metadata, fixing that means using\nseparate data and metadata block groups\nbooting - the zone at offset 0 contains superblock, resetting the zone would\ndestroy the bootloader data\nInitial support lacks some features but they’re planned:\nonly single (data, metadata) and DUP (metadata) profile is supported\nfstrim - due to dependency on free space cache v1\nSuper block\n\nAs said above, super block is handled in a special way. In order to be crash\nsafe, at least one zone in a known location must contain a valid superblock.\nThis is implemented as a ring buffer in two consecutive zones, starting from\nknown offsets 0B, 512GiB and 4TiB.\nThe values are different than on non-zoned devices. Each new super block is\nappended to the end of the zone, once it’s filled, the zone is reset and writes\ncontinue to the next one. Looking up the latest super block needs to read\noffsets of both zones and determine the last written version.\nThe amount of space reserved for super block depends on the zone size. The\nsecondary and tertiary copies are at distant offsets as the capacity of the\ndevices is expected to be large, tens of terabytes. Maximum zone size supported\nis 8GiB, which would mean that e.g. offset 0-16GiB would be reserved just for\nthe super block on a hypothetical device of that zone size. This is wasteful\nbut required to guarantee crash safety.\nZone reclaim, garbage collection\n\nAs the zones are append-only, overwriting data or COW changes in metadata\nmake parts of the zones used but not connected to the filesystem structures.\nThis makes the space unusable and grows over time. Once the ratio hits a\n(configurable) threshold a background reclaim process is started and relocates\nthe remaining blocks in use to a new zone. The old one is reset and can be used\nagain.\nThis process may take some time depending on other background work or\namount of new data written. It is possible to hit an intermittent ENOSPC.\nSome devices also limit number of active zones.\nDevices\n\nReal hardware\n\nThe WD Ultrastar series 600 advertises HM-SMR, i.e. the host-managed zoned\nmode. There are two more: DA (device managed, no zoned information exported to\nthe system), HA (host aware, can be used as regular disk but zoned writes\nimprove performance). There are not many devices available at the moment, the\ninformation about exact zoned mode is hard to find, check data sheets or\ncommunity sources gathering information from real devices.\nNote: zoned mode won’t work with DM-SMR disks.\nUltrastar® DC ZN540 NVMe ZNS SSD (\nproduct\nbrief\n)\nEmulated: null_blk\n\nThe driver\nnull_blk\nprovides memory backed device and is suitable for\ntesting. There are some quirks setting up the devices. The module must be\nloaded with\nnr_devices=0\nor the numbering of device nodes will be offset. The\nconfigfs\nmust be mounted at\n/sys/kernel/config\nand the administration of\nthe null_blk devices is done in\n/sys/kernel/config/nullb\n. The device nodes\nare named like\n/dev/nullb0\nand are numbered sequentially. NOTE: the device\nname may be different than the named directory in sysfs!\nSetup:\nmodprobe\nconfigfs\nmodprobe\nnull_blk\nnr_devices\n=\n0\nCreate a device\nmydev\n, assuming no other previously created devices, size is\n2048MiB, zone size 256MiB. There are more tunable parameters, this is a minimal\nexample taking defaults:\ncd\n/sys/kernel/config/nullb/\nmkdir\nmydev\ncd\nmydev\necho\n2048\n>\nsize\necho\n1\n>\nzoned\necho\n1\n>\nmemory_backed\necho\n256\n>\nzone_size\necho\n1\n>\npower\nThis will create a device\n/dev/nullb0\nand the value of file\nindex\nwill\nmatch the ending number of the device node.\nRemove the device:\nrmdir\n/sys/kernel/config/nullb/mydev\nThen continue with\nmkfs.btrfs /dev/nullb0\n, the zoned mode is auto-detected.\nFor convenience, there’s a script wrapping the basic null_blk management operations\nhttps://github.com/kdave/nullb.git\n, the above commands become:\nnullb\nsetup\nnullb\ncreate\n-s\n2g\n-z\n256\nmkfs.btrfs\n/dev/nullb0\n...\nnullb\nrm\nnullb0\nEmulated: TCMU runner\n\nTCMU is a framework to emulate SCSI devices in userspace, providing various\nbackends for the storage, with zoned support as well. A file-backed zoned\ndevice can provide more options for larger storage and zone size. Please follow\nthe instructions at\nhttps://zonedstorage.io/projects/tcmu-runner/\n.\nCompatibility, incompatibility\n\nthe feature sets an incompat bit and requires new kernel to access the\nfilesystem (for both read and write)\nsuperblock needs to be handled in a special way, there are still 3 copies\nbut at different offsets (0, 512GiB, 4TiB) and the 2 consecutive zones are a\nring buffer of the superblocks, finding the latest one needs reading it from\nthe write pointer or do a full scan of the zones\nmixing zoned and non zoned devices is possible (zones are emulated) but is\nrecommended only for testing\nmixing zoned devices with different zone sizes is not possible\nzone sizes must be power of two, zone sizes of real devices are e.g. 256MiB\nor 1GiB, larger size is expected, maximum zone size supported by btrfs is\n8GiB\nStatus, stability, reporting bugs\n\nThe zoned mode has been released in 5.12 and there are still some rough edges\nand corner cases one can hit during testing. Please report bugs to\nhttps://github.com/naota/linux/issues/\n.\nReferences\n\nhttps://zonedstorage.io\nhttps://zonedstorage.io/projects/libzbc/\n--\nlibzbc\nis library and set\nof tools to directly manipulate devices with ZBC/ZAC support\nhttps://zonedstorage.io/projects/libzbd/\n--\nlibzbd\nuses the kernel\nprovided zoned block device interface based on the ioctl() system calls\nhttps://hddscan.com/blog/2020/hdd-wd-smr.html\n-- some details about exact device types\nhttps://lwn.net/Articles/853308/\n--\nBtrfs on zoned block devices\nhttps://www.usenix.org/conference/vault20/presentation/bjorling\n-- Zone\nAppend: A New Way of Writing to Zoned Storage\nCONTROL DEVICE\n\nThere’s a character special device\n/dev/btrfs-control\nwith major and minor\nnumbers 10 and 234 (the device can be found under the\nmisc\ncategory).\n$ ls -l /dev/btrfs-control\ncrw------- 1 root root 10, 234 Jan  1 12:00 /dev/btrfs-control\nThe device accepts some ioctl calls that can perform following actions on the\nfilesystem module:\nscan devices for btrfs filesystem (i.e. to let multi-device filesystems mount\nautomatically) and register them with the kernel module\nsimilar to scan, but also wait until the device scanning process is finished\nfor a given filesystem\nget the supported features (can be also found under\n/sys/fs/btrfs/features\n)\nThe device is created when btrfs is initialized, either as a module or a\nbuilt-in functionality and makes sense only in connection with that. Running\ne.g. mkfs without the module loaded will not register the device and will\nprobably warn about that.\nIn rare cases when the module is loaded but the device is not present (most\nlikely accidentally deleted), it’s possible to recreate it by\n# mknod --mode=600 /dev/btrfs-control c 10 234\nor (since 5.11) by a convenience command\n# btrfs rescue create-control-device\nThe control device is not strictly required but the device scanning will not\nwork and a workaround would need to be used to mount a multi-device filesystem.\nThe mount option\ndevice\ncan trigger the device scanning during mount, see\nalso\nbtrfs device scan\n.\nFILESYSTEM WITH MULTIPLE PROFILES\n\nIt is possible that a btrfs filesystem contains multiple block group profiles\nof the same type.  This could happen when a profile conversion using balance\nfilters is interrupted (see\nbtrfs-balance(8)\n).  Some\nbtrfs\ncommands perform\na test to detect this kind of condition and print a warning like this:\nWARNING: Multiple block group profiles detected, see 'man btrfs(5)'.\nWARNING:   Data: single, raid1\nWARNING:   Metadata: single, raid1\nThe corresponding output of\nbtrfs filesystem df\nmight look like:\nWARNING: Multiple block group profiles detected, see 'man btrfs(5)'.\nWARNING:   Data: single, raid1\nWARNING:   Metadata: single, raid1\nData, RAID1: total=832.00MiB, used=0.00B\nData, single: total=1.63GiB, used=0.00B\nSystem, single: total=4.00MiB, used=16.00KiB\nMetadata, single: total=8.00MiB, used=112.00KiB\nMetadata, RAID1: total=64.00MiB, used=32.00KiB\nGlobalReserve, single: total=16.25MiB, used=0.00B\nThere’s more than one line for type\nData\nand\nMetadata\n, while the profiles\nare\nsingle\nand\nRAID1\n.\nThis state of the filesystem OK but most likely needs the user/administrator to\ntake an action and finish the interrupted tasks. This cannot be easily done\nautomatically, also the user knows the expected final profiles.\nIn the example above, the filesystem started as a single device and\nsingle\nblock group profile. Then another device was added, followed by balance with\nconvert=raid1\nbut for some reason hasn’t finished. Restarting the balance\nwith\nconvert=raid1\nwill continue and end up with filesystem with all block\ngroup profiles\nRAID1\n.\nNote\nIf you’re familiar with balance filters, you can use\nconvert=raid1,profiles=single,soft\n, which will take only the unconverted\nsingle\nprofiles and convert them to\nraid1\n. This may speed up the conversion\nas it would not try to rewrite the already convert\nraid1\nprofiles.\nHaving just one profile is desired as this also clearly defines the profile of\nnewly allocated block groups, otherwise this depends on internal allocation\npolicy. When there are multiple profiles present, the order of selection is\nRAID56, RAID10, RAID1, RAID0 as long as the device number constraints are\nsatisfied.\nCommands that print the warning were chosen so they’re brought to user\nattention when the filesystem state is being changed in that regard. This is:\ndevice add\n,\ndevice delete\n,\nbalance cancel\n,\nbalance pause\n. Commands\nthat report space usage:\nfilesystem df\n,\ndevice usage\n. The command\nfilesystem usage\nprovides a line in the overall summary:\nMultiple profiles:                 yes (data, metadata)\nSEEDING DEVICE\n\nThe COW mechanism and multiple devices under one hood enable an interesting\nconcept, called a seeding device: extending a read-only filesystem on a\ndevice with another device that captures all writes. For example\nimagine an immutable golden image of an operating system enhanced with another\ndevice that allows to use the data from the golden image and normal operation.\nThis idea originated on CD-ROMs with base OS and allowing to use them for live\nsystems, but this became obsolete. There are technologies providing similar\nfunctionality, like\nunionmount\n,\noverlayfs\nor\nqcow2\nimage snapshot.\nThe seeding device starts as a normal filesystem, once the contents is ready,\nbtrfstune -S 1\nis used to flag it as a seeding device. Mounting such device\nwill not allow any writes, except adding a new device by\nbtrfs device add\n.\nThen the filesystem can be remounted as read-write.\nGiven that the filesystem on the seeding device is always recognized as\nread-only, it can be used to seed multiple filesystems from one device at the\nsame time. The UUID that is normally attached to a device is automatically\nchanged to a random UUID on each mount.\nNote\nBefore v6.17 kernel, a seed device could have been mounted\nindependently along with sprouted filesystems.\nBut since 6.17 kernel, a seed device can only be mounted either through\na sprouted filesystem, or the seed device itself, not both at the same time.\nThis is to ensure a block device to have only a single filesystem bound\nto it, so that runtime device missing events can be properly handled.\nOnce the seeding device is mounted, it needs the writable device. After adding\nit, unmounting and mounting with\numount /path; mount /dev/writable\n/path\nor remounting read-write with\nremount -o remount,rw\nmakes the\nfilesystem at\n/path\nready for use.\nNote\nThere was a known bug with using remount to make the mount writeable:\nremount will leave the filesystem in a state where it is unable to\nclean deleted snapshots, so it will leak space until it is unmounted\nand mounted properly.\nThat bug has been fixed in 5.11 and newer kernels.\nFurthermore, deleting the seeding device from the filesystem can turn it into\na normal filesystem, provided that the writable device can also contain all the\ndata from the seeding device.\nThe seeding device flag can be cleared again by\nbtrfstune -f -S 0\n, e.g.\nallowing to update with newer data but please note that this will invalidate\nall existing filesystems that use this particular seeding device. This works\nfor some use cases, not for others, and the forcing flag to the command is\nmandatory to avoid accidental mistakes.\nExample how to create and use one seeding device:\n# mkfs.btrfs /dev/sda\n# mount /dev/sda /mnt/mnt1\n...\nfill\nmnt1\nwith\ndata\n# umount /mnt/mnt1\n# btrfstune -S 1 /dev/sda\n# mount /dev/sda /mnt/mnt1\n# btrfs device add /dev/sdb /mnt/mnt1\n# umount /mnt/mnt1\n# mount /dev/sdb /mnt/mnt1\n...\n/mnt/mnt1\nis\nnow\nwritable\nNow\n/mnt/mnt1\ncan be used normally. The device\n/dev/sda\ncan be mounted\nagain with a another writable device:\n# mount /dev/sda /mnt/mnt2\n# btrfs device add /dev/sdc /mnt/mnt2\n# umount /mnt/mnt2\n# mount /dev/sdc /mnt/mnt2\n...\n/mnt/mnt2\nis\nnow\nwritable\nThe writable device (file:\n/dev/sdb\n) can be decoupled from the seeding device and\nused independently:\n# btrfs device delete /dev/sda /mnt/mnt1\nAs the contents originated in the seeding device, it’s possible to turn\n/dev/sdb\nto a seeding device again and repeat the whole process.\nA few things to note:\nit’s recommended to use only single device for the seeding device, it works\nfor multiple devices but the\nsingle\nprofile must be used in order to make\nthe seeding device deletion work\nblock group profiles\nsingle\nand\ndup\nsupport the use cases above\nthe label is copied from the seeding device and can be changed by\nbtrfs filesystem label\neach new mount of the seeding device gets a new random UUID\numount /path; mount /dev/writable /path\ncan be replaced with\nmount -o remount,rw /path\nbut it won’t reclaim space of deleted subvolumes until the seeding device\nis mounted read-write again before making it seeding again\nChained seeding devices\n\nThough it’s not recommended and is rather an obscure and untested use case,\nchaining seeding devices is possible. In the first example, the writable device\n/dev/sdb\ncan be turned onto another seeding device again, depending on the\nunchanged seeding device\n/dev/sda\n. Then using\n/dev/sdb\nas the primary\nseeding device it can be extended with another writable device, say\n/dev/sdd\n,\nand it continues as before as a simple tree structure on devices.\n# mkfs.btrfs /dev/sda\n# mount /dev/sda /mnt/mnt1\n...\nfill\nmnt1\nwith\ndata\n# umount /mnt/mnt1\n# btrfstune -S 1 /dev/sda\n# mount /dev/sda /mnt/mnt1\n# btrfs device add /dev/sdb /mnt/mnt1\n# mount -o remount,rw /mnt/mnt1\n...\n/mnt/mnt1\nis\nnow\nwritable\n# umount /mnt/mnt1\n# btrfstune -S 1 /dev/sdb\n# mount /dev/sdb /mnt/mnt1\n# btrfs device add /dev/sdc /mnt\n# mount -o remount,rw /mnt/mnt1\n...\n/mnt/mnt1\nis\nnow\nwritable\n# umount /mnt/mnt1\nAs a result we have:\nsda\nis a single seeding device, with its initial contents\nsdb\nis a seeding device but requires\nsda\n, the contents are from the time\nwhen\nsdb\nis made seeding, i.e. contents of\nsda\nwith any later changes\nsdc\nlast writable, can be made a seeding one the same way as was\nsdb\n,\npreserving its contents and depending on\nsda\nand\nsdb\nAs long as the seeding devices are unmodified and available, they can be used\nto start another branch.\nRAID56 STATUS AND RECOMMENDED PRACTICES\n\nThe RAID56 feature provides striping and parity over several devices, same as\nthe traditional RAID5/6. There are some implementation and design deficiencies\nthat make it unreliable for some corner cases and the feature\nshould not be\nused in production, only for evaluation or testing\n.  The power failure safety\nfor metadata with RAID56 is not 100%.\nMetadata\n\nDo not use\nraid5\nnor\nraid6\nfor metadata. Use\nraid1\nor\nraid1c3\nrespectively.\nThe substitute profiles provide the same guarantees against loss of 1 or 2\ndevices, and in some respect can be an improvement.  Recovering from one\nmissing device will only need to access the remaining 1st or 2nd copy, that in\ngeneral may be stored on some other devices due to the way RAID1 works on\nbtrfs, unlike on a striped profile (similar to\nraid0\n) that would need all\ndevices all the time.\nThe space allocation pattern and consumption is different (e.g. on N devices):\nfor\nraid5\nas an example, a 1GiB chunk is reserved on each device, while with\nraid1\nthere’s each 1GiB chunk stored on 2 devices. The consumption of each\n1GiB of used metadata is then\nN * 1GiB\nfor vs\n2 * 1GiB\n. Using\nraid1\nis also more convenient for balancing/converting to other profile due to lower\nrequirement on the available chunk space.\nMissing/incomplete support\n\nWhen RAID56 is on the same filesystem with different raid profiles, the space\nreporting is inaccurate, e.g.\ndf\n,\nbtrfs filesystem df\nor\nbtrfs filesystem usage\n. When there’s only a one profile per block\ngroup type (e.g. RAID5 for data) the reporting is accurate.\nWhen scrub is started on a RAID56 filesystem, it’s started on all devices that\ndegrade the performance. The workaround is to start it on each device\nseparately. Due to that the device stats may not match the actual state and\nsome errors might get reported multiple times.\nThe\nwrite hole\nproblem. An unclean shutdown could leave a partially written\nstripe in a state where the some stripe ranges and the parity are from the old\nwrites and some are new. The information which is which is not tracked. Write\njournal is not implemented. Alternatively a full read-modify-write would make\nsure that a full stripe is always written, avoiding the write hole completely,\nbut performance in that case turned out to be too bad for use.\nThe striping happens on all available devices (at the time the chunks were\nallocated), so in case a new device is added it may not be utilized\nimmediately and would require a rebalance. A fixed configured stripe width is\nnot implemented.\nGLOSSARY\n\nTerms in\nitalics\nalso appear in this glossary.\nallocator\nUsually\nallocator\nmeans the\nblock\nallocator, i.e. the logic\ninside the filesystem which decides where to place newly allocated blocks\nin order to maintain several constraints (like data locality, low\nfragmentation).\nIn btrfs, allocator may also refer to\nchunk\nallocator, i.e. the\nlogic behind placing chunks on devices.\nbalance\nAn operation that can be done to a btrfs filesystem, for example\nthrough\nbtrfs balance /path\n. A\nbalance passes all data in the filesystem through the\nallocator\nagain. It is primarily intended to rebalance the data in the filesystem\nacross the\ndevices\nwhen a device is added or removed. A balance\nwill regenerate missing copies for the redundant\nRAID\nlevels, if a\ndevice has failed. As of Linux kernel 3.3, a balance operation can be\nmade selective about which parts of the filesystem are rewritten\nusing\nfilters\n.\nbarrier\nAn instruction to the underlying hardware to ensure that everything before\nthe barrier is physically written to permanent storage before anything\nafter it. Used in btrfs’s\ncopy on write\napproach to ensure\nfilesystem consistency.\nblock\nA single physically and logically contiguous piece of storage on a\ndevice, of size e.g. 4K. In some contexts also referred to as\nsector\n,\nthough the term\nblock\nis preferred.\nblock group\nThe unit of allocation of space in btrfs. A block group is laid out on\nthe disk by the btrfs\nallocator\n, and will consist of one or more\nchunks\n, each stored on a different\ndevice\n. The number of chunks\nused in a block group will depend on its\nRAID\nlevel.\nB-tree\nThe fundamental storage data structure used in btrfs. Except for the\nsuperblocks\n, all of btrfs\nmetadata\nis stored in one of several\nB-trees on disk. B-trees store key/item pairs. While the same code is\nused to implement all of the B-trees, there are a few different\ncategories of B-tree. The name\nbtrfs\nrefers to its use of B-trees.\nbtrfsck, fsck, btrfs-check\nTool in\nbtrfs-progs\nthat checks an unmounted filesystem (\noffline\n)\nand reports on any errors in the filesystem structures it finds.  By\ndefault the tool runs in read-only mode as fixing errors is potentially\ndangerous.  See also\nscrub\n.\nbtrfs-progs\nUser mode tools to manage btrfs-specific features. Maintained at\nhttp://github.com/kdave/btrfs-progs.git\n. The main frontend to btrfs\nfeatures is the standalone tool\nbtrfs\n, although\nother tools such as\nmkfs.btrfs\nand\nbtrfstune\nare also part of\nbtrfs-progs.\nchunk\nA part of a\nblock group\n. Chunks are either 1 GiB in size (for data)\nor 256 MiB (for\nmetadata\n), depending on the overall filesystem size.\nchunk tree\nA layer that keeps information about mapping between physical and\nlogical block addresses. It’s stored within the\nsystem\ngroup.\ncleaner\nUsually referred to in context of deleted subvolumes. It’s a background\nprocess that removes the actual data once a subvolume has been deleted.\nCleaning can involve lots of IO and CPU activity depending on the\nfragmentation and amount of shared data with other subvolumes.\nThe cleaner kernel thread also processes defragmentation triggered by the\nautodefrag\nmount option, removing of empty blocks groups and some\nother finalization tasks.\ncopy-on-write, COW\nAlso known as\nCOW\n. The method that btrfs uses for modifying data.\nInstead of directly overwriting data in place, btrfs takes a copy of\nthe data, alters it, and then writes the modified data back to a\ndifferent (unused) location on the disk. It then updates the\nmetadata\nto reflect the new location of the data. In order to update the\nmetadata, the affected metadata blocks are also treated in the same\nway. In COW filesystems, files tend to fragment as they are modified.\nCopy-on-write is also used in the implementation of\nsnapshots\nand\nreflink copies\n. A copy-on-write filesystem is, in theory,\nalways\nconsistent, provided the underlying hardware supports\nbarriers\n.\ndefault subvolume\nThe\nsubvolume\nin a btrfs filesystem which is mounted when mounting\nthe filesystem without using the\nsubvol=\nmount option.\ndevice\nA Linux block device, e.g. a whole disk, partition, LVM logical volume,\nloopback device, or network block device. A btrfs filesystem can reside\non one or more devices.\ndf\nA standard Unix tool for reporting the amount of space used and free in\na filesystem. The standard tool does not give accurate results, but the\nbtrfs\ncommand from\nbtrfs-progs\nhas\nan implementation of\ndf\nwhich shows space available in more detail. See\nthe\n[[FAQ#Why_does_df_show_incorrect_free_space_for_my_RAID_volume.3F|FAQ]]\nfor a more detailed explanation of btrfs free space accounting.\nDUP\nA form of “\nRAID\n” which stores two copies of each piece of data on\nthe same\ndevice\n. This is similar to\nRAID1\n, and protects\nagainst\nblock\n-level errors on the device, but does not provide any\nguarantees if the entire device fails. By default, btrfs uses\nDUP\nprofile for metadata on single device filesystem.s\nENOSPC\nError code returned by the OS to a user program when the filesystem\ncannot allocate enough data to fulfill the user request. In most\nfilesystems, it indicates there is no free space available in the\nfilesystem. Due to the additional space requirements from btrfs’s\nCOW\nbehaviour, btrfs can sometimes return ENOSPC when there is\napparently (in terms of\ndf\n) a large amount of space free. This is\neffectively a bug in btrfs, and (if it is repeatable), using the mount\noption\nenospc_debug\nmay give a report\nthat will help the btrfs developers. See the\n[[FAQ#if_your_device_is_large_.28.3E16GiB.29|FAQ entry]] on free space.\nextent\nContiguous sequence of bytes on disk that holds file data. It’s a compact\nrepresentation that tracks the start and length of the byte range, so the\nlogic behind allocating blocks (\ndelayed allocation\n) strives for\nmaximizing the length before writing the extents to the devices.\nextent buffer\nAn abstraction of a\nb-tree\nmetadata block storing item keys and item\ndata. The underlying related structures are physical device block and a\nCPU memory page.\nfallocate\nCommand line tool in util-linux, and a syscall, that reserves space in\nthe filesystem for a file, without actually writing any file data to\nthe filesystem. First data write will turn the preallocated extents\ninto regular ones. See\nfallocate(1)\nand\nfallocate(2)\nmanual pages\nfor more details.\nfilefrag\nA tool to show the number of extents in a file, and hence the amount of\nfragmentation in the file. It is usually part of the e2fsprogs package\non most Linux distributions. While initially developed for the ext2\nfilesystem, it works on Btrfs as well. It uses the\nFIEMAP\nioctl.\nfree space cache\nAlso known as “space cache v1”. A separate cache tracking free space as\nbtrfs only tracks the allocated space. The free space is by definition\nany hole between allocated ranges. Finding the free ranges can be\nI/O intensive so the cache stores a condensed representation of it.\nIt is updated every\ntransaction\ncommit.\nThe v1 free space cache has been superseded by free space tree.\nfree space tree\nSuccessor of\nfree space cache\n, also known as “space cache v2” and now\ndefault. The free space is tracked in a better way and using COW\nunlike a custom mechanism of v1.\nfsync\nOn Unix and Unix-like operating systems (of which Linux is the latter),\nthe\nfsync(2)\nsystem call causes all buffered file\ndescriptor related data changes to be flushed to the underlying block\ndevice. When a file is modified on a modern operating system the\nchanges are generally not written to the disk immediately but rather\nthose changes are buffered in memory for performance reasons,\ncalling\nfsync(2)\ncauses any in-memory changes to be written\nto disk.\ngeneration\nAn internal counter which updates for each\ntransaction\n. When a\nmetadata\nblock is written (using\ncopy on write\n), current\ngeneration is stored in the block, so that blocks which are too new\n(and hence possibly inconsistent) can be identified.\nkey\nA fixed sized tuple used to identify and sort items in a\nB-tree\n.\nThe key is broken up into 3 parts:\nobjectid\n,\ntype\n, and\noffset\n. The\ntype\nfield indicates how each of the other two\nfields should be used, and what to expect to find in the item.\nitem\nA variable sized structure stored in B-tree leaves. Items hold\ndifferent types of data depending on key type.\nlog tree\nA b-tree that temporarily tracks ongoing metadata updates until a full\ntransaction commit is done. It’s a performance optimization of\nfsync\n. The log tracked in the tree are replayed if the filesystem\nis not unmounted cleanly.\nmetadata\nData about data. In btrfs, this includes all of the internal data\nstructures of the filesystem, including directory structures,\nfilenames, file permissions, checksums, and the location of each file’s\nextents\n. All btrfs metadata is stored in\nB-trees\n.\nmkfs.btrfs\nThe tool (from\nbtrfs-progs\n) to create a btrfs filesystem.\noffline\nA filesystem which is not mounted is offline. Some tools (e.g.\nbtrfsck\n) will only work on offline filesystems. Compare\nonline\n.\nonline\nA filesystem which is mounted is online. Most btrfs tools will only\nwork on online filesystems. Compare\noffline\n.\norphan\nA file that’s still in use (opened by a running process) but all\ndirectory entries of that file have been removed.\nRAID\nA class of different methods for writing some additional redundant data\nacross multiple\ndevices\nso that if one device fails, the missing\ndata can be reconstructed from the remaining ones. See\nRAID0\n,\nRAID1\n,\nRAID5\n,\nRAID6\n,\nRAID10\n,\nDUP\nand\nsingle\n. Traditional RAID methods operate across multiple devices of\nequal size, whereas btrfs’ RAID implementation works inside\nblock\ngroups\n.\nRAID0\nA form of\nRAID\nwhich provides no guarantees of error recovery, but\nstripes a single copy of data across multiple devices for performance\npurposes. The stripe size is fixed to 64KB for now.\nRAID1, RAID1C3, RAID1C4\nA form of\nRAID\nwhich stores two/three/four complete copies of each\npiece of data. Each copy is stored on a different\ndevice\n. btrfs\nrequires a minimum of two devices to use RAID-1 or three/four respectively.\nThis is the default block group profile for btrfs’s\nmetadata\non more\nthan one device.\nRAID5\nA form of\nRAID\nwhich stripes a single copy of data across multiple\ndevices\n, including one device’s worth of additional parity data.\nCan be used to recover from a single device failure.\nRAID6\nA form of\nRAID\nwhich stripes a single copy of data across multiple\ndevices\n, including two device’s worth of additional parity data. Can\nbe used to recover from the failure of two devices.\nRAID10\nA form of\nRAID\nwhich stores two complete copies of each piece of\ndata, and also stripes each copy across multiple devices for\nperformance.\nreflink\nCommonly used as a reference to a shallow copy of file extents that share\nthe extents until the first change. Reflinked files (e.g. by the\ncp\n)\nare different files but point to the same extents, any change will be\ndetected and new copy of the data created, keeping the files independent.\nRelated to that is extent range cloning, that works on a range of a file.\nrelocation\nThe process of moving block groups within the filesystem while\nmaintaining full filesystem integrity and consistency. This\nfunctionality is underlying\nbalance\nand\ndevice\nremoving features.\nscrub\nAn\nonline\nfilesystem checking tool. Reads all the data and metadata\non the filesystem, verifies\nchecksums\nand eventually uses redundant\ncopies from\nRAID\nor\nDUP\nrepair any corrupt data/metadata.\nseed device\nA readonly device can be used as a filesystem seed or template (e.g. a\nCD-ROM containing an OS image). Read/write devices can be added to\nstore modifications (using\ncopy on write\n), changes to the writable\ndevices are persistent across reboots. The original device remains\nunchanged and can be removed at any time (after Btrfs has been\ninstructed to copy over all missing blocks). Multiple read/write file\nsystems can be built from the same seed.\nsingle\nA block group profile storing a single copy of each piece of data.\nsnapshot\nA\nsubvolume\nwhich is a\ncopy on write\ncopy of another subvolume. The\ntwo subvolumes share all of their common (unmodified) data, which means\nthat snapshots can be used to keep the historical state of a filesystem\nvery cheaply. After the snapshot is made, the original subvolume and\nthe snapshot are of equal status: the original does not “own” the\nsnapshot, and either one can be deleted without affecting the other\none.\nsubvolume\nA tree of files and directories inside a btrfs that can be mounted as\nif it were an independent filesystem. A subvolume is created by taking\na reference on the root of another subvolume. Each btrfs filesystem has\nat least one subvolume, the\ntop-level subvolume\n, which contains\neverything else in the filesystem. Additional subvolumes can be created\nand deleted with the\nbtrfs<\ntool. All subvolumes share the same pool\nof free space in the filesystem. See also\ndefault subvolume\n.\nsuper block\nA special metadata block that is a main access point of the filesystem\nstructures. It’s size is fixed and there are fixed locations on the devices\nused for detecting and opening the filesystem. Updating the superblock\ndefines one\ntransaction\n. The super blocks contains filesystem\nidentification (UUID), checksum type, block pointers to fundamental\ntrees, features and creation parameters.\nsystem array\nA technical term for\nsuper block\nmetadata describing how to assemble a\nfilesystem from multiple device, storing information about chunks and devices that are\nrequired to be scanned/registered at the time the mount happens.\nScanning is done by command\nbtrfs device scan\n, alternatively\nall the required devices can be specified by a mount option\ndevice=/path\n.\ntop-level subvolume\nThe\nsubvolume\nat the very top of the filesystem. This is the only\nsubvolume present in a newly-created btrfs filesystem, and internally has ID 5,\notherwise could be referenced as 0 (e.g. within the\nset-default\nsubcommand of\nbtrfs\n).\ntransaction\nA consistent set of changes. To avoid generating very large amounts of\ndisk activity, btrfs caches changes in RAM for up to 30 seconds\n(sometimes more often if the filesystem is running short on space or\ndoing a lot of\nfsync*s), and then writes (commits) these changes out\nto disk in one go (using *copy on write\nbehaviour). This period of\ncaching is called a transaction. Only one transaction is active on the\nfilesystem at any one time.\ntransid\nAn alternative term for\ngeneration\n.\nwriteback\nWriteback\nin the context of the Linux kernel can be defined as the\nprocess of writing “dirty” memory from the page cache to the disk,\nwhen certain conditions are met (timeout, number of dirty pages over a\nratio).\nSTORAGE MODEL, HARDWARE CONSIDERATIONS\n\nStorage model\n\nA storage model is a model that captures key physical aspects of data\nstructure in a data store. A filesystem is the logical structure organizing\ndata on top of the storage device.\nThe filesystem assumes several features or limitations of the storage device\nand utilizes them or applies measures to guarantee reliability. BTRFS in\nparticular is based on a COW (copy on write) mode of writing, i.e. not updating\ndata in place but rather writing a new copy to a different location and then\natomically switching the pointers.\nIn an ideal world, the device does what it promises. The filesystem assumes\nthat this may not be true so additional mechanisms are applied to either detect\nmisbehaving hardware or get valid data by other means. The devices may (and do)\napply their own detection and repair mechanisms but we won’t assume any.\nThe following assumptions about storage devices are considered (sorted by\nimportance, numbers are for further reference):\natomicity of reads and writes of blocks/sectors (the smallest unit of data\nthe device presents to the upper layers)\nthere’s a flush command that instructs the device to forcibly order writes\nbefore and after the command; alternatively there’s a barrier command that\nfacilitates the ordering but may not flush the data\ndata sent to write to a given device offset will be written without further\nchanges to the data and to the offset\nwrites can be reordered by the device, unless explicitly serialized by the\nflush command\nreads and writes can be freely reordered and interleaved\nThe consistency model of BTRFS builds on these assumptions. The logical data\nupdates are grouped, into a generation, written on the device, serialized by\nthe flush command and then the super block is written ending the generation.\nAll logical links among metadata comprising a consistent view of the data may\nnot cross the generation boundary.\nWhen things go wrong\n\nNo or partial atomicity of block reads/writes (1)\nProblem\n: a partial block contents is written (\ntorn write\n), e.g. due to a\npower glitch or other electronics failure during the read/write\nDetection\n: checksum mismatch on read\nRepair\n: use another copy or rebuild from multiple blocks using some encoding\nscheme\nThe flush command does not flush (2)\nThis is perhaps the most serious problem and impossible to mitigate by\nfilesystem without limitations and design restrictions. What could happen in\nthe worst case is that writes from one generation bleed to another one, while\nstill letting the filesystem consider the generations isolated. Crash at any\npoint would leave data on the device in an inconsistent state without any hint\nwhat exactly got written, what is missing and leading to stale metadata link\ninformation.\nDevices usually honor the flush command, but for performance reasons may do\ninternal caching, where the flushed data are not yet persistently stored. A\npower failure could lead to a similar scenario as above, although it’s less\nlikely that later writes would be written before the cached ones. This is\nbeyond what a filesystem can take into account. Devices or controllers are\nusually equipped with batteries or capacitors to write the cache contents even\nafter power is cut. (\nBattery backed write cache\n)\nData get silently changed on write (3)\nSuch thing should not happen frequently, but still can happen spuriously due\nthe complex internal workings of devices or physical effects of the storage\nmedia itself.\nProblem\n: while the data are written atomically, the contents get changed\nDetection\n: checksum mismatch on read\nRepair\n: use another copy or rebuild from multiple blocks using some\nencoding scheme\nData get silently written to another offset (3)\nThis would be another serious problem as the filesystem has no information\nwhen it happens. For that reason the measures have to be done ahead of time.\nThis problem is also commonly called\nghost write\n.\nThe metadata blocks have the checksum embedded in the blocks, so a correct\natomic write would not corrupt the checksum. It’s likely that after reading\nsuch block the data inside would not be consistent with the rest. To rule that\nout there’s embedded block number in the metadata block. It’s the logical\nblock number because this is what the logical structure expects and verifies.\nThe following is based on information publicly available, user feedback,\ncommunity discussions or bug report analyses. It’s not complete and further\nresearch is encouraged when in doubt.\nMain memory\n\nThe data structures and raw data blocks are temporarily stored in computer\nmemory before they get written to the device. It is critical that memory is\nreliable because even simple bit flips can have vast consequences and lead to\ndamaged structures, not only in the filesystem but in the whole operating\nsystem.\nBased on experience in the community, memory bit flips are more common than one\nwould think. When it happens, it’s reported by the tree-checker or by a checksum\nmismatch after reading blocks. There are some very obvious instances of bit\nflips that happen, e.g. in an ordered sequence of keys in metadata blocks. We can\neasily infer from the other data what values get damaged and how. However, fixing\nthat is not straightforward and would require cross-referencing data from the\nentire filesystem to see the scope.\nIf available, ECC memory should lower the chances of bit flips, but this\ntype of memory is not available in all cases. A memory test should be performed\nin case there’s a visible bit flip pattern, though this may not detect a faulty\nmemory module because the actual load of the system could be the factor making\nthe problems appear. In recent years attacks on how the memory modules operate\nhave been demonstrated (\nrowhammer\n) achieving specific bits to be flipped.\nWhile these were targeted, this shows that a series of reads or writes can\naffect unrelated parts of memory.\nBlock group profiles with redundancy (like RAID1) will not protect against\nmemory errors as the blocks are first stored in memory before they are written\nto the devices from the same source.\nA filesystem mounted read-only will not affect the underlying block device in\nalmost 100% (with highly unlikely exceptions). The exception is a tree-log that\nneeds to be replayed during mount (and before the read-only mount takes place),\nworking memory is needed for that and that can be affected by bit flips.\nThere’s a theoretical case where bit flip changes the filesystem status from\nread-only to read-write.\nFurther reading:\nhttps://en.wikipedia.org/wiki/Row_hammer\nmemory overclocking, XMP, potential risks\nWhat to do:\nrun\nmemtest\n, note that sometimes memory errors happen only when the system\nis under heavy load that the default memtest cannot trigger\nmemory errors may appear as filesystem going read-only due to “pre write”\ncheck, that verify meta data before they get written but fail some basic\nconsistency checks\nnewly built systems should be tested before being put to production use,\nideally start a IO/CPU load that will be run on such system later; namely\nsystems that will utilize overclocking or special performance features\nDirect memory access (DMA)\n\nAnother class of errors is related to DMA (direct memory access) performed\nby device drivers. While this could be considered a software error, the\ndata transfers that happen without CPU assistance may accidentally corrupt\nother pages. Storage devices utilize DMA for performance reasons, the\nfilesystem structures and data pages are passed back and forth, making\nerrors possible in case page life time is not properly tracked.\nThere are lots of quirks (device-specific workarounds) in Linux kernel\ndrivers (regarding not only DMA) that are added when found. The quirks\nmay avoid specific errors or disable some features to avoid worse problems.\nWhat to do:\nuse up-to-date kernel (recent releases or maintained long term support versions)\nas this may be caused by faulty drivers, keep the systems up-to-date\nRotational disks (HDD)\n\nRotational HDDs typically fail at the level of individual sectors or small clusters.\nRead failures are caught on the levels below the filesystem and are returned to\nthe user as\nEIO - Input/output error\n. Reading the blocks repeatedly may\nreturn the data eventually, but this is better done by specialized tools and\nfilesystem takes the result of the lower layers. Rewriting the sectors may\ntrigger internal remapping but this inevitably leads to data loss.\nDisk firmware is technically software but from the filesystem perspective is\npart of the hardware. IO requests are processed, and caching or various\nother optimizations are performed, which may lead to bugs under high load or\nunexpected physical conditions or unsupported use cases.\nDisks are connected by cables with two ends, both of which can cause problems\nwhen not attached properly. Data transfers are protected by checksums and the\nlower layers try hard to transfer the data correctly or not at all. The errors\nfrom badly-connecting cables may manifest as large amount of failed read or\nwrite requests, or as short error bursts depending on physical conditions.\nWhat to do:\ncheck\nsmartctl\nfor potential issues\nSolid state drives (SSD)\n\nThe mechanism of information storage is different from HDDs and this affects\nthe failure mode as well. The data are stored in cells grouped in large blocks\nwith limited number of resets and other write constraints. The firmware tries\nto avoid unnecessary resets and performs optimizations to maximize the storage\nmedia lifetime. The known techniques are deduplication (blocks with same\nfingerprint/hash are mapped to same physical block), compression or internal\nremapping and garbage collection of used memory cells. Due to the additional\nprocessing there are measures to verify the data e.g. by ECC codes.\nThe observations of failing SSDs show that the whole electronic fails at once\nor affects a lot of data (e.g. stored on one chip). Recovering such data\nmay need specialized equipment and reading data repeatedly does not help as\nit’s possible with HDDs.\nThere are several technologies of the memory cells with different\ncharacteristics and price. The lifetime is directly affected by the type and\nfrequency of data written.  Writing “too much” distinct data (e.g. encrypted)\nmay render the internal deduplication ineffective and lead to a lot of rewrites\nand increased wear of the memory cells.\nThere are several technologies and manufacturers so it’s hard to describe them\nbut there are some that exhibit similar behaviour:\nexpensive SSD will use more durable memory cells and is optimized for\nreliability and high load\ncheap SSD is projected for a lower load (“desktop user”) and is optimized for\ncost, it may employ the optimizations and/or extended error reporting\npartially or not at all\nIt’s not possible to reliably determine the expected lifetime of an SSD due to\nlack of information about how it works or due to lack of reliable stats provided\nby the device.\nMetadata writes tend to be the biggest component of lifetime writes to a SSD,\nso there is some value in reducing them. Depending on the device class (high\nend/low end) the features like DUP block group profiles may affect the\nreliability in both ways:\nhigh end\nare typically more reliable and using\nsingle\nfor data and\nmetadata could be suitable to reduce device wear\nlow end\ncould lack ability to identify errors so an additional redundancy\nat the filesystem level (checksums,\nDUP\n) could help\nOnly users who consume 50 to 100% of the SSD’s actual lifetime writes need to be\nconcerned by the write amplification of btrfs DUP metadata. Most users will be\nfar below 50% of the actual lifetime, or will write the drive to death and\ndiscover how many writes 100% of the actual lifetime was. SSD firmware often\nadds its own write multipliers that can be arbitrary and unpredictable and\ndependent on application behavior, and these will typically have far greater\neffect on SSD lifespan than DUP metadata. It’s more or less impossible to\npredict when a SSD will run out of lifetime writes to within a factor of two, so\nit’s hard to justify wear reduction as a benefit.\nFurther reading:\nhttps://www.snia.org/educational-library/ssd-and-deduplication-end-spinning-disk-2012\nhttps://www.snia.org/educational-library/realities-solid-state-storage-2013-2013\nhttps://www.snia.org/educational-library/ssd-performance-primer-2013\nhttps://www.snia.org/educational-library/how-controllers-maximize-ssd-life-2013\nWhat to do:\nrun\nsmartctl\nor self-tests to look for potential issues\nkeep the firmware up-to-date\nNVM express, non-volatile memory (NVMe)\n\nNVMe is a type of persistent memory usually connected over a system bus (PCIe)\nor similar interface and the speeds are an order of magnitude faster than SSD.\nIt is also a non-rotating type of storage, and is not typically connected by a\ncable. It’s not a SCSI type device either but rather a complete specification\nfor logical device interface.\nIn a way the errors could be compared to a combination of SSD class and regular\nmemory. Errors may exhibit as random bit flips or IO failures. There are tools\nto access the internal log (\nnvme log\nand\nnvme-cli\n) for a more detailed\nanalysis.\nThere are separate error detection and correction steps performed e.g. on the\nbus level and in most cases never making in to the filesystem level. Once this\nhappens it could mean there’s some systematic error like overheating or bad\nphysical connection of the device. You may want to run self-tests (using\nsmartctl\n).\nhttps://en.wikipedia.org/wiki/NVM_Express\nhttps://www.smartmontools.org/wiki/NVMe_Support\nDrive firmware\n\nFirmware is technically still software but embedded into the hardware. As all\nsoftware has bugs, so does firmware. Storage devices can update the firmware\nand fix known bugs. In some cases the it’s possible to avoid certain bugs by\nquirks (device-specific workarounds) in Linux kernel.\nA faulty firmware can cause wide range of corruptions from small and localized\nto large affecting lots of data. Self-repair capabilities may not be sufficient.\nWhat to do:\ncheck for firmware updates in case there are known problems, note that\nupdating firmware can be risky on itself\nuse up-to-date kernel (recent releases or maintained long term support versions)\nSD flash cards\n\nThere are a lot of devices with low power consumption and thus using storage\nmedia based on low power consumption too, typically flash memory stored on\na chip enclosed in a detachable card package. An improperly inserted card may be\ndamaged by electrical spikes when the device is turned on or off. The chips\nstoring data in turn may be damaged permanently. All types of flash memory\nhave a limited number of rewrites, so the data are internally translated by FTL\n(flash translation layer). This is implemented in firmware (technically a\nsoftware) and prone to bugs that manifest as hardware errors.\nAdding redundancy like using DUP profiles for both data and metadata can help\nin some cases but a full backup might be the best option once problems appear\nand replacing the card could be required as well.\nHardware as the main source of filesystem corruptions\n\nIf you use unreliable hardware and don’t know about that, don’t blame the\nfilesystem when it tells you.\nSEE ALSO\n\nacl(5)\n,\nbtrfs(8)\n,\nchattr(1)\n,\nfstrim(8)\n,\nioctl(2)\n,\nbtrfs-ioctl(2)\n,\nmkfs.btrfs(8)\n,\nmount(8)\n,\nswapon(8)", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://btrfs.readthedocs.io/en/latest/btrfs-man5.html"}}
{"text": "start [Wiki]\nskip to content\nWiki\nUser Tools\nLog In\nSite Tools\nSearch\nTools\nShow pagesource\nOld revisions\nFold/unfold all\nBacklinks\nRecent Changes\nMedia Manager\nSitemap\nLog In\n>\nRecent Changes\nMedia Manager\nSitemap\nTrace:\nâ¢\nstart\nstart\nTable of Contents\nThe XFS Linux wiki\nTalk with Us!\nDocumentation\nKernel Source\nUserspace Utilities\nTest Suite\nOBSOLETE CONTENT\nThis wiki has been archived and the content is no longer updated.\nThe XFS Linux wiki\nWelcome to the Linux XFS wiki.\nNOTE: the old wiki\nXFS.org\nis being migrated to this wiki, if you don't find here what you are looking for, please refer to the old wiki.\nTalk with Us!\nOur project uses 1990s-era communication systems:\nemail:\nsend email\n,\nsubscribe to the list\n, or\nbrowse list archives\n.\nIRC\n: #xfs on oftc\nDocumentation\nVarious parts of the XFS design and on-disk artifacts are documented in a git repository.\ngit:\nhttps://git.kernel.org/pub/scm/fs/xfs/xfs-documentation.git/\ndisk format guide:\nPDF\nKernel Source\nLatest source code that's (probably) going into upstream Linux soon.\ndevelopment git:\nhttps://git.kernel.org/pub/scm/fs/xfs/xfs-linux.git/\nUserspace Utilities\nxfsprogs are the userspace utilities that manage XFS filesystems.\nxfsprogs git:\nhttps://git.kernel.org/pub/scm/fs/xfs/xfsprogs-dev.git/\nxfsprogs release tarballs:\nhttps://www.kernel.org/pub/linux/utils/fs/xfs/xfsprogs/\nxfsdump tools create and restore backups of directory trees on XFS filesystems.\nxfsdump git:\nhttps://git.kernel.org/pub/scm/fs/xfs/xfsdump-dev.git/\nxfsdump release tarballs:\nhttps://www.kernel.org/pub/linux/utils/fs/xfs/xfsdump/\nTest Suite\n[x]fstests is the test suite for the userspace interfaces to filesystems.  Originally developed for XFS, they have grown to cover ext[2-4], btrfs, overlayfs, cifs, f2fs, nfs, ocfs2, and udf.\nxfstests git:\nhttps://git.kernel.org/pub/scm/fs/xfs/xfstests-dev.git/\nstart.txt\nÂ· Last modified: 2021/07/02 12:22 by\nAnthony Iliopoulos\nPage Tools\nShow pagesource\nOld revisions\nBacklinks\nFold/unfold all\nBack to top\nExcept where otherwise noted, content on this wiki is licensed under the following license:\nCC Attribution-Noncommercial-Share Alike 4.0 International", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://xfs.wiki.kernel.org/"}}
{"text": "Ext4 Howto - Ext4\nExt4 Howto\nFrom Ext4\nJump to:\nnavigation\n,\nsearch\nOBSOLETE CONTENT\nThis wiki has been archived and the content is no longer updated.\nFor latest ext4 documentation, see\nKernel Docs\n.\nContents\n1\nGeneral Information\n2\nEXT4 features\n2.1\nCompatibility\n2.2\nBigger File System and File Sizes\n2.3\nSub directory scalability\n2.4\nExtents\n2.5\nMultiblock allocation\n2.6\nDelayed allocation\n2.7\nFast fsck\n2.8\nJournal checksumming\n2.9\n\"No Journaling\" mode\n2.10\nOnline defragmentation\n2.11\nInode-related features\n2.12\nPersistent preallocation\n2.13\nBarriers on by default\n2.14\nExt4 code implements discard/TRIM\n3\nGetting Ext4 code\n3.1\nFor people who build their own kernel and utilities\n3.2\nFor people who are running Fedora and RHEL/CentOS\n3.3\nFor people who are running RHEL/CentOS\n3.4\nFor people who are running openSuSE\n3.5\nFor people who are running Ubuntu\n3.6\nFor people who are running Debian\n4\nCreating ext4 filesystems\n5\nBooting from an ext4 filesystem\n6\nConverting an ext3 filesystem to ext4\n7\nAcknowledgements\nGeneral Information\nExt4 was released as a functionally complete and stable filesystem in Linux 2.6.28, and it's getting included in all the modern distros (in some cases as the default fs), so if you are using a modern distro, it's possible that you already have Ext4 support and you don't need to modify your system to run Ext4.\nIt's safe to use it in production environments, but as any piece of software, it has bugs (which are more likely to be hit in the first stable versions). Any known critical bug will be quickly fixed. If you find one, you can contact the Ext4 developers at the\next4 mailing list\n. They sometimes also can be found on\nIRC\n.\nEXT4 features\nCompatibility\nAny existing Ext3 filesystem can be mounted as Ext4 without requiring any on-disk format changes.  However, it is possible to upgrade an Ext3 filesystem to take advantage of some Ext4 features by running a couple of commands in read-only mode (described in the next section).  This means that you can improve the performance, storage limits and features of your current filesystems without reformatting and/or reinstalling your OS and software environment. If you need the advantages of Ext4 on a production system, you can upgrade the filesystem. The procedure is safe and doesn't risk your data (obviously, backup of critical data is recommended, even if you aren't updating your filesystem :). Ext4 will use the new data structures only on new data, the old structures will remain untouched and it will be possible to read/modify them when needed. This means that if you convert your filesystem to Ext4 you won't be able to go back to Ext3 again.\nBigger File System and File Sizes\nCurrently, Ext3 support 16 TiB of maximum file system size and 2 TiB of maximum file size for 4 KiB block size. Ext4 adds 48-bit block addressing, so it will have 1\nEiB\n1\nof maximum file system size and 16 TiB of maximum file size for 4 KiB block size. Why 48-bit and not 64-bit? There are some limitations that would need to be fixed before making Ext4 fully 64-bit capable, which have not been addressed in Ext4, and with 4 KiB block size (2\n12\nbytes) the filesystem size limit is already 2\n60\nbytes.  With 64 KiB block size the limit is 2\n64\nbytes so there isn't really a pressing need to have full 64-bit block addresses. The Ext4 data structures have been designed in case this is ever required, so a future update to Ext4 may implement full 64-bit support at some point. 1 EiB will be enough (really :)) until that happens.\nFOOTNOTES\n[Δ]\nAn\nEiB\nor\nExbibyte\nis 2\n60\nbytes or 1,048,576 TiB.\n1EiB=1024PiB\n1PiB=1024TiB\n1TiB=1024GiB\nIn order to map blocks beyond 2^32 to a file, extents must be enabled since block maps only know about 32-bit block numbers.  As of e2fsprogs 1.42.9, this requirement is enforced by mke2fs.\nSub directory scalability\nRight now the maximum possible number of sub directories contained in a single directory in Ext3 is 32000. Ext4 doubles that limit and allows 64000 sub directories.\nExtents\nTraditional, Unix-derived, file systems, like Ext3, use a indirect block mapping scheme to keep track of each block used for the blocks corresponding to the data of a file. This is inefficient for large files, especially during large file delete and truncate operations, because the mapping keeps an entry for every single block, and big files have many blocks -> huge mappings, slow to handle. Modern file systems use a different approach called \"extents\". An extent is basically a bunch of contiguous physical blocks. It basically says \"The data is in the next n blocks\". For example, a 100 MiB file can be allocated into a single extent of that size, instead of needing to create the indirect mapping for 25600 blocks (4 KiB per block). Huge files are split in several extents. Extents improve the performance and also help to reduce the fragmentation, since an extent encourages continuous layouts on the disk.\nMultiblock allocation\nWhen Ext3 needs to write new data to the disk, there's a block allocator that decides which free blocks will be used to write the data. But the Ext3 block allocator only allocates one block (4KiB) at a time. That means that if the system needs to write the 100 MiB data mentioned in the previous point, it will need to call the block allocator 25600 times (and it was just 100 MiB!). Not only this is inefficient, it doesn't allow the block allocator to optimize the allocation policy because it doesn't know how many total data is being allocated, it only knows about a single block. Ext4 uses a \"multiblock allocator\" (mballoc) which allocates many blocks in a single call, instead of a single block per call, avoiding a lot of overhead. This improves the performance, and it's particularly useful with delayed allocation and extents. This feature doesn't affect the disk format. Also, note that the Ext4 block/inode allocator has other improvements, described in details in this paper.\nDelayed allocation\nDelayed allocation\nis a performance feature (it doesn't change the disk format) found in a few modern filesystems such as XFS, ZFS, btrfs or Reiser 4, and it consists in delaying the allocation of blocks as much as possible, contrary to what traditionally filesystems (such as Ext3, reiser3, etc) do: allocate the blocks as soon as possible. For example, if a process write()s, the filesystem code will allocate immediately the blocks where the data will be placed - even if the data is not being written right now to the disk and it's going to be kept in the cache for some time. This approach has disadvantages. For example when a process is writing continually to a file that grows, successive write()s allocate blocks for the data, but they don't know if the file will keep growing. Delayed allocation, on the other hand, does not allocate the blocks immediately when the process write()s, rather, it delays the allocation of the blocks while the file is kept in cache, until it is really going to be written to the disk. This gives the block allocator the opportunity to optimize the allocation in situations where the old system couldn't. Delayed allocation plays very nicely with the two previous features mentioned, extents and multiblock allocation, because in many workloads when the file is written finally to the disk it will be allocated in extents whose block allocation is done with the mballoc allocator. The performance is much better, and the fragmentation is much improved in some workloads.\nFast fsck\nFsck is a very slow operation, especially the first step: checking all the inodes in the file system. In Ext4, at the end of each group's inode table will be stored a list of unused inodes (with a checksum, for safety), so fsck will not check those inodes. The result is that total fsck time improves from 2 to 20 times, depending on the number of used inodes (\nhttp://kerneltrap.org/Linux/Improving_fsck_Speeds_in_Ext4\n). It must be noticed that it's fsck, and not Ext4, who will build the list of unused inodes. This means that you must run fsck to get the list of unused inodes built, and only the next fsck run will be faster (you need to pass a fsck in order to convert a Ext3 filesystem to Ext4 anyway). There's also a feature that takes part in this fsck speed up - \"flexible block groups\" - that also speeds up file system operations.\nJournal checksumming\nThe journal is the most used part of the disk, making the blocks that form part of it more prone to hardware failure. And recovering from a corrupted journal can lead to massive corruption. Ext4 checksums the journal data to know if the journal blocks are failing or corrupted. But journal checksumming has a bonus: it allows one to convert the two-phase commit system of Ext3's journaling to a single phase, speeding the filesystem operation up to 20% in some cases - so reliability and performance are improved at the same time. (Note: the part of the feature that improves the performance, the asynchronous logging, is turned off by default for now, and will be enabled in future releases, when its reliability improves)\n\"No Journaling\" mode\nJournaling ensures the integrity of the filesystem by keeping a log of the ongoing disk changes. However, it is known to have a small overhead. Some people with special requirements and workloads can run without a journal and its integrity advantages. In Ext4 the journaling feature can be disabled, which provides a small performance improvement.\nOnline defragmentation\n(This feature is being developed and will be included in future releases). While delayed allocation, extents and multiblock allocation help to reduce the fragmentation, with usage filesystems can still fragment. For example: You write three files in a directory and continually on the disk. Some day you need to update the file of the middle, but the updated file has grown a bit, so there's not enough room for it. You have no option but fragment the excess of data to another place of the disk, which will cause a seek, or allocate the updated file continually in another place, far from the other two files, resulting in seeks if an application needs to read all the files on a directory (say, a file manager doing thumbnails on a directory full of images). Besides, the filesystem can only care about certain types of fragmentation, it can't know, for example, that it must keep all the boot-related files contiguous, because it doesn't know which files are boot-related. To solve this issue, Ext4 will support online defragmentation, and there's a\ne4defrag\ntool which can defragment individual files or the whole filesystem.\nInode-related features\nLarger inodes, nanosecond timestamps, fast extended attributes, inodes reservation...\nLarger inodes: Ext3 supports configurable inode sizes (via the -I mkfs parameter), but the default inode size is 128 bytes. Ext4 will default to 256 bytes. This is needed to accommodate some extra fields (like nanosecond timestamps or inode versioning), and the remaining space of the inode will be used to store extend attributes that are small enough to fit it that space. This will make the access to those attributes much faster, and improves the performance of applications that use extend attributes by a factor of 3-7 times.\nInode reservation consists in reserving several inodes when a directory is created, expecting that they will be used in the future. This improves the performance, because when new files are created in that directory they'll be able to use the reserved inodes. File creation and deletion is hence more efficient.\nNanoseconds timestamps means that inode fields like \"modified time\" will be able to use nanosecond resolution instead of the second resolution of Ext3.\nPersistent preallocation\nThis feature, available in Ext4 in the latest kernel versions, and emulated by glibc in the filesystems that don't support it, allows applications to preallocate disk space: Applications tell the filesystem to preallocate the space, and the filesystem preallocates the necessary blocks and data structures, but there's no data on it until the application really needs to write the data in the future. This is what P2P applications do in their own when they \"preallocate\" the necessary space for a download that will last hours or days, but implemented much more efficiently by the filesystem and with a generic API. This have several uses: first, to avoid applications (like P2P apps) doing it themselves inefficiently by filling a file with zeros. Second, to improve fragmentation, since the blocks will be allocated at one time, as contiguously as possible. Third, to ensure that applications has always the space they know they will need, which is important for RT-ish applications, since without preallocation the filesystem could get full in the middle of an important operation. The feature is available via the libc posix_fallocate() interface.\nBarriers on by default\nThis is an option that improves the integrity of the filesystem at the cost of some performance (you can disable it with \"mount -o barrier=0\", recommended trying it if you're benchmarking).\nFrom this LWN article\n: \"The filesystem code must, before writing the [journaling] commit record, be absolutely sure that all of the transaction's information has made it to the journal. Just doing the writes in the proper order is insufficient; contemporary drives maintain large internal caches and will reorder operations for better performance. So the filesystem must explicitly instruct the disk to get all of the journal data onto the media before writing the commit record; if the commit record gets written first, the journal may be corrupted. The kernel's block I/O subsystem makes this capability available through the use of barriers; in essence, a barrier forbids the writing of any blocks after the barrier until all blocks written before the barrier are committed to the media. By using barriers, filesystems can make sure that their on-disk structures remain consistent at all times.\"\nExt4 code implements discard/TRIM\nRequires mounting with \"discard\" flag. See\nhowto\nand\nVerifying TRIM\n.\nGetting Ext4 code\nFor the vast majority of users today, ext4 will be included as a default part of any Linux installation, since it has been included in the mainline kernel since 2009.\nFor people who build their own kernel and utilities\n1. Start with a 2.6.29 or later kernel.  It is highly recommended that you apply the latest\npatchset\n(if available) to get the latest bug fixes.  In your kernel's\n.config\nfile, enable\nEXT4_FS\n(along with\nEXT4_FS_XATTR\nand\nEXT4_FS_POSIX_ACL\nif you like).\n2.  Compile the latest version of e2fsprogs (as of this writing 1.44.5) from\nkernel.org\n.   Note that it may be desirable to install the\nmke2fs.conf\nfile that comes with the e2fsprogs sources in\n/etc/mke2fs.conf\nto get the latest features enabled by default, but if you also need to boot with an older kernel it is prudent to stick with the older\n/etc/mke2fs.conf\nfeatures and only enable new features on the\nmke2fs\ncommand-line as needed.\nFor people who are running Fedora and RHEL/CentOS\nRecent Fedora is generally very up to date with respect to ext4 code in kernelspace and userspace.\nFor people who are running RHEL/CentOS\next4 is included as the default filesystem in RHEL6, and is available in RHEL7 and later.\nFor people who are running openSuSE\nExt4\next4 is the default filesystem for openSuSE 11.2\n. Converting to ext4 from\nearlier versions\nshould be done just like on any other\ninitrd\n-based system:\nRun\ntune2fs -O extents,uninit_bg /dev/ROOT\nChange ext3 to ext4 in\n/etc/fstab\nAdd \"ext4\" to\nINITRD_MODULES\nin\n/etc/sysconfig/kernel\nRun\nsudo mkinitrd\n(takes about 2 min here, dunno why)\nRun\ne2fsck\non the ROOT fs. As this is currently mounted, you might want to change into single user mode (\ninit 1\n) and remount read-only (\nmount -o remount,ro /\n) or just reboot and the bootprocess will complain a bit and run\ne2fsck\nanyway - don't know if this is The Right Thing To Do though.\nFor people who are running Ubuntu\nUbuntu 9.04 and later include ext4 as a manual partitioning option at installation time, including support for ext4 as the root filesystem.\nFor people who are running Debian\nExt4 has been available since Debian Lenny and later releases.\nCreating ext4 filesystems\nCreating a new ext4 filesystem is very easy once you have upgraded to e2fsprogs 1.41 or later.  Simply type:\n# mke2fs -t ext4 /dev/\nDEV\nor\n# mkfs.ext4 /dev/\nDEV\nOnce the filesystem is created, it can be mounted as follows:\n# mount -t ext4 /dev/\nDEV\n/wherever\nIf you have a sufficiently new system, the \"-t ext4\" should not be needed.\nNOTE: Although very large fileystems are on ext4's feature list, current e2fsprogs currently still limits the filesystem size to 2^32 blocks (16TiB for a 4KiB block filesystem).  Allowing filesystems larger than 16T is one of the very next high-priority features to complete for ext4.\nBooting from an ext4 filesystem\nExt4 support has been\nadded\nin the 1.97 version of GRUB2\nThere's also a Google Summer of Code\nproject\n(from opensuse) which seem to have\ndeveloped\next4 support for grub legacy (0.97). Both projects—GRUB2 and the GSoC projects—seem (sadly) to be different efforts.\nThe grub package in Ubuntu 9.04 and later includes a patch to support booting from ext4 filesystems (see\nbug 314350\n).\nSyslinux 4.00 and higher (currently in beta) supports ext4 in extlinux.  See\n[1]\n.\nConverting an ext3 filesystem to ext4\nThis section is now a\nseparate page\n.\nAcknowledgements\nPortions of this article have been taken from the\nExt4 article\nfrom\nthe Kernel Newbies website\n.  The Kernel Newbies article was written by\ndiegocalleja\nand was released under\nCreative Commons Attribution-2.5-Generic license\n.\nRetrieved from \"\nhttps://ext4.wiki.kernel.org/index.php?title=Ext4_Howto&oldid=9145\n\"\nViews\nPage\nDiscussion\nView source\nHistory\nPersonal tools\nLog in / create account\nNavigation\nMain page\nRecent changes\nRandom page\nSearch\nTools\nWhat links here\nRelated changes\nSpecial pages\nPrintable version\nPermanent link\nThis page was last modified on 11 February 2019, at 23:30.\nPrivacy policy\nAbout Ext4\nDisclaimers", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://ext4.wiki.kernel.org/index.php/Ext4_Howto"}}
