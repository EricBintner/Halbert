{"text": "Quickstart Guide | Helm\nSkip to main content\nðŸŽ‰ Helm v4.0.0 is out! See the\nHelm 4 Overview\nfor details!\nVersion: 4.0.0\nOn this page\nQuickstart Guide\nThis guide covers how you can quickly get started using Helm.\nPrerequisites\nâ€‹\nThe following prerequisites are required for a successful and properly secured\nuse of Helm.\nA Kubernetes cluster\nDeciding what security configurations to apply to your installation, if any\nInstalling and configuring Helm.\nInstall Kubernetes or have access to a cluster\nâ€‹\nYou must have Kubernetes installed. For the latest release of Helm, we\nrecommend the latest stable release of Kubernetes, which in most cases is the\nsecond-latest minor release.\nYou should also have a local configured copy of\nkubectl\n.\nSee the\nHelm Version Support Policy\nfor the maximum version skew supported between Helm and Kubernetes.\nInstall Helm\nâ€‹\nDownload a binary release of the Helm client. You can use tools like\nhomebrew\n,\nor look at\nthe official releases page\n.\nFor more details, or for other options, see\nthe installation guide\n.\nInitialize a Helm Chart Repository\nâ€‹\nOnce you have Helm ready, you can add a chart repository. Check\nArtifact\nHub\nfor available Helm chart\nrepositories.\n$ helm repo add bitnami https://charts.bitnami.com/bitnami\nOnce this is installed, you will be able to list the charts you can install:\n$ helm search repo bitnami\nNAME                             \tCHART VERSION\tAPP VERSION  \tDESCRIPTION\nbitnami/bitnami-common           \t0.0.9        \t0.0.9        \tDEPRECATED Chart with custom templates used in ...\nbitnami/airflow                  \t8.0.2        \t2.0.0        \tApache Airflow is a platform to programmaticall...\nbitnami/apache                   \t8.2.3        \t2.4.46       \tChart for Apache HTTP Server\nbitnami/aspnet-core              \t1.2.3        \t3.1.9        \tASP.NET Core is an open-source framework create...\n# ... and many more\nInstall an Example Chart\nâ€‹\nTo install a chart, you can run the\nhelm install\ncommand. Helm has several\nways to find and install a chart, but the easiest is to use the\nbitnami\ncharts.\n$ helm repo update              # Make sure we get the latest list of charts\n$ helm install bitnami/mysql --generate-name\nNAME: mysql-1612624192\nLAST DEPLOYED: Sat Feb  6 16:09:56 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES: ...\nIn the example above, the\nbitnami/mysql\nchart was released, and the name of\nour new release is\nmysql-1612624192\n.\nYou get a simple idea of the features of this MySQL chart by running\nhelm show chart bitnami/mysql\n. Or you could run\nhelm show all bitnami/mysql\nto get all\ninformation about the chart.\nWhenever you install a chart, a new release is created. So one chart can be\ninstalled multiple times into the same cluster. And each can be independently\nmanaged and upgraded.\nThe\nhelm install\ncommand is a very powerful command with many capabilities. To\nlearn more about it, check out the\nUsing Helm Guide\nLearn About Releases\nâ€‹\nIt's easy to see what has been released using Helm:\n$ helm list\nNAME            \tNAMESPACE\tREVISION\tUPDATED                             \tSTATUS  \tCHART      \tAPP VERSION\nmysql-1612624192\tdefault  \t1       \t2021-02-06 16:09:56.283059 +0100 CET\tdeployed\tmysql-8.3.0\t8.0.23\nThe\nhelm list\n(or\nhelm ls\n) function will show you a list of all deployed releases.\nUninstall a Release\nâ€‹\nTo uninstall a release, use the\nhelm uninstall\ncommand:\n$ helm uninstall mysql-1612624192\nrelease \"mysql-1612624192\" uninstalled\nThis will uninstall\nmysql-1612624192\nfrom Kubernetes, which will remove all\nresources associated with the release as well as the release history.\nIf the flag\n--keep-history\nis provided, release history will be kept. You will\nbe able to request information about that release:\n$ helm status mysql-1612624192\nStatus: UNINSTALLED\n...\nBecause Helm tracks your releases even after you've uninstalled them, you can\naudit a cluster's history, and even undelete a release (with\nhelm rollback\n).\nReading the Help Text\nâ€‹\nTo learn more about the available Helm commands, use\nhelm help\nor type a\ncommand followed by the\n-h\nflag:\n$ helm get -h\nPrerequisites\nInstall Kubernetes or have access to a cluster\nInstall Helm\nInitialize a Helm Chart Repository\nInstall an Example Chart\nLearn About Releases\nUninstall a Release\nReading the Help Text", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://helm.sh/docs/intro/quickstart/"}}
{"text": "Installing Helm | Helm\nSkip to main content\nðŸŽ‰ Helm v4.0.0 is out! See the\nHelm 4 Overview\nfor details!\nVersion: 4.0.0\nOn this page\nInstalling Helm\nThis guide shows how to install the Helm CLI. Helm can be installed either from\nsource, or from pre-built binary releases.\nFrom The Helm Project\nâ€‹\nThe Helm project provides two ways to fetch and install Helm. These are the\nofficial methods to get Helm releases. In addition to that, the Helm community\nprovides methods to install Helm through different package managers.\nInstallation through those methods can be found below the official methods.\nFrom the Binary Releases\nâ€‹\nEvery\nrelease\nof Helm provides binary\nreleases for a variety of OSes. These binary versions can be manually downloaded\nand installed.\nDownload your\ndesired version\nUnpack it (\ntar -zxvf helm-v4.0.0-linux-amd64.tar.gz\n)\nFind the\nhelm\nbinary in the unpacked directory, and move it to its desired\ndestination (\nmv linux-amd64/helm /usr/local/bin/helm\n)\nFrom there, you should be able to run the client and\nadd the stable\nchart repository\n:\nhelm help\n.\nNote:\nHelm automated tests are performed for Linux AMD64 only during\nGitHub Actions builds and releases. Testing of other OSes are the responsibility of\nthe community requesting Helm for the OS in question.\nFrom Script\nâ€‹\nHelm now has an installer script that will automatically grab the latest version\nof Helm and\ninstall it\nlocally\n.\nYou can fetch that script, and then execute it locally. It's well documented so\nthat you can read through it and understand what it is doing before you run it.\n$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4\n$ chmod 700 get_helm.sh\n$ ./get_helm.sh\nYes, you can\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4 | bash\nif\nyou want to live on the edge.\nThrough Package Managers\nâ€‹\nThe Helm community provides the ability to install Helm through operating system\npackage managers. These are not supported by the Helm project and are not\nconsidered trusted 3rd parties.\nFrom Homebrew (macOS)\nâ€‹\nMembers of the Helm community have contributed a Helm formula build to Homebrew.\nThis formula is generally up to date.\nbrew install helm\n(Note: There is also a formula for emacs-helm, which is a different project.)\nFrom Chocolatey (Windows)\nâ€‹\nMembers of the Helm community have contributed a\nHelm\npackage\nbuild to\nChocolatey\n. This package is generally up to date.\nchoco install kubernetes-helm\nFrom Scoop (Windows)\nâ€‹\nMembers of the Helm community have contributed a\nHelm\npackage\nbuild to\nScoop\n. This package is generally up to date.\nscoop install helm\nFrom Winget (Windows)\nâ€‹\nMembers of the Helm community have contributed a\nHelm\npackage\nbuild to\nWinget\n. This package is generally up to date.\nwinget install Helm.Helm\nFrom Apt (Debian/Ubuntu)\nâ€‹\nMembers of the Helm community have contributed an Apt package for Debian/Ubuntu. This package is\ngenerally up to date. Thanks to\nBuildkite\nfor hosting the repo.\nsudo apt-get install curl gpg apt-transport-https --yes\ncurl -fsSL https://packages.buildkite.com/helm-linux/helm-debian/gpgkey | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null\necho \"deb [signed-by=/usr/share/keyrings/helm.gpg] https://packages.buildkite.com/helm-linux/helm-debian/any/ any main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\nFrom dnf/yum (fedora)\nâ€‹\nSince Fedora 35, Helm is available on the official repository.\nYou can install Helm by invoking:\nsudo dnf install helm\nFrom Snap\nâ€‹\nThe\nSnapcrafters\ncommunity maintains the Snap\nversion of the\nHelm package\n:\nsudo snap install helm --classic\nFrom pkg (FreeBSD)\nâ€‹\nMembers of the FreeBSD community have contributed a\nHelm\npackage\nbuild to the\nFreeBSD Ports Collection\n.\nThis package is generally up to date.\npkg install helm\nDevelopment Builds\nâ€‹\nIn addition to releases you can download or install development snapshots of\nHelm.\nFrom Canary Builds\nâ€‹\n\"Canary\" builds are versions of the Helm software that are built from the latest\nmain\nbranch. They are not official releases, and may not be stable. However,\nthey offer the opportunity to test the cutting edge features.\nCanary Helm binaries are stored at\nget.helm.sh\n. Here are\nlinks to the common builds:\nLinux AMD64\nmacOS AMD64\nExperimental Windows\nAMD64\nFrom Source (Linux, macOS)\nâ€‹\nBuilding Helm from source is slightly more work, but is the best way to go if\nyou want to test the latest Helm version.\nYou must have a working Go environment.\n$ git clone https://github.com/helm/helm.git\n$ cd helm\n$ make\nIf required, it will fetch the dependencies and cache them, and validate\nconfiguration. It will then compile\nhelm\nand place it in\nbin/helm\n.\nConclusion\nâ€‹\nIn most cases, installation is as simple as getting a pre-built\nhelm\nbinary.\nThis document covers additional cases for those who want to do more\nsophisticated things with Helm.\nOnce you have the Helm Client successfully installed, you can move on to using\nHelm to manage charts and\nadd the stable\nchart repository\n.\nFrom The Helm Project\nFrom the Binary Releases\nFrom Script\nThrough Package Managers\nFrom Homebrew (macOS)\nFrom Chocolatey (Windows)\nFrom Scoop (Windows)\nFrom Winget (Windows)\nFrom Apt (Debian/Ubuntu)\nFrom dnf/yum (fedora)\nFrom Snap\nFrom pkg (FreeBSD)\nDevelopment Builds\nFrom Canary Builds\nFrom Source (Linux, macOS)\nConclusion", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://helm.sh/docs/intro/install/"}}
{"text": "Using Helm | Helm\nSkip to main content\nðŸŽ‰ Helm v4.0.0 is out! See the\nHelm 4 Overview\nfor details!\nVersion: 4.0.0\nOn this page\nUsing Helm\nwarning\nThis page has not yet been updated for Helm 4. Some of the content might be inaccurate or not applicable to Helm 4. For more information about the Helm 4 new features, improvements, and breaking changes, see\nHelm 4 Overview\n.\nThis guide explains the basics of using Helm to manage packages on your\nKubernetes cluster. It assumes that you have already\ninstalled\nthe Helm client.\nIf you are simply interested in running a few quick commands, you may wish to\nbegin with the\nQuickstart Guide\n. This chapter\ncovers the particulars of Helm commands, and explains how to use Helm.\nThree Big Concepts\nâ€‹\nA\nChart\nis a Helm package. It contains all of the resource definitions\nnecessary to run an application, tool, or service inside of a Kubernetes\ncluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an\nApt dpkg, or a Yum RPM file.\nA\nRepository\nis the place where charts can be collected and shared. It's like\nPerl's\nCPAN archive\nor the\nFedora Package\nDatabase\n, but for Kubernetes packages.\nA\nRelease\nis an instance of a chart running in a Kubernetes cluster. One chart\ncan often be installed many times into the same cluster. And each time it is\ninstalled, a new\nrelease\nis created. Consider a MySQL chart. If you want two\ndatabases running in your cluster, you can install that chart twice. Each one\nwill have its own\nrelease\n, which will in turn have its own\nrelease name\n.\nWith these concepts in mind, we can now explain Helm like this:\nHelm installs\ncharts\ninto Kubernetes, creating a new\nrelease\nfor each\ninstallation. And to find new charts, you can search Helm chart\nrepositories\n.\n'helm search': Finding Charts\nâ€‹\nHelm comes with a powerful search command. It can be used to search two\ndifferent types of source:\nhelm search hub\nsearches\nthe Artifact Hub\n, which\nlists helm charts from dozens of different repositories.\nhelm search repo\nsearches the repositories that you have added to your local\nhelm client (with\nhelm repo add\n). This search is done over local data, and\nno public network connection is needed.\nYou can find publicly available charts by running\nhelm search hub\n:\n$ helm search hub wordpress\nURL                                                 CHART VERSION APP VERSION DESCRIPTION\nhttps://hub.helm.sh/charts/bitnami/wordpress        7.6.7         5.2.4       Web publishing platform for building blogs and ...\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.6.3        v0.6.3      Presslabs WordPress Operator Helm Chart\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.7.1        v0.7.1      A Helm chart for deploying a WordPress site on ...\nThe above searches for all\nwordpress\ncharts on Artifact Hub.\nWith no filter,\nhelm search hub\nshows you all of the available charts.\nhelm search hub\nexposes the URL to the location on\nartifacthub.io\nbut not the actual Helm repo.\nhelm search hub --list-repo-url\nexposes the actual Helm repo URL which comes in handy when you are looking to add a new repo:\nhelm repo add [NAME] [URL]\n.\nUsing\nhelm search repo\n, you can find the names of the charts in repositories\nyou have already added:\n$ helm repo add brigade https://brigadecore.github.io/charts\n\"brigade\" has been added to your repositories\n$ helm search repo brigade\nNAME                          CHART VERSION APP VERSION DESCRIPTION\nbrigade/brigade               1.3.2         v1.2.1      Brigade provides event-driven scripting of Kube...\nbrigade/brigade-github-app    0.4.1         v0.2.1      The Brigade GitHub App, an advanced gateway for...\nbrigade/brigade-github-oauth  0.2.0         v0.20.0     The legacy OAuth GitHub Gateway for Brigade\nbrigade/brigade-k8s-gateway   0.1.0                     A Helm chart for Kubernetes\nbrigade/brigade-project       1.0.0         v1.0.0      Create a Brigade project\nbrigade/kashti                0.4.0         v0.4.0      A Helm chart for Kubernetes\nHelm search uses a fuzzy string matching algorithm, so you can type parts of\nwords or phrases:\n$ helm search repo kash\nNAME            CHART VERSION APP VERSION DESCRIPTION\nbrigade/kashti  0.4.0         v0.4.0      A Helm chart for Kubernetes\nSearch is a good way to find available packages. Once you have found a package\nyou want to install, you can use\nhelm install\nto install it.\n'helm install': Installing a Package\nâ€‹\nTo install a new package, use the\nhelm install\ncommand. At its simplest, it\ntakes two arguments: A release name that you pick, and the name of the chart you\nwant to install.\n$ helm install happy-panda bitnami/wordpress\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\nYour WordPress site can be accessed through the following DNS name from within your cluster:\nhappy-panda-wordpress.default.svc.cluster.local (port 80)\nTo access your WordPress site from outside the cluster follow the steps below:\n1. Get the WordPress URL by running these commands:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\nWatch the status with: 'kubectl get svc --namespace default -w happy-panda-wordpress'\nexport SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\necho \"WordPress URL: http://$SERVICE_IP/\"\necho \"WordPress Admin URL: http://$SERVICE_IP/admin\"\n2. Open a browser and access WordPress using the obtained URL.\n3. Login with the following credentials below to see your blog:\necho Username: user\necho Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode)\nNow the\nwordpress\nchart is installed. Note that installing a chart creates a\nnew\nrelease\nobject. The release above is named\nhappy-panda\n. (If you want\nHelm to generate a name for you, leave off the release name and use\n--generate-name\n.)\nDuring installation, the\nhelm\nclient will print useful information about which\nresources were created, what the state of the release is, and also whether there\nare additional configuration steps you can or should take.\nHelm installs resources in the following order:\nNamespace\nNetworkPolicy\nResourceQuota\nLimitRange\nPodSecurityPolicy\nPodDisruptionBudget\nServiceAccount\nSecret\nSecretList\nConfigMap\nStorageClass\nPersistentVolume\nPersistentVolumeClaim\nCustomResourceDefinition\nClusterRole\nClusterRoleList\nClusterRoleBinding\nClusterRoleBindingList\nRole\nRoleList\nRoleBinding\nRoleBindingList\nService\nDaemonSet\nPod\nReplicationController\nReplicaSet\nDeployment\nHorizontalPodAutoscaler\nStatefulSet\nJob\nCronJob\nIngress\nAPIService\nMutatingWebhookConfiguration\nValidatingWebhookConfiguration\nHelm does not wait until all of the resources are running before it exits. Many\ncharts require Docker images that are over 600MB in size, and may take a long\ntime to install into the cluster.\nTo keep track of a release's state, or to re-read configuration information, you\ncan use\nhelm status\n:\n$ helm status happy-panda\nNAME: happy-panda\nLAST DEPLOYED: Tue Jan 26 10:27:17 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nNOTES:\n** Please be patient while the chart is being deployed **\nYour WordPress site can be accessed through the following DNS name from within your cluster:\nhappy-panda-wordpress.default.svc.cluster.local (port 80)\nTo access your WordPress site from outside the cluster follow the steps below:\n1. Get the WordPress URL by running these commands:\nNOTE: It may take a few minutes for the LoadBalancer IP to be available.\nWatch the status with: 'kubectl get svc --namespace default -w happy-panda-wordpress'\nexport SERVICE_IP=$(kubectl get svc --namespace default happy-panda-wordpress --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\necho \"WordPress URL: http://$SERVICE_IP/\"\necho \"WordPress Admin URL: http://$SERVICE_IP/admin\"\n2. Open a browser and access WordPress using the obtained URL.\n3. Login with the following credentials below to see your blog:\necho Username: user\necho Password: $(kubectl get secret --namespace default happy-panda-wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode)\nThe above shows the current state of your release.\nCustomizing the Chart Before Installing\nâ€‹\nInstalling the way we have here will only use the default configuration options\nfor this chart. Many times, you will want to customize the chart to use your\npreferred configuration.\nTo see what options are configurable on a chart, use\nhelm show values\n:\n$ helm show values bitnami/wordpress\n## Global Docker image parameters\n## Please, note that this will override the image parameters, including dependencies, configured to use the global value\n## Current available global Docker image parameters: imageRegistry and imagePullSecrets\n##\n# global:\n#   imageRegistry: myRegistryName\n#   imagePullSecrets:\n#     - myRegistryKeySecretName\n#   storageClass: myStorageClass\n## Bitnami WordPress image version\n## ref: https://hub.docker.com/r/bitnami/wordpress/tags/\n##\nimage:\nregistry: docker.io\nrepository: bitnami/wordpress\ntag: 5.6.0-debian-10-r35\n[..]\nYou can then override any of these settings in a YAML formatted file, and then\npass that file during installation.\n$ echo '{mariadb.auth.database: user0db, mariadb.auth.username: user0}' > values.yaml\n$ helm install -f values.yaml bitnami/wordpress --generate-name\nThe above will create a default MariaDB user with the name\nuser0\n, and grant\nthis user access to a newly created\nuser0db\ndatabase, but will accept all the\nrest of the defaults for that chart.\nThere are two ways to pass configuration data during install:\n--values\n(or\n-f\n): Specify a YAML file with overrides. This can be\nspecified multiple times and the rightmost file will take precedence\n--set\n: Specify overrides on the command line.\nIf both are used,\n--set\nvalues are merged into\n--values\nwith higher\nprecedence. Overrides specified with\n--set\nare persisted in a Secret.\nValues that have been\n--set\ncan be viewed for a given release with\nhelm get values <release-name>\n. Values that have been\n--set\ncan be cleared by running\nhelm upgrade\nwith\n--reset-values\nspecified.\nThe Format and Limitations of\n--set\nâ€‹\nThe\n--set\noption takes zero or more name/value pairs. At its simplest, it is\nused like this:\n--set name=value\n. The YAML equivalent of that is:\nname\n:\nvalue\nMultiple values are separated by\n,\ncharacters. So\n--set a=b,c=d\nbecomes:\na\n:\nb\nc\n:\nd\nMore complex expressions are supported. For example,\n--set outer.inner=value\nis translated into this:\nouter\n:\ninner\n:\nvalue\nLists can be expressed by enclosing values in\n{\nand\n}\n. For example,\n--set name={a, b, c}\ntranslates to:\nname\n:\n-\na\n-\nb\n-\nc\nCertain name/key can be set to be\nnull\nor to be an empty array\n[]\n. For example,\n--set name=[],a=null\ntranslates\nname\n:\n-\na\n-\nb\n-\nc\na\n:\nb\nto\nname\n:\n[\n]\na\n:\nnull\nAs of Helm 2.5.0, it is possible to access list items using an array index\nsyntax. For example,\n--set servers[0].port=80\nbecomes:\nservers\n:\n-\nport\n:\n80\nMultiple values can be set this way. The line\n--set servers[0].port=80,servers[0].host=example\nbecomes:\nservers\n:\n-\nport\n:\n80\nhost\n:\nexample\nSometimes you need to use special characters in your\n--set\nlines. You can use\na backslash to escape the characters;\n--set name=value1\\,value2\nwill become:\nname\n:\n\"value1,value2\"\nSimilarly, you can escape dot sequences as well, which may come in handy when\ncharts use the\ntoYaml\nfunction to parse annotations, labels and node\nselectors. The syntax for\n--set nodeSelector.\"kubernetes\\.io/role\"=master\nbecomes:\nnodeSelector\n:\nkubernetes.io/role\n:\nmaster\nDeeply nested data structures can be difficult to express using\n--set\n. Chart\ndesigners are encouraged to consider the\n--set\nusage when designing the format\nof a\nvalues.yaml\nfile  (read more about\nValues Files\n).\nMore Installation Methods\nâ€‹\nThe\nhelm install\ncommand can install from several sources:\nA chart repository (as we've seen above)\nA local chart archive (\nhelm install foo foo-0.1.1.tgz\n)\nAn unpacked chart directory (\nhelm install foo path/to/foo\n)\nA full URL (\nhelm install foo https://example.com/charts/foo-1.2.3.tgz\n)\n'helm upgrade' and 'helm rollback': Upgrading a Release, and Recovering on Failure\nâ€‹\nWhen a new version of a chart is released, or when you want to change the\nconfiguration of your release, you can use the\nhelm upgrade\ncommand.\nAn upgrade takes an existing release and upgrades it according to the\ninformation you provide. Because Kubernetes charts can be large and complex,\nHelm tries to perform the least invasive upgrade. It will only update things\nthat have changed since the last release.\n$ helm upgrade -f panda.yaml happy-panda bitnami/wordpress\nIn the above case, the\nhappy-panda\nrelease is upgraded with the same chart,\nbut with a new YAML file:\nmariadb.auth.username\n:\nuser1\nWe can use\nhelm get values\nto see whether that new setting took effect.\n$ helm get values happy-panda\nmariadb:\nauth:\nusername: user1\nThe\nhelm get\ncommand is a useful tool for looking at a release in the cluster.\nAnd as we can see above, it shows that our new values from\npanda.yaml\nwere\ndeployed to the cluster.\nNow, if something does not go as planned during a release, it is easy to roll\nback to a previous release using\nhelm rollback [RELEASE] [REVISION]\n.\n$ helm rollback happy-panda 1\nThe above rolls back our happy-panda to its very first release version. A\nrelease version is an incremental revision. Every time an install, upgrade, or\nrollback happens, the revision number is incremented by 1. The first revision\nnumber is always 1. And we can use\nhelm history [RELEASE]\nto see revision\nnumbers for a certain release.\nHelpful Options for Install/Upgrade/Rollback\nâ€‹\nThere are several other helpful options you can specify for customizing the\nbehavior of Helm during an install/upgrade/rollback. Please note that this is\nnot a full list of cli flags. To see a description of all flags, just run\nhelm <command> --help\n.\n--timeout\n: A\nGo duration\nvalue\nto wait for Kubernetes commands to complete. This defaults to\n5m0s\n.\n--wait\n: Waits until all Pods are in a ready state, PVCs are bound,\nDeployments have minimum (\nDesired\nminus\nmaxUnavailable\n) Pods in ready\nstate and Services have an IP address (and Ingress if a\nLoadBalancer\n) before\nmarking the release as successful. It will wait for as long as the\n--timeout\nvalue. If timeout is reached, the release will be marked as\nFAILED\n. Note: In\nscenarios where Deployment has\nreplicas\nset to 1 and\nmaxUnavailable\nis not\nset to 0 as part of rolling update strategy,\n--wait\nwill return as ready as\nit has satisfied the minimum Pod in ready condition.\n--no-hooks\n: This skips running hooks for the command\n--recreate-pods\n(only available for\nupgrade\nand\nrollback\n): This flag\nwill cause all pods to be recreated (with the exception of pods belonging to\ndeployments). (DEPRECATED in Helm 3)\n'helm uninstall': Uninstalling a Release\nâ€‹\nWhen it is time to uninstall a release from the cluster, use the\nhelm uninstall\ncommand:\n$ helm uninstall happy-panda\nThis will remove the release from the cluster. You can see all of your currently\ndeployed releases with the\nhelm list\ncommand:\n$ helm list\nNAME            VERSION UPDATED                         STATUS          CHART\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\nFrom the output above, we can see that the\nhappy-panda\nrelease was\nuninstalled.\nIn previous versions of Helm, when a release was deleted, a record of its\ndeletion would remain. In Helm 3, deletion removes the release record as well.\nIf you wish to keep a deletion release record, use\nhelm uninstall --keep-history\n. Using\nhelm list --uninstalled\nwill only show releases that\nwere uninstalled with the\n--keep-history\nflag.\nThe\nhelm list --all\nflag will show you all release records that Helm has\nretained, including records for failed or deleted items (if\n--keep-history\nwas\nspecified):\n$  helm list --all\nNAME            VERSION UPDATED                         STATUS          CHART\nhappy-panda     2       Wed Sep 28 12:47:54 2016        UNINSTALLED     wordpress-10.4.5.6.0\ninky-cat        1       Wed Sep 28 12:59:46 2016        DEPLOYED        alpine-0.1.0\nkindred-angelf  2       Tue Sep 27 16:16:10 2016        UNINSTALLED     alpine-0.1.0\nNote that because releases are now deleted by default, it is no longer possible\nto rollback an uninstalled resource.\n'helm repo': Working with Repositories\nâ€‹\nHelm 3 no longer ships with a default chart repository. The\nhelm repo\ncommand\ngroup provides commands to add, list, and remove repositories.\nYou can see which repositories are configured using\nhelm repo list\n:\n$ helm repo list\nNAME            URL\nstable          https://charts.helm.sh/stable\nmumoshu         https://mumoshu.github.io/charts\nAnd new repositories can be added with\nhelm repo add [NAME] [URL]\n:\n$ helm repo add dev https://example.com/dev-charts\nBecause chart repositories change frequently, at any point you can make sure\nyour Helm client is up to date by running\nhelm repo update\n.\nRepositories can be removed with\nhelm repo remove\n.\nCreating Your Own Charts\nâ€‹\nThe\nChart Development Guide\nexplains how\nto develop your own charts. But you can get started quickly by using the\nhelm create\ncommand:\n$ helm create deis-workflow\nCreating deis-workflow\nNow there is a chart in\n./deis-workflow\n. You can edit it and create your own\ntemplates.\nAs you edit your chart, you can validate that it is well-formed by running\nhelm lint\n.\nWhen it's time to package the chart up for distribution, you can run the\nhelm package\ncommand:\n$ helm package deis-workflow\ndeis-workflow-0.1.0.tgz\nAnd that chart can now easily be installed by\nhelm install\n:\n$ helm install deis-workflow ./deis-workflow-0.1.0.tgz\n...\nCharts that are packaged can be loaded into chart repositories. See the\ndocumentation for\nHelm chart\nrepositories\nfor more details.\nConclusion\nâ€‹\nThis chapter has covered the basic usage patterns of the\nhelm\nclient,\nincluding searching, installation, upgrading, and uninstalling. It has also\ncovered useful utility commands like\nhelm status\n,\nhelm get\n, and\nhelm repo\n.\nFor more information on these commands, take a look at Helm's built-in help:\nhelm help\n.\nIn the\nnext chapter\n, we look at the process of developing charts.\nThree Big Concepts\n'helm search': Finding Charts\n'helm install': Installing a Package\nCustomizing the Chart Before Installing\nMore Installation Methods\n'helm upgrade' and 'helm rollback': Upgrading a Release, and Recovering on Failure\nHelpful Options for Install/Upgrade/Rollback\n'helm uninstall': Uninstalling a Release\n'helm repo': Working with Repositories\nCreating Your Own Charts\nConclusion", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://helm.sh/docs/intro/using_helm/"}}
{"text": "Charts | Helm\nSkip to main content\nðŸŽ‰ Helm v4.0.0 is out! See the\nHelm 4 Overview\nfor details!\nVersion: 4.0.0\nOn this page\nCharts\nwarning\nThis page has not yet been updated for Helm 4. Some of the content might be inaccurate or not applicable to Helm 4. For more information about the Helm 4 new features, improvements, and breaking changes, see\nHelm 4 Overview\n.\nHelm uses a packaging format called\ncharts\n. A chart is a collection of files\nthat describe a related set of Kubernetes resources. A single chart might be\nused to deploy something simple, like a memcached pod, or something complex,\nlike a full web app stack with HTTP servers, databases, caches, and so on.\nCharts are created as files laid out in a particular directory tree. They can be\npackaged into versioned archives to be deployed.\nIf you want to download and look at the files for a published chart, without\ninstalling it, you can do so with\nhelm pull chartrepo/chartname\n.\nThis document explains the chart format, and provides basic guidance for\nbuilding charts with Helm.\nThe Chart File Structure\nâ€‹\nA chart is organized as a collection of files inside of a directory. The\ndirectory name is the name of the chart (without versioning information). Thus,\na chart describing WordPress would be stored in a\nwordpress/\ndirectory.\nInside of this directory, Helm will expect a structure that matches this:\nwordpress/\nChart.yaml          # A YAML file containing information about the chart\nLICENSE             # OPTIONAL: A plain text file containing the license for the chart\nREADME.md           # OPTIONAL: A human-readable README file\nvalues.yaml         # The default configuration values for this chart\nvalues.schema.json  # OPTIONAL: A JSON Schema for imposing a structure on the values.yaml file\ncharts/             # A directory containing any charts upon which this chart depends.\ncrds/               # Custom Resource Definitions\ntemplates/          # A directory of templates that, when combined with values,\n# will generate valid Kubernetes manifest files.\ntemplates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes\nHelm reserves use of the\ncharts/\n,\ncrds/\n, and\ntemplates/\ndirectories, and\nof the listed file names. Other files will be left as they are.\nThe Chart.yaml File\nâ€‹\nThe\nChart.yaml\nfile is required for a chart. It contains the following fields:\napiVersion\n:\nThe chart API version (required)\nname\n:\nThe name of the chart (required)\nversion\n:\nThe version of the chart (required)\nkubeVersion\n:\nA SemVer range of compatible Kubernetes versions (optional)\ndescription\n:\nA single\n-\nsentence description of this project (optional)\ntype\n:\nThe type of the chart (optional)\nkeywords\n:\n-\nA list of keywords about this project (optional)\nhome\n:\nThe URL of this projects home page (optional)\nsources\n:\n-\nA list of URLs to source code for this project (optional)\ndependencies\n:\n# A list of the chart requirements (optional)\n-\nname\n:\nThe name of the chart (nginx)\nversion\n:\nThe version of the chart (\"1.2.3\")\nrepository\n:\n(optional) The repository URL (\"https\n:\n//example.com/charts\") or alias (\"@repo\n-\nname\")\ncondition\n:\n(optional) A yaml path that resolves to a boolean\n,\nused for enabling/disabling charts (e.g. subchart1.enabled )\ntags\n:\n# (optional)\n-\nTags can be used to group charts for enabling/disabling together\nimport-values\n:\n# (optional)\n-\nImportValues holds the mapping of source values to parent key to be imported. Each item can be a string or pair of child/parent sublist items.\nalias\n:\n(optional) Alias to be used for the chart. Useful when you have to add the same chart multiple times\nmaintainers\n:\n# (optional)\n-\nname\n:\nThe maintainers name (required for each maintainer)\nemail\n:\nThe maintainers email (optional for each maintainer)\nurl\n:\nA URL for the maintainer (optional for each maintainer)\nicon\n:\nA URL to an SVG or PNG image to be used as an icon (optional).\nappVersion\n:\nThe version of the app that this contains (optional). Needn't be SemVer. Quotes recommended.\ndeprecated\n:\nWhether this chart is deprecated (optional\n,\nboolean)\nannotations\n:\nexample\n:\nA list of annotations keyed by name (optional).\nAs of\nv3.3.2\n, additional\nfields are not allowed.\nThe recommended approach is to add custom metadata in\nannotations\n.\nCharts and Versioning\nâ€‹\nEvery chart must have a version number. A version should follow the\nSemVer\n2\nstandard but it is not strictly enforced. Unlike Helm Classic, Helm v2\nand later uses version numbers as release markers. Packages in repositories are\nidentified by name plus version.\nFor example, an\nnginx\nchart whose version field is set to\nversion: 1.2.3\nwill be named:\nnginx-1.2.3.tgz\nMore complex SemVer 2 names are also supported, such as\nversion: 1.2.3-alpha.1+ef365\n. But non-SemVer names are explicitly disallowed by the\nsystem. Subject to exception are versions in format\nx\nor\nx.y\n.\nFor example, if there is a leading v or a version listed without all 3 parts (e.g. v1.2) it will attempt to coerce it into a valid semantic version (e.g., v1.2.0).\nNOTE:\nWhereas Helm Classic and Deployment Manager were both very GitHub\noriented when it came to charts, Helm v2 and later does not rely upon or require\nGitHub or even Git. Consequently, it does not use Git SHAs for versioning at\nall.\nThe\nversion\nfield inside of the\nChart.yaml\nis used by many of the Helm\ntools, including the CLI. When generating a package, the\nhelm package\ncommand\nwill use the version that it finds in the\nChart.yaml\nas a token in the package\nname. The system assumes that the version number in the chart package name\nmatches the version number in the\nChart.yaml\n. Failure to meet this assumption\nwill cause an error.\nThe\napiVersion\nField\nâ€‹\nThe\napiVersion\nfield should be\nv2\nfor Helm charts that require at least Helm\n3. Charts supporting previous Helm versions have an\napiVersion\nset to\nv1\nand\nare still installable by Helm 3.\nChanges from\nv1\nto\nv2\n:\nA\ndependencies\nfield defining chart dependencies, which were located in a\nseparate\nrequirements.yaml\nfile for\nv1\ncharts (see\nChart\nDependencies\n).\nThe\ntype\nfield, discriminating application and library charts (see\nChart\nTypes\n).\nThe\nappVersion\nField\nâ€‹\nNote that the\nappVersion\nfield is not related to the\nversion\nfield. It is a\nway of specifying the version of the application. For example, the\ndrupal\nchart may have an\nappVersion: \"8.2.1\"\n, indicating that the version of Drupal\nincluded in the chart (by default) is\n8.2.1\n. This field is informational, and\nhas no impact on chart version calculations. Wrapping the version in quotes is highly recommended. It forces the YAML parser to treat the version number as a string. Leaving it unquoted can lead to parsing issues in some cases. For example, YAML interprets\n1.0\nas a floating point value, and a git commit SHA like\n1234e10\nas scientific notation.\nAs of Helm v3.5.0,\nhelm create\nwraps the default\nappVersion\nfield in quotes.\nThe\nkubeVersion\nField\nâ€‹\nThe optional\nkubeVersion\nfield can define semver constraints on supported\nKubernetes versions. Helm will validate the version constraints when installing\nthe chart and fail if the cluster runs an unsupported Kubernetes version.\nVersion constraints may comprise space separated AND comparisons such as\n>= 1.13.0 < 1.15.0\nwhich themselves can be combined with the OR\n||\noperator like in the following\nexample\n>= 1.13.0 < 1.14.0 || >= 1.14.1 < 1.15.0\nIn this example the version\n1.14.0\nis excluded, which can make sense if a bug\nin certain versions is known to prevent the chart from running properly.\nApart from version constrains employing operators\n=\n!=\n>\n<\n>=\n<=\nthe\nfollowing shorthand notations are supported\nhyphen ranges for closed intervals, where\n1.1 - 2.3.4\nis equivalent to\n>= 1.1 <= 2.3.4\n.\nwildcards\nx\n,\nX\nand\n*\n, where\n1.2.x\nis equivalent to\n>= 1.2.0 < 1.3.0\n.\ntilde ranges (patch version changes allowed), where\n~1.2.3\nis equivalent to\n>= 1.2.3 < 1.3.0\n.\ncaret ranges (minor version changes allowed), where\n^1.2.3\nis equivalent to\n>= 1.2.3 < 2.0.0\n.\nFor a detailed explanation of supported semver constraints see\nMasterminds/semver\n.\nDeprecating Charts\nâ€‹\nWhen managing charts in a Chart Repository, it is sometimes necessary to\ndeprecate a chart. The optional\ndeprecated\nfield in\nChart.yaml\ncan be used\nto mark a chart as deprecated. If the\nlatest\nversion of a chart in the\nrepository is marked as deprecated, then the chart as a whole is considered to\nbe deprecated. The chart name can be later reused by publishing a newer version\nthat is not marked as deprecated. The workflow for deprecating charts is:\nUpdate chart's\nChart.yaml\nto mark the chart as deprecated, bumping the\nversion\nRelease the new chart version in the Chart Repository\nRemove the chart from the source repository (e.g. git)\nChart Types\nâ€‹\nThe\ntype\nfield defines the type of chart. There are two types:\napplication\nand\nlibrary\n. Application is the default type and it is the standard chart\nwhich can be operated on fully. The\nlibrary chart\nprovides utilities or functions for the\nchart builder. A library chart differs from an application chart because it is\nnot installable and usually doesn't contain any resource objects.\nNote:\nAn application chart can be used as a library chart. This is enabled\nby setting the type to\nlibrary\n. The chart will then be rendered as a library\nchart where all utilities and functions can be leveraged. All resource objects\nof the chart will not be rendered.\nChart LICENSE, README and NOTES\nâ€‹\nCharts can also contain files that describe the installation, configuration,\nusage and license of a chart.\nA LICENSE is a plain text file containing the\nlicense\nfor the chart. The\nchart can contain a license as it may have programming logic in the templates\nand would therefore not be configuration only. There can also be separate\nlicense(s) for the application installed by the chart, if required.\nA README for a chart should be formatted in Markdown (README.md), and should\ngenerally contain:\nA description of the application or service the chart provides\nAny prerequisites or requirements to run the chart\nDescriptions of options in\nvalues.yaml\nand default values\nAny other information that may be relevant to the installation or\nconfiguration of the chart\nWhen hubs and other user interfaces display details about a chart that detail is\npulled from the content in the\nREADME.md\nfile.\nThe chart can also contain a short plain text\ntemplates/NOTES.txt\nfile that\nwill be printed out after installation, and when viewing the status of a\nrelease. This file is evaluated as a\ntemplate\n, and can\nbe used to display usage notes, next steps, or any other information relevant to\na release of the chart. For example, instructions could be provided for\nconnecting to a database, or accessing a web UI. Since this file is printed to\nSTDOUT when running\nhelm install\nor\nhelm status\n, it is recommended to keep\nthe content brief and point to the README for greater detail.\nChart Dependencies\nâ€‹\nIn Helm, one chart may depend on any number of other charts. These dependencies\ncan be dynamically linked using the\ndependencies\nfield in\nChart.yaml\nor\nbrought in to the\ncharts/\ndirectory and managed manually.\nManaging Dependencies with the\ndependencies\nfield\nâ€‹\nThe charts required by the current chart are defined as a list in the\ndependencies\nfield.\ndependencies\n:\n-\nname\n:\napache\nversion\n:\n1.2.3\nrepository\n:\nhttps\n:\n//example.com/charts\n-\nname\n:\nmysql\nversion\n:\n3.2.1\nrepository\n:\nhttps\n:\n//another.example.com/charts\nThe\nname\nfield is the name of the chart you want.\nThe\nversion\nfield is the version of the chart you want.\nThe\nrepository\nfield is the full URL to the chart repository. Note that you\nmust also use\nhelm repo add\nto add that repo locally.\nYou might use the name of the repo instead of URL\n$ helm repo add fantastic-charts https://charts.helm.sh/incubator\ndependencies\n:\n-\nname\n:\nawesomeness\nversion\n:\n1.0.0\nrepository\n:\n\"@fantastic-charts\"\nOnce you have defined dependencies, you can run\nhelm dependency update\nand it\nwill use your dependency file to download all the specified charts into your\ncharts/\ndirectory for you.\n$ helm dep up foochart\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"local\" chart repository\n...Successfully got an update from the \"stable\" chart repository\n...Successfully got an update from the \"example\" chart repository\n...Successfully got an update from the \"another\" chart repository\nUpdate Complete. Happy Helming!\nSaving 2 charts\nDownloading apache from repo https://example.com/charts\nDownloading mysql from repo https://another.example.com/charts\nWhen\nhelm dependency update\nretrieves charts, it will store them as chart\narchives in the\ncharts/\ndirectory. So for the example above, one would expect\nto see the following files in the charts directory:\ncharts/\napache-1.2.3.tgz\nmysql-3.2.1.tgz\nAlias field in dependencies\nâ€‹\nIn addition to the other fields above, each requirements entry may contain the\noptional field\nalias\n.\nAdding an alias for a dependency chart would put a chart in dependencies using\nalias as name of new dependency.\nOne can use\nalias\nin cases where they need to access a chart with other\nname(s).\n# parentchart/Chart.yaml\ndependencies\n:\n-\nname\n:\nsubchart\nrepository\n:\nhttp\n:\n//localhost\n:\n10191\nversion\n:\n0.1.0\nalias\n:\nnew\n-\nsubchart\n-\n1\n-\nname\n:\nsubchart\nrepository\n:\nhttp\n:\n//localhost\n:\n10191\nversion\n:\n0.1.0\nalias\n:\nnew\n-\nsubchart\n-\n2\n-\nname\n:\nsubchart\nrepository\n:\nhttp\n:\n//localhost\n:\n10191\nversion\n:\n0.1.0\nIn the above example we will get 3 dependencies in all for\nparentchart\n:\nsubchart\nnew-subchart-1\nnew-subchart-2\nThe manual way of achieving this is by copy/pasting the same chart in the\ncharts/\ndirectory multiple times with different names.\nTags and Condition fields in dependencies\nâ€‹\nIn addition to the other fields above, each requirements entry may contain the\noptional fields\ntags\nand\ncondition\n.\nAll charts are loaded by default. If\ntags\nor\ncondition\nfields are present,\nthey will be evaluated and used to control loading for the chart(s) they are\napplied to.\nCondition - The condition field holds one or more YAML paths (delimited by\ncommas). If this path exists in the top parent's values and resolves to a\nboolean value, the chart will be enabled or disabled based on that boolean\nvalue.  Only the first valid path found in the list is evaluated and if no paths\nexist then the condition has no effect.\nTags - The tags field is a YAML list of labels to associate with this chart. In\nthe top parent's values, all charts with tags can be enabled or disabled by\nspecifying the tag and a boolean value.\n# parentchart/Chart.yaml\ndependencies\n:\n-\nname\n:\nsubchart1\nrepository\n:\nhttp\n:\n//localhost\n:\n10191\nversion\n:\n0.1.0\ncondition\n:\nsubchart1.enabled\n,\nglobal.subchart1.enabled\ntags\n:\n-\nfront\n-\nend\n-\nsubchart1\n-\nname\n:\nsubchart2\nrepository\n:\nhttp\n:\n//localhost\n:\n10191\nversion\n:\n0.1.0\ncondition\n:\nsubchart2.enabled\n,\nglobal.subchart2.enabled\ntags\n:\n-\nback\n-\nend\n-\nsubchart2\n# parentchart/values.yaml\nsubchart1\n:\nenabled\n:\ntrue\ntags\n:\nfront-end\n:\nfalse\nback-end\n:\ntrue\nIn the above example all charts with the tag\nfront-end\nwould be disabled but\nsince the\nsubchart1.enabled\npath evaluates to 'true' in the parent's values,\nthe condition will override the\nfront-end\ntag and\nsubchart1\nwill be enabled.\nSince\nsubchart2\nis tagged with\nback-end\nand that tag evaluates to\ntrue\n,\nsubchart2\nwill be enabled. Also note that although\nsubchart2\nhas a condition\nspecified, there is no corresponding path and value in the parent's values so\nthat condition has no effect.\nUsing the CLI with Tags and Conditions\nâ€‹\nThe\n--set\nparameter can be used as usual to alter tag and condition values.\nhelm install --set tags.front-end=true --set subchart2.enabled=false\nTags and Condition Resolution\nâ€‹\nConditions (when set in values) always override tags.\nThe first condition\npath that exists wins and subsequent ones for that chart are ignored.\nTags are evaluated as 'if any of the chart's tags are true then enable the\nchart'.\nTags and conditions values must be set in the top parent's values.\nThe\ntags:\nkey in values must be a top level key. Globals and nested\ntags:\ntables are not currently supported.\nImporting Child Values via dependencies\nâ€‹\nIn some cases it is desirable to allow a child chart's values to propagate to\nthe parent chart and be shared as common defaults. An additional benefit of\nusing the\nexports\nformat is that it will enable future tooling to introspect\nuser-settable values.\nThe keys containing the values to be imported can be specified in the parent\nchart's\ndependencies\nin the field\nimport-values\nusing a YAML list. Each item\nin the list is a key which is imported from the child chart's\nexports\nfield.\nTo import values not contained in the\nexports\nkey, use the\nchild-parent\nformat. Examples of both formats\nare described below.\nUsing the exports format\nâ€‹\nIf a child chart's\nvalues.yaml\nfile contains an\nexports\nfield at the root,\nits contents may be imported directly into the parent's values by specifying the\nkeys to import as in the example below:\n# parent's Chart.yaml file\ndependencies\n:\n-\nname\n:\nsubchart\nrepository\n:\nhttp\n:\n//localhost\n:\n10191\nversion\n:\n0.1.0\nimport-values\n:\n-\ndata\n# child's values.yaml file\nexports\n:\ndata\n:\nmyint\n:\n99\nSince we are specifying the key\ndata\nin our import list, Helm looks in the\nexports\nfield of the child chart for\ndata\nkey and imports its contents.\nThe final parent values would contain our exported field:\n# parent's values\nmyint\n:\n99\nPlease note the parent key\ndata\nis not contained in the parent's final values.\nIf you need to specify the parent key, use the 'child-parent' format.\nUsing the child-parent format\nâ€‹\nTo access values that are not contained in the\nexports\nkey of the child\nchart's values, you will need to specify the source key of the values to be\nimported (\nchild\n) and the destination path in the parent chart's values\n(\nparent\n).\nThe\nimport-values\nin the example below instructs Helm to take any values found\nat\nchild:\npath and copy them to the parent's values at the path specified in\nparent:\n# parent's Chart.yaml file\ndependencies\n:\n-\nname\n:\nsubchart1\nrepository\n:\nhttp\n:\n//localhost\n:\n10191\nversion\n:\n0.1.0\n...\nimport-values\n:\n-\nchild\n:\ndefault.data\nparent\n:\nmyimports\nIn the above example, values found at\ndefault.data\nin the subchart1's values\nwill be imported to the\nmyimports\nkey in the parent chart's values as detailed\nbelow:\n# parent's values.yaml file\nmyimports\n:\nmyint\n:\n0\nmybool\n:\nfalse\nmystring\n:\n\"helm rocks!\"\n# subchart1's values.yaml file\ndefault\n:\ndata\n:\nmyint\n:\n999\nmybool\n:\ntrue\nThe parent chart's resulting values would be:\n# parent's final values\nmyimports\n:\nmyint\n:\n999\nmybool\n:\ntrue\nmystring\n:\n\"helm rocks!\"\nThe parent's final values now contains the\nmyint\nand\nmybool\nfields imported\nfrom subchart1.\nManaging Dependencies manually via the\ncharts/\ndirectory\nâ€‹\nIf more control over dependencies is desired, these dependencies can be\nexpressed explicitly by copying the dependency charts into the\ncharts/\ndirectory.\nA dependency should be an unpacked chart directory but its name cannot start\nwith\n_\nor\n.\n. Such files are ignored by the chart loader.\nFor example, if the WordPress chart depends on the Apache chart, the Apache\nchart (of the correct version) is supplied in the WordPress chart's\ncharts/\ndirectory:\nwordpress\n:\nChart.yaml\n# ...\ncharts/\napache/\nChart.yaml\n# ...\nmysql/\nChart.yaml\n# ...\nThe example above shows how the WordPress chart expresses its dependency on\nApache and MySQL by including those charts inside of its\ncharts/\ndirectory.\nTIP:\nTo drop a dependency into your\ncharts/\ndirectory, use the\nhelm pull\ncommand\nOperational aspects of using dependencies\nâ€‹\nThe above sections explain how to specify chart dependencies, but how does this\naffect chart installation using\nhelm install\nand\nhelm upgrade\n?\nSuppose that a chart named \"A\" creates the following Kubernetes objects\nnamespace \"A-Namespace\"\nstatefulset \"A-StatefulSet\"\nservice \"A-Service\"\nFurthermore, A is dependent on chart B that creates objects\nnamespace \"B-Namespace\"\nreplicaset \"B-ReplicaSet\"\nservice \"B-Service\"\nAfter installation/upgrade of chart A a single Helm release is created/modified.\nThe release will create/update all of the above Kubernetes objects in the\nfollowing order:\nA-Namespace\nB-Namespace\nA-Service\nB-Service\nB-ReplicaSet\nA-StatefulSet\nThis is because when Helm installs/upgrades charts, the Kubernetes objects from\nthe charts and all its dependencies are\naggregated into a single set; then\nsorted by type followed by name; and then\ncreated/updated in that order.\nHence a single release is created with all the objects for the chart and its\ndependencies.\nThe install order of Kubernetes types is given by the enumeration InstallOrder\nin kind_sorter.go (see\nthe Helm source\nfile\n).\nTemplates and Values\nâ€‹\nHelm Chart templates are written in the\nGo template\nlanguage\n, with the addition of 50 or so\nadd-on template functions\nfrom the Sprig\nlibrary\nand a few other\nspecialized\nfunctions\n.\nAll template files are stored in a chart's\ntemplates/\nfolder. When Helm\nrenders the charts, it will pass every file in that directory through the\ntemplate engine.\nValues for the templates are supplied two ways:\nChart developers may supply a file called\nvalues.yaml\ninside of a chart.\nThis file can contain default values.\nChart users may supply a YAML file that contains values. This can be provided\non the command line with\nhelm install\n.\nWhen a user supplies custom values, these values will override the values in the\nchart's\nvalues.yaml\nfile.\nTemplate Files\nâ€‹\nTemplate files follow the standard conventions for writing Go templates (see\nthe text/template Go package\ndocumentation\nfor details). An example\ntemplate file might look something like this:\napiVersion\n:\nv1\nkind\n:\nReplicationController\nmetadata\n:\nname\n:\ndeis\n-\ndatabase\nnamespace\n:\ndeis\nlabels\n:\napp.kubernetes.io/managed-by\n:\ndeis\nspec\n:\nreplicas\n:\n1\nselector\n:\napp.kubernetes.io/name\n:\ndeis\n-\ndatabase\ntemplate\n:\nmetadata\n:\nlabels\n:\napp.kubernetes.io/name\n:\ndeis\n-\ndatabase\nspec\n:\nserviceAccount\n:\ndeis\n-\ndatabase\ncontainers\n:\n-\nname\n:\ndeis\n-\ndatabase\nimage\n:\n{\n{\n.Values.imageRegistry\n}\n}\n/postgres\n:\n{\n{\n.Values.dockerTag\n}\n}\nimagePullPolicy\n:\n{\n{\n.Values.pullPolicy\n}\n}\nports\n:\n-\ncontainerPort\n:\n5432\nenv\n:\n-\nname\n:\nDATABASE_STORAGE\nvalue\n:\n{\n{\ndefault \"minio\" .Values.storage\n}\n}\nThe above example, based loosely on\nhttps://github.com/deis/charts\n, is a template\nfor a Kubernetes replication controller. It can use the following four template\nvalues (usually defined in a\nvalues.yaml\nfile):\nimageRegistry\n: The source registry for the Docker image.\ndockerTag\n: The tag for the docker image.\npullPolicy\n: The Kubernetes pull policy.\nstorage\n: The storage backend, whose default is set to\n\"minio\"\nAll of these values are defined by the template author. Helm does not require or\ndictate parameters.\nTo see many working charts, check out the CNCF\nArtifact\nHub\n.\nPredefined Values\nâ€‹\nValues that are supplied via a\nvalues.yaml\nfile (or via the\n--set\nflag) are\naccessible from the\n.Values\nobject in a template. But there are other\npre-defined pieces of data you can access in your templates.\nThe following values are pre-defined, are available to every template, and\ncannot be overridden. As with all values, the names are\ncase sensitive\n.\nRelease.Name\n: The name of the release (not the chart)\nRelease.Namespace\n: The namespace the chart was released to.\nRelease.Service\n: The service that conducted the release.\nRelease.IsUpgrade\n: This is set to true if the current operation is an\nupgrade or rollback.\nRelease.IsInstall\n: This is set to true if the current operation is an\ninstall.\nChart\n: The contents of the\nChart.yaml\n. Thus, the chart version is\nobtainable as\nChart.Version\nand the maintainers are in\nChart.Maintainers\n.\nFiles\n: A map-like object containing all non-special files in the chart. This\nwill not give you access to templates, but will give you access to additional\nfiles that are present (unless they are excluded using\n.helmignore\n). Files\ncan be accessed using\n{{ index .Files \"file.name\" }}\nor using the\n{{.Files.Get name }}\nfunction. You can also access the contents of the file\nas\n[]byte\nusing\n{{ .Files.GetBytes }}\nCapabilities\n: A map-like object that contains information about the versions\nof Kubernetes (\n{{ .Capabilities.KubeVersion }}\n) and the supported Kubernetes\nAPI versions (\n{{ .Capabilities.APIVersions.Has \"batch/v1\" }}\n)\nNOTE:\nAny unknown\nChart.yaml\nfields will be dropped. They will not be\naccessible inside of the\nChart\nobject. Thus,\nChart.yaml\ncannot be used to\npass arbitrarily structured data into the template. The values file can be used\nfor that, though.\nValues files\nâ€‹\nConsidering the template in the previous section, a\nvalues.yaml\nfile that\nsupplies the necessary values would look like this:\nimageRegistry\n:\n\"quay.io/deis\"\ndockerTag\n:\n\"latest\"\npullPolicy\n:\n\"Always\"\nstorage\n:\n\"s3\"\nA values file is formatted in YAML. A chart may include a default\nvalues.yaml\nfile. The Helm install command allows a user to override values by supplying\nadditional YAML values:\n$ helm install --generate-name --values=myvals.yaml wordpress\nWhen values are passed in this way, they will be merged into the default values\nfile. For example, consider a\nmyvals.yaml\nfile that looks like this:\nstorage\n:\n\"gcs\"\nWhen this is merged with the\nvalues.yaml\nin the chart, the resulting generated\ncontent will be:\nimageRegistry\n:\n\"quay.io/deis\"\ndockerTag\n:\n\"latest\"\npullPolicy\n:\n\"Always\"\nstorage\n:\n\"gcs\"\nNote that only the last field was overridden.\nNOTE:\nThe default values file included inside of a chart\nmust\nbe named\nvalues.yaml\n. But files specified on the command line can be named anything.\nNOTE:\nIf the\n--set\nflag is used on\nhelm install\nor\nhelm upgrade\n, those\nvalues are simply converted to YAML on the client side.\nNOTE:\nIf any required entries in the values file exist, they can be declared\nas required in the chart template by using the\n'required' function\nAny of these values are then accessible inside of templates using the\n.Values\nobject:\napiVersion\n:\nv1\nkind\n:\nReplicationController\nmetadata\n:\nname\n:\ndeis\n-\ndatabase\nnamespace\n:\ndeis\nlabels\n:\napp.kubernetes.io/managed-by\n:\ndeis\nspec\n:\nreplicas\n:\n1\nselector\n:\napp.kubernetes.io/name\n:\ndeis\n-\ndatabase\ntemplate\n:\nmetadata\n:\nlabels\n:\napp.kubernetes.io/name\n:\ndeis\n-\ndatabase\nspec\n:\nserviceAccount\n:\ndeis\n-\ndatabase\ncontainers\n:\n-\nname\n:\ndeis\n-\ndatabase\nimage\n:\n{\n{\n.Values.imageRegistry\n}\n}\n/postgres\n:\n{\n{\n.Values.dockerTag\n}\n}\nimagePullPolicy\n:\n{\n{\n.Values.pullPolicy\n}\n}\nports\n:\n-\ncontainerPort\n:\n5432\nenv\n:\n-\nname\n:\nDATABASE_STORAGE\nvalue\n:\n{\n{\ndefault \"minio\" .Values.storage\n}\n}\nScope, Dependencies, and Values\nâ€‹\nValues files can declare values for the top-level chart, as well as for any of\nthe charts that are included in that chart's\ncharts/\ndirectory. Or, to phrase\nit differently, a values file can supply values to the chart as well as to any\nof its dependencies. For example, the demonstration WordPress chart above has\nboth\nmysql\nand\napache\nas dependencies. The values file could supply values\nto all of these components:\ntitle\n:\n\"My WordPress Site\"\n# Sent to the WordPress template\nmysql\n:\nmax_connections\n:\n100\n# Sent to MySQL\npassword\n:\n\"secret\"\napache\n:\nport\n:\n8080\n# Passed to Apache\nCharts at a higher level have access to all of the variables defined beneath. So\nthe WordPress chart can access the MySQL password as\n.Values.mysql.password\n.\nBut lower level charts cannot access things in parent charts, so MySQL will not\nbe able to access the\ntitle\nproperty. Nor, for that matter, can it access\napache.port\n.\nValues are namespaced, but namespaces are pruned. So for the WordPress chart, it\ncan access the MySQL password field as\n.Values.mysql.password\n. But for the\nMySQL chart, the scope of the values has been reduced and the namespace prefix\nremoved, so it will see the password field simply as\n.Values.password\n.\nGlobal Values\nâ€‹\nAs of 2.0.0-Alpha.2, Helm supports special \"global\" value. Consider this\nmodified version of the previous example:\ntitle\n:\n\"My WordPress Site\"\n# Sent to the WordPress template\nglobal\n:\napp\n:\nMyWordPress\nmysql\n:\nmax_connections\n:\n100\n# Sent to MySQL\npassword\n:\n\"secret\"\napache\n:\nport\n:\n8080\n# Passed to Apache\nThe above adds a\nglobal\nsection with the value\napp: MyWordPress\n. This value\nis available to\nall\ncharts as\n.Values.global.app\n.\nFor example, the\nmysql\ntemplates may access\napp\nas\n{{ .Values.global.app}}\n, and so can the\napache\nchart. Effectively, the values\nfile above is regenerated like this:\ntitle\n:\n\"My WordPress Site\"\n# Sent to the WordPress template\nglobal\n:\napp\n:\nMyWordPress\nmysql\n:\nglobal\n:\napp\n:\nMyWordPress\nmax_connections\n:\n100\n# Sent to MySQL\npassword\n:\n\"secret\"\napache\n:\nglobal\n:\napp\n:\nMyWordPress\nport\n:\n8080\n# Passed to Apache\nThis provides a way of sharing one top-level variable with all subcharts, which\nis useful for things like setting\nmetadata\nproperties like labels.\nIf a subchart declares a global variable, that global will be passed\ndownward\n(to the subchart's subcharts), but not\nupward\nto the parent chart. There is no\nway for a subchart to influence the values of the parent chart.\nAlso, global variables of parent charts take precedence over the global\nvariables from subcharts.\nSchema Files\nâ€‹\nSometimes, a chart maintainer might want to define a structure on their values.\nThis can be done by defining a schema in the\nvalues.schema.json\nfile. A schema\nis represented as a\nJSON Schema\n. It might look\nsomething like this:\n{\n\"$schema\"\n:\n\"https://json-schema.org/draft-07/schema#\"\n,\n\"properties\"\n:\n{\n\"image\"\n:\n{\n\"description\"\n:\n\"Container Image\"\n,\n\"properties\"\n:\n{\n\"repo\"\n:\n{\n\"type\"\n:\n\"string\"\n}\n,\n\"tag\"\n:\n{\n\"type\"\n:\n\"string\"\n}\n}\n,\n\"type\"\n:\n\"object\"\n}\n,\n\"name\"\n:\n{\n\"description\"\n:\n\"Service name\"\n,\n\"type\"\n:\n\"string\"\n}\n,\n\"port\"\n:\n{\n\"description\"\n:\n\"Port\"\n,\n\"minimum\"\n:\n0\n,\n\"type\"\n:\n\"integer\"\n}\n,\n\"protocol\"\n:\n{\n\"type\"\n:\n\"string\"\n}\n}\n,\n\"required\"\n:\n[\n\"protocol\"\n,\n\"port\"\n]\n,\n\"title\"\n:\n\"Values\"\n,\n\"type\"\n:\n\"object\"\n}\nThis schema will be applied to the values to validate it. Validation occurs when\nany of the following commands are invoked:\nhelm install\nhelm upgrade\nhelm lint\nhelm template\nAn example of a\nvalues.yaml\nfile that meets the requirements of this schema\nmight look something like this:\nname\n:\nfrontend\nprotocol\n:\nhttps\nport\n:\n443\nNote that the schema is applied to the final\n.Values\nobject, and not just to\nthe\nvalues.yaml\nfile. This means that the following\nyaml\nfile is valid,\ngiven that the chart is installed with the appropriate\n--set\noption shown\nbelow.\nname\n:\nfrontend\nprotocol\n:\nhttps\nhelm install --set port=443\nFurthermore, the final\n.Values\nobject is checked against\nall\nsubchart\nschemas. This means that restrictions on a subchart can't be circumvented by a\nparent chart. This also works backwards - if a subchart has a requirement that\nis not met in the subchart's\nvalues.yaml\nfile, the parent chart\nmust\nsatisfy\nthose restrictions in order to be valid.\nSchema validation can be disabled by setting the option shown below.\nThis is particularly useful in air-gapped environments when a chart's JSON Schema file contains remote references.\nhelm install --skip-schema-validation\nReferences\nâ€‹\nWhen it comes to writing templates, values, and schema files, there are several\nstandard references that will help you out.\nGo templates\nExtra template functions\nThe YAML format\nJSON Schema\nCustom Resource Definitions (CRDs)\nâ€‹\nKubernetes provides a mechanism for declaring new types of Kubernetes objects.\nUsing CustomResourceDefinitions (CRDs), Kubernetes developers can declare custom\nresource types.\nIn Helm 3, CRDs are treated as a special kind of object. They are installed\nbefore the rest of the chart, and are subject to some limitations.\nCRD YAML files should be placed in the\ncrds/\ndirectory inside of a chart.\nMultiple CRDs (separated by YAML start and end markers) may be placed in the\nsame file. Helm will attempt to load\nall\nof the files in the CRD directory\ninto Kubernetes.\nCRD files\ncannot be templated\n. They must be plain YAML documents.\nWhen Helm installs a new chart, it will upload the CRDs, pause until the CRDs\nare made available by the API server, and then start the template engine, render\nthe rest of the chart, and upload it to Kubernetes. Because of this ordering,\nCRD information is available in the\n.Capabilities\nobject in Helm templates,\nand Helm templates may create new instances of objects that were declared in\nCRDs.\nFor example, if your chart had a CRD for\nCronTab\nin the\ncrds/\ndirectory, you\nmay create instances of the\nCronTab\nkind in the\ntemplates/\ndirectory:\ncrontabs/\nChart.yaml\ncrds/\ncrontab.yaml\ntemplates/\nmycrontab.yaml\nThe\ncrontab.yaml\nfile must contain the CRD with no template directives:\nkind\n:\nCustomResourceDefinition\nmetadata\n:\nname\n:\ncrontabs.stable.example.com\nspec\n:\ngroup\n:\nstable.example.com\nversions\n:\n-\nname\n:\nv1\nserved\n:\ntrue\nstorage\n:\ntrue\nscope\n:\nNamespaced\nnames\n:\nplural\n:\ncrontabs\nsingular\n:\ncrontab\nkind\n:\nCronTab\nThen the template\nmycrontab.yaml\nmay create a new\nCronTab\n(using templates\nas usual):\napiVersion\n:\nstable.example.com\nkind\n:\nCronTab\nmetadata\n:\nname\n:\n{\n{\n.Values.name\n}\n}\nspec\n:\n# ...\nHelm will make sure that the\nCronTab\nkind has been installed and is available\nfrom the Kubernetes API server before it proceeds installing the things in\ntemplates/\n.\nLimitations on CRDs\nâ€‹\nUnlike most objects in Kubernetes, CRDs are installed globally. For that reason,\nHelm takes a very cautious approach in managing CRDs. CRDs are subject to the\nfollowing limitations:\nCRDs are never reinstalled. If Helm determines that the CRDs in the\ncrds/\ndirectory are already present (regardless of version), Helm will not attempt\nto install or upgrade.\nCRDs are never installed on upgrade or rollback. Helm will only create CRDs on\ninstallation operations.\nCRDs are never deleted. Deleting a CRD automatically deletes all of the CRD's\ncontents across all namespaces in the cluster. Consequently, Helm will not\ndelete CRDs.\nOperators who want to upgrade or delete CRDs are encouraged to do this manually\nand with great care.\nUsing Helm to Manage Charts\nâ€‹\nThe\nhelm\ntool has several commands for working with charts.\nIt can create a new chart for you:\n$ helm create mychart\nCreated mychart/\nOnce you have edited a chart,\nhelm\ncan package it into a chart archive for\nyou:\n$ helm package mychart\nArchived mychart-0.1.-.tgz\nYou can also use\nhelm\nto help you find issues with your chart's formatting or\ninformation:\n$ helm lint mychart\nNo issues found\nChart Repositories\nâ€‹\nA\nchart repository\nis an HTTP server that houses one or more packaged charts.\nWhile\nhelm\ncan be used to manage local chart directories, when it comes to\nsharing charts, the preferred mechanism is a chart repository.\nAny HTTP server that can serve YAML files and tar files and can answer GET\nrequests can be used as a repository server. The Helm team has tested some\nservers, including Google Cloud Storage with website mode enabled, and S3 with\nwebsite mode enabled.\nA repository is characterized primarily by the presence of a special file called\nindex.yaml\nthat has a list of all of the packages supplied by the repository,\ntogether with metadata that allows retrieving and verifying those packages.\nOn the client side, repositories are managed with the\nhelm repo\ncommands.\nHowever, Helm does not provide tools for uploading charts to remote repository\nservers. This is because doing so would add substantial requirements to an\nimplementing server, and thus raise the barrier for setting up a repository.\nChart Starter Packs\nâ€‹\nThe\nhelm create\ncommand takes an optional\n--starter\noption that lets you\nspecify a \"starter chart\". Also, the starter option has a short alias\n-p\n.\nExamples of usage:\nhelm create my-chart --starter starter-name\nhelm create my-chart -p starter-name\nhelm create my-chart -p /absolute/path/to/starter-name\nStarters are just regular charts, but are located in\n$XDG_DATA_HOME/helm/starters\n. As a chart developer, you may author charts that\nare specifically designed to be used as starters. Such charts should be designed\nwith the following considerations in mind:\nThe\nChart.yaml\nwill be overwritten by the generator.\nUsers will expect to modify such a chart's contents, so documentation should\nindicate how users can do so.\nAll occurrences of\n<CHARTNAME>\nwill be replaced with the specified chart name so that starter charts can be used as templates, except for some variable files. For example, if you use custom files in the\nvars\ndirectory or certain\nREADME.md\nfiles,\n<CHARTNAME>\nwill NOT override inside them. Additionally, the chart description is not inherited.\nCurrently the only way to add a chart to\n$XDG_DATA_HOME/helm/starters\nis to\nmanually copy it there. In your chart's documentation, you may want to explain\nthat process.\nThe Chart File Structure\nThe Chart.yaml File\nCharts and Versioning\nThe\napiVersion\nField\nThe\nappVersion\nField\nThe\nkubeVersion\nField\nDeprecating Charts\nChart Types\nChart LICENSE, README and NOTES\nChart Dependencies\nManaging Dependencies with the\ndependencies\nfield\nManaging Dependencies manually via the\ncharts/\ndirectory\nOperational aspects of using dependencies\nTemplates and Values\nTemplate Files\nPredefined Values\nValues files\nScope, Dependencies, and Values\nSchema Files\nReferences\nCustom Resource Definitions (CRDs)\nLimitations on CRDs\nUsing Helm to Manage Charts\nChart Repositories\nChart Starter Packs", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://helm.sh/docs/topics/charts/"}}
{"text": "Chart Development Tips and Tricks | Helm\nSkip to main content\nðŸŽ‰ Helm v4.0.0 is out! See the\nHelm 4 Overview\nfor details!\nVersion: 4.0.0\nOn this page\nChart Development Tips and Tricks\nThis guide covers some of the tips and tricks Helm chart developers have learned\nwhile building production-quality charts.\nKnow Your Template Functions\nâ€‹\nHelm uses\nGo templates\nfor templating your\nresource files. While Go ships several built-in functions, we have added many\nothers.\nFirst, we added all of the functions in the\nSprig\nlibrary\n, except\nenv\nand\nexpandenv\n, for security reasons.\nWe also added two special template functions:\ninclude\nand\nrequired\n. The\ninclude\nfunction allows you to bring in another template, and then pass the\nresults to other template functions. The\nrequired\nfunction allows you to declare\na particular values entry as required for template rendering. If the value is empty,\nthe template rendering will fail with a user submitted error message.\nUsing the 'include' Function\nâ€‹\nGo provides a way of including one template in another using a built-in\ntemplate\ndirective. However, the built-in function cannot be used in Go\ntemplate pipelines.\nTo make it possible to include a template, and then perform an operation on that\ntemplate's output, Helm has a special\ninclude\nfunction:\n{{ include \"toYaml\" $value | indent 2 }}\nThe above includes a template called\ntoYaml\n, passes it\n$value\n, and then\npasses the output of that template to the\nindent\nfunction.\nBecause YAML ascribes significance to indentation levels and whitespace, this is\none great way to include snippets of code, but handle indentation in a relevant\ncontext.\nThis template snippet includes a template called\nmytpl\n, then\nlowercases the result, then wraps that in double quotes.\nvalue\n:\n{\n{\ninclude \"mytpl\" .\n|\nlower\n|\nquote\n}\n}\nUsing the 'required' function\nâ€‹\nGo provides a\nmissingkey\ntemplate option to control behavior when a map is indexed\nwith a key that's not present in the map. There may be situations where a chart\ndeveloper wants to enforce this behavior for select values in the\nvalues.yaml\nfile.\nThe\nrequired\nfunction gives developers the ability to declare a value entry as\nrequired for template rendering. If the entry is empty in\nvalues.yaml\n, the\ntemplate will not render and will return an error message supplied by the\ndeveloper.\nFor example:\n{{ required \"A valid foo is required!\" .Values.foo }}\nThe above will render the template when\n.Values.foo\nis defined, but will fail\nto render and exit when\n.Values.foo\nis undefined.\nThe following example of the\nrequired\nfunction declares an entry for\n.Values.who\nis required, and will print an error message when that entry is\nmissing:\nvalue\n:\n{\n{\nrequired \"A valid .Values.who entry required\n!\n\" .Values.who\n}\n}\nQuote Strings, Don't Quote Integers\nâ€‹\nWhen you are working with string data, you are always safer quoting the strings\nthan leaving them as bare words:\nname\n:\n{\n{\n.Values.MyName\n|\nquote\n}\n}\nBut when working with integers\ndo not quote the values.\nThat can, in many\ncases, cause parsing errors inside of Kubernetes.\nport\n:\n{\n{\n.Values.Port\n}\n}\nThis remark does not apply to env variables values which are expected to be\nstring, even if they represent integers:\nenv\n:\n-\nname\n:\nHOST\nvalue\n:\n\"http://host\"\n-\nname\n:\nPORT\nvalue\n:\n\"1234\"\nUsing the 'tpl' Function\nâ€‹\nThe\ntpl\nfunction allows developers to evaluate strings as templates inside a\ntemplate. This is useful to pass a template string as a value to a chart or\nrender external configuration files. Syntax:\n{{ tpl TEMPLATE_STRING VALUES }}\nExamples:\n# values\ntemplate\n:\n\"{{ .Values.name }}\"\nname\n:\n\"Tom\"\n# template\n{\n{\ntpl .Values.template .\n}\n}\n# output\nTom\nRendering an external configuration file:\n# external configuration file conf/app.conf\nfirstName=\n{\n{\n.Values.firstName\n}\n}\nlastName=\n{\n{\n.Values.lastName\n}\n}\n# values\nfirstName\n:\nPeter\nlastName\n:\nParker\n# template\n{\n{\ntpl (.Files.Get \"conf/app.conf\") .\n}\n}\n# output\nfirstName=Peter\nlastName=Parker\nCreating Image Pull Secrets\nâ€‹\nImage pull secrets are essentially a combination of\nregistry\n,\nusername\n, and\npassword\n.  You may need them in an application you are deploying, but to\ncreate them requires running\nbase64\na couple of times.  We can write a helper\ntemplate to compose the Docker configuration file for use as the Secret's\npayload.  Here is an example:\nFirst, assume that the credentials are defined in the\nvalues.yaml\nfile like\nso:\nimageCredentials\n:\nregistry\n:\nquay.io\nusername\n:\nsomeone\npassword\n:\nsillyness\nemail\n:\nsomeone@host.com\nWe then define our helper template as follows:\n{{- define \"imagePullSecret\" }}\n{{- with .Values.imageCredentials }}\n{{- printf \"{\\\"auths\\\":{\\\"%s\\\":{\\\"username\\\":\\\"%s\\\",\\\"password\\\":\\\"%s\\\",\\\"email\\\":\\\"%s\\\",\\\"auth\\\":\\\"%s\\\"}}}\" .registry .username .password .email (printf \"%s:%s\" .username .password | b64enc) | b64enc }}\n{{- end }}\n{{- end }}\nFinally, we use the helper template in a larger template to create the Secret\nmanifest:\napiVersion\n:\nv1\nkind\n:\nSecret\nmetadata\n:\nname\n:\nmyregistrykey\ntype\n:\nkubernetes.io/dockerconfigjson\ndata\n:\n.dockerconfigjson\n:\n{\n{\ntemplate \"imagePullSecret\" .\n}\n}\nAutomatically Roll Deployments\nâ€‹\nOften times ConfigMaps or Secrets are injected as configuration files in\ncontainers or there are other external dependency changes that require rolling\npods. Depending on the application a restart may be required should those be\nupdated with a subsequent\nhelm upgrade\n, but if the deployment spec itself\ndidn't change the application keeps running with the old configuration resulting\nin an inconsistent deployment.\nThe\nsha256sum\nfunction can be used to ensure a deployment's annotation section\nis updated if another file changes:\nkind\n:\nDeployment\nspec\n:\ntemplate\n:\nmetadata\n:\nannotations\n:\nchecksum/config\n:\n{\n{\ninclude (print $.Template.BasePath \"/configmap.yaml\") .\n|\nsha256sum\n}\n}\n[\n...\n]\nNOTE: If you're adding this to a library chart you won't be able to access your\nfile in\n$.Template.BasePath\n. Instead you can reference your definition with\n{{ include (\"mylibchart.configmap\") . | sha256sum }}\n.\nIn the event you always want to roll your deployment, you can use a similar\nannotation step as above, instead replacing with a random string so it always\nchanges and causes the deployment to roll:\nkind\n:\nDeployment\nspec\n:\ntemplate\n:\nmetadata\n:\nannotations\n:\nrollme\n:\n{\n{\nrandAlphaNum 5\n|\nquote\n}\n}\n[\n...\n]\nEach invocation of the template function will generate a unique random string.\nThis means that if it's necessary to sync the random strings used by multiple\nresources, all relevant resources will need to be in the same template file.\nBoth of these methods allow your Deployment to leverage the built in update\nstrategy logic to avoid taking downtime.\nNOTE: In the past we recommended using the\n--recreate-pods\nflag as another\noption. This flag has been marked as deprecated in Helm 3 in favor of the more\ndeclarative method above.\nTell Helm Not To Uninstall a Resource\nâ€‹\nSometimes there are resources that should not be uninstalled when Helm runs a\nhelm uninstall\n. Chart developers can add an annotation to a resource to\nprevent it from being uninstalled.\nkind\n:\nSecret\nmetadata\n:\nannotations\n:\nhelm.sh/resource-policy\n:\nkeep\n[\n...\n]\nThe annotation\nhelm.sh/resource-policy: keep\ninstructs Helm to skip deleting\nthis resource when a helm operation (such as\nhelm uninstall\n,\nhelm upgrade\nor\nhelm rollback\n) would result in its deletion.\nHowever\n, this resource becomes\norphaned. Helm will no longer manage it in any way. This can lead to problems if\nusing\nhelm install --replace\non a release that has already been uninstalled,\nbut has kept resources.\nUsing \"Partials\" and Template Includes\nâ€‹\nSometimes you want to create some reusable parts in your chart, whether they're\nblocks or template partials. And often, it's cleaner to keep these in their own\nfiles.\nIn the\ntemplates/\ndirectory, any file that begins with an underscore(\n_\n) is\nnot expected to output a Kubernetes manifest file. So by convention, helper\ntemplates and partials are placed in a\n_helpers.tpl\nfile.\nComplex Charts with Many Dependencies\nâ€‹\nMany of the charts in the CNCF\nArtifact\nHub\nare \"building blocks\" for\ncreating more advanced applications. But charts may be used to create instances\nof large-scale applications. In such cases, a single umbrella chart may have\nmultiple subcharts, each of which functions as a piece of the whole.\nThe current best practice for composing a complex application from discrete\nparts is to create a top-level umbrella chart that exposes the global\nconfigurations, and then use the\ncharts/\nsubdirectory to embed each of the\ncomponents.\nYAML is a Superset of JSON\nâ€‹\nAccording to the YAML specification, YAML is a superset of JSON. That means that\nany valid JSON structure ought to be valid in YAML.\nThis has an advantage: Sometimes template developers may find it easier to\nexpress a datastructure with a JSON-like syntax rather than deal with YAML's\nwhitespace sensitivity.\nAs a best practice, templates should follow a YAML-like syntax\nunless\nthe JSON\nsyntax substantially reduces the risk of a formatting issue.\nBe Careful with Generating Random Values\nâ€‹\nThere are functions in Helm that allow you to generate random data,\ncryptographic keys, and so on. These are fine to use. But be aware that during\nupgrades, templates are re-executed. When a template run generates data that\ndiffers from the last run, that will trigger an update of that resource.\nInstall or Upgrade a Release with One Command\nâ€‹\nHelm provides a way to perform an install-or-upgrade as a single command. Use\nhelm upgrade\nwith the\n--install\ncommand. This will cause Helm to see if the\nrelease is already installed. If not, it will run an install. If it is, then the\nexisting release will be upgraded.\n$ helm upgrade --install <release name> --values <values file> <chart directory>\nKnow Your Template Functions\nUsing the 'include' Function\nUsing the 'required' function\nQuote Strings, Don't Quote Integers\nUsing the 'tpl' Function\nCreating Image Pull Secrets\nAutomatically Roll Deployments\nTell Helm Not To Uninstall a Resource\nUsing \"Partials\" and Template Includes\nComplex Charts with Many Dependencies\nYAML is a Superset of JSON\nBe Careful with Generating Random Values\nInstall or Upgrade a Release with One Command", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://helm.sh/docs/howto/charts_tips_and_tricks/"}}
{"text": "Deployments | Kubernetes\nDeployments\nA Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.\nA\nDeployment\nprovides declarative updates for\nPods\nand\nReplicaSets\n.\nYou describe a\ndesired state\nin a Deployment, and the Deployment\nController\nchanges the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.\nNote:\nDo not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.\nUse Case\nThe following are typical use cases for Deployments:\nCreate a Deployment to rollout a ReplicaSet\n. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.\nDeclare the new state of the Pods\nby updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created, and the Deployment gradually scales it up while scaling down the old ReplicaSet, ensuring Pods are replaced at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.\nRollback to an earlier Deployment revision\nif the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.\nScale up the Deployment to facilitate more load\n.\nPause the rollout of a Deployment\nto apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.\nUse the status of the Deployment\nas an indicator that a rollout has stuck.\nClean up older ReplicaSets\nthat you don't need anymore.\nCreating a Deployment\nThe following is an example of a Deployment. It creates a ReplicaSet to bring up three\nnginx\nPods:\ncontrollers/nginx-deployment.yaml\napiVersion\n:\napps/v1\nkind\n:\nDeployment\nmetadata\n:\nname\n:\nnginx-deployment\nlabels\n:\napp\n:\nnginx\nspec\n:\nreplicas\n:\n3\nselector\n:\nmatchLabels\n:\napp\n:\nnginx\ntemplate\n:\nmetadata\n:\nlabels\n:\napp\n:\nnginx\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:1.14.2\nports\n:\n-\ncontainerPort\n:\n80\nIn this example:\nA Deployment named\nnginx-deployment\nis created, indicated by the\n.metadata.name\nfield. This name will become the basis for the ReplicaSets\nand Pods which are created later. See\nWriting a Deployment Spec\nfor more details.\nThe Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the\n.spec.replicas\nfield.\nThe\n.spec.selector\nfield defines how the created ReplicaSet finds which Pods to manage.\nIn this case, you select a label that is defined in the Pod template (\napp: nginx\n).\nHowever, more sophisticated selection rules are possible,\nas long as the Pod template itself satisfies the rule.\nNote:\nThe\n.spec.selector.matchLabels\nfield is a map of {key,value} pairs.\nA single {key,value} in the\nmatchLabels\nmap is equivalent to an element of\nmatchExpressions\n,\nwhose\nkey\nfield is \"key\", the\noperator\nis \"In\", and the\nvalues\narray contains only \"value\".\nAll of the requirements, from both\nmatchLabels\nand\nmatchExpressions\n, must be satisfied in order to match.\nThe\n.spec.template\nfield contains the following sub-fields:\nThe Pods are labeled\napp: nginx\nusing the\n.metadata.labels\nfield.\nThe Pod template's specification, or\n.spec\nfield, indicates that\nthe Pods run one container,\nnginx\n, which runs the\nnginx\nDocker Hub\nimage at version 1.14.2.\nCreate one container and name it\nnginx\nusing the\n.spec.containers[0].name\nfield.\nBefore you begin, make sure your Kubernetes cluster is up and running.\nFollow the steps given below to create the above Deployment:\nCreate the Deployment by running the following command:\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\nRun\nkubectl get deployments\nto check if the Deployment was created.\nIf the Deployment is still being created, the output is similar to the following:\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   0/3     0            0           1s\nWhen you inspect the Deployments in your cluster, the following fields are displayed:\nNAME\nlists the names of the Deployments in the namespace.\nREADY\ndisplays how many replicas of the application are available to your users. It follows the pattern ready/desired.\nUP-TO-DATE\ndisplays the number of replicas that have been updated to achieve the desired state.\nAVAILABLE\ndisplays how many replicas of the application are available to your users.\nAGE\ndisplays the amount of time that the application has been running.\nNotice how the number of desired replicas is 3 according to\n.spec.replicas\nfield.\nTo see the Deployment rollout status, run\nkubectl rollout status deployment/nginx-deployment\n.\nThe output is similar to:\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\ndeployment \"nginx-deployment\" successfully rolled out\nRun the\nkubectl get deployments\nagain a few seconds later.\nThe output is similar to this:\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           18s\nNotice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.\nTo see the ReplicaSet (\nrs\n) created by the Deployment, run\nkubectl get rs\n. The output is similar to this:\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-75675f5897   3         3         3       18s\nReplicaSet output shows the following fields:\nNAME\nlists the names of the ReplicaSets in the namespace.\nDESIRED\ndisplays the desired number of\nreplicas\nof the application, which you define when you create the Deployment. This is the\ndesired state\n.\nCURRENT\ndisplays how many replicas are currently running.\nREADY\ndisplays how many replicas of the application are available to your users.\nAGE\ndisplays the amount of time that the application has been running.\nNotice that the name of the ReplicaSet is always formatted as\n[DEPLOYMENT-NAME]-[HASH]\n. This name will become the basis for the Pods\nwhich are created.\nThe\nHASH\nstring is the same as the\npod-template-hash\nlabel on the ReplicaSet.\nTo see the labels automatically generated for each Pod, run\nkubectl get pods --show-labels\n.\nThe output is similar to:\nNAME                                READY     STATUS    RESTARTS   AGE       LABELS\nnginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nThe created ReplicaSet ensures that there are three\nnginx\nPods.\nNote:\nYou must specify an appropriate selector and Pod template labels in a Deployment\n(in this case,\napp: nginx\n).\nDo not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.\nPod-template-hash label\nCaution:\nDo not change this label.\nThe\npod-template-hash\nlabel is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.\nThis label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the\nPodTemplate\nof the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,\nand in any existing Pods that the ReplicaSet might have.\nUpdating a Deployment\nNote:\nA Deployment's rollout is triggered if and only if the Deployment's Pod template (that is,\n.spec.template\n)\nis changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.\nFollow the steps given below to update your Deployment:\nLet's update the nginx Pods to use the\nnginx:1.16.1\nimage instead of the\nnginx:1.14.2\nimage.\nkubectl\nset\nimage deployment.v1.apps/nginx-deployment\nnginx\n=\nnginx:1.16.1\nor use the following command:\nkubectl\nset\nimage deployment/nginx-deployment\nnginx\n=\nnginx:1.16.1\nwhere\ndeployment/nginx-deployment\nindicates the Deployment,\nnginx\nindicates the Container the update will take place and\nnginx:1.16.1\nindicates the new image and its tag.\nThe output is similar to:\ndeployment.apps/nginx-deployment image updated\nAlternatively, you can\nedit\nthe Deployment and change\n.spec.template.spec.containers[0].image\nfrom\nnginx:1.14.2\nto\nnginx:1.16.1\n:\nkubectl edit deployment/nginx-deployment\nThe output is similar to:\ndeployment.apps/nginx-deployment edited\nTo see the rollout status, run:\nkubectl rollout status deployment/nginx-deployment\nThe output is similar to this:\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nor\ndeployment \"nginx-deployment\" successfully rolled out\nGet more details on your updated Deployment:\nAfter the rollout succeeds, you can view the Deployment by running\nkubectl get deployments\n.\nThe output is similar to this:\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           36s\nRun\nkubectl get rs\nto see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it\nup to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.\nkubectl get rs\nThe output is similar to this:\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       6s\nnginx-deployment-2035384211   0         0         0       36s\nRunning\nget pods\nshould now show only the new Pods:\nkubectl get pods\nThe output is similar to this:\nNAME                                READY     STATUS    RESTARTS   AGE\nnginx-deployment-1564180365-khku8   1/1       Running   0          14s\nnginx-deployment-1564180365-nacti   1/1       Running   0          14s\nnginx-deployment-1564180365-z9gth   1/1       Running   0          14s\nNext time you want to update these Pods, you only need to update the Deployment's Pod template again.\nDeployment ensures that only a certain number of Pods are down while they are being updated. By default,\nit ensures that at least 75% of the desired number of Pods are up (25% max unavailable).\nDeployment also ensures that only a certain number of Pods are created above the desired number of Pods.\nBy default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).\nFor example, if you look at the above Deployment closely, you will see that it first creates a new Pod,\nthen deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of\nnew Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.\nIt makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of\na Deployment with 4 replicas, the number of Pods would be between 3 and 5.\nGet details of your Deployment:\nkubectl describe deployments\nThe output is similar to this:\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=2\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\nEvents:\nType    Reason             Age   From                   Message\n----    ------             ----  ----                   -------\nNormal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3\nNormal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3\nNormal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0\nHere you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)\nand scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet\n(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet\nto 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.\nIt then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.\nFinally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.\nNote:\nKubernetes doesn't count terminating Pods when calculating the number of\navailableReplicas\n, which must be between\nreplicas - maxUnavailable\nand\nreplicas + maxSurge\n. As a result, you might notice that there are more Pods than\nexpected during a rollout, and that the total resources consumed by the Deployment is more than\nreplicas + maxSurge\nuntil the\nterminationGracePeriodSeconds\nof the terminating Pods expires.\nRollover (aka multiple updates in-flight)\nEach time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up\nthe desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels\nmatch\n.spec.selector\nbut whose template does not match\n.spec.template\nis scaled down. Eventually, the new\nReplicaSet is scaled to\n.spec.replicas\nand all old ReplicaSets is scaled to 0.\nIf you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet\nas per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously\n-- it will add it to its list of old ReplicaSets and start scaling it down.\nFor example, suppose you create a Deployment to create 5 replicas of\nnginx:1.14.2\n,\nbut then update the Deployment to create 5 replicas of\nnginx:1.16.1\n, when only 3\nreplicas of\nnginx:1.14.2\nhad been created. In that case, the Deployment immediately starts\nkilling the 3\nnginx:1.14.2\nPods that it had created, and starts creating\nnginx:1.16.1\nPods. It does not wait for the 5 replicas of\nnginx:1.14.2\nto be created\nbefore changing course.\nLabel selector updates\nIt is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.\nIn any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped\nall of the implications.\nNote:\nIn API version\napps/v1\n, a Deployment's label selector is immutable after it gets created.\nSelector additions require the Pod template labels in the Deployment spec to be updated with the new label too,\notherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does\nnot select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and\ncreating a new ReplicaSet.\nSelector updates changes the existing value in a selector key -- result in the same behavior as additions.\nSelector removals removes an existing key from the Deployment selector -- do not require any changes in the\nPod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the\nremoved label still exists in any existing Pods and ReplicaSets.\nRolling Back a Deployment\nSometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.\nBy default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want\n(you can change that by modifying revision history limit).\nNote:\nA Deployment's revision is created when a Deployment's rollout is triggered. This means that the\nnew revision is created if and only if the Deployment's Pod template (\n.spec.template\n) is changed,\nfor example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,\ndo not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.\nThis means that when you roll back to an earlier revision, only the Deployment's Pod template part is\nrolled back.\nSuppose that you made a typo while updating the Deployment, by putting the image name as\nnginx:1.161\ninstead of\nnginx:1.16.1\n:\nkubectl\nset\nimage deployment/nginx-deployment\nnginx\n=\nnginx:1.161\nThe output is similar to this:\ndeployment.apps/nginx-deployment image updated\nThe rollout gets stuck. You can verify it by checking the rollout status:\nkubectl rollout status deployment/nginx-deployment\nThe output is similar to this:\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nPress Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,\nread more here\n.\nYou see that the number of old replicas (adding the replica count from\nnginx-deployment-1564180365\nand\nnginx-deployment-2035384211\n) is 3, and the number of\nnew replicas (from\nnginx-deployment-3066724191\n) is 1.\nkubectl get rs\nThe output is similar to this:\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       25s\nnginx-deployment-2035384211   0         0         0       36s\nnginx-deployment-3066724191   1         1         0       6s\nLooking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.\nkubectl get pods\nThe output is similar to this:\nNAME                                READY     STATUS             RESTARTS   AGE\nnginx-deployment-1564180365-70iae   1/1       Running            0          25s\nnginx-deployment-1564180365-jbqqo   1/1       Running            0          25s\nnginx-deployment-1564180365-hysrc   1/1       Running            0          25s\nnginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s\nNote:\nThe Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (\nmaxUnavailable\nspecifically) that you have specified. Kubernetes by default sets the value to 25%.\nGet the description of the Deployment:\nkubectl describe deployment\nThe output is similar to this:\nName:           nginx-deployment\nNamespace:      default\nCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\nLabels:         app=nginx\nSelector:       app=nginx\nReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\nStrategyType:       RollingUpdate\nMinReadySeconds:    0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.161\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    ReplicaSetUpdated\nOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\nNewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\nEvents:\nFirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message\n--------- --------    -----   ----                    -------------   --------    ------              -------\n1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1\nTo fix this, you need to rollback to a previous revision of Deployment that is stable.\nChecking Rollout History of a Deployment\nFollow the steps given below to check the rollout history:\nFirst, check the revisions of this Deployment:\nkubectl rollout\nhistory\ndeployment/nginx-deployment\nThe output is similar to this:\ndeployments \"nginx-deployment\"\nREVISION    CHANGE-CAUSE\n1           <none>\n2           <none>\n3           <none>\nCHANGE-CAUSE\nis copied from the Deployment annotation\nkubernetes.io/change-cause\nto its revisions upon creation. You can specify the\nCHANGE-CAUSE\nmessage by:\nAnnotating the Deployment with\nkubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"\nManually editing the manifest of the resource.\nUsing tooling that sets the annotation automatically.\nNote:\nIn older versions of Kubernetes, you could use the\n--record\nflag with kubectl commands to automatically populate the\nCHANGE-CAUSE\nfield. This flag is deprecated and will be removed in a future release.\nTo see the details of each revision, run:\nkubectl rollout\nhistory\ndeployment/nginx-deployment --revision\n=\n2\nThe output is similar to this:\ndeployments \"nginx-deployment\" revision 2\nLabels:       app=nginx\npod-template-hash=1159050644\nContainers:\nnginx:\nImage:      nginx:1.16.1\nPort:       80/TCP\nQoS Tier:\ncpu:      BestEffort\nmemory:   BestEffort\nEnvironment Variables:      <none>\nNo volumes.\nRolling Back to a Previous Revision\nFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.\nNow you've decided to undo the current rollout and rollback to the previous revision:\nkubectl rollout undo deployment/nginx-deployment\nThe output is similar to this:\ndeployment.apps/nginx-deployment rolled back\nAlternatively, you can rollback to a specific revision by specifying it with\n--to-revision\n:\nkubectl rollout undo deployment/nginx-deployment --to-revision\n=\n2\nThe output is similar to this:\ndeployment.apps/nginx-deployment rolled back\nFor more details about rollout related commands, read\nkubectl rollout\n.\nThe Deployment is now rolled back to a previous stable revision. As you can see, a\nDeploymentRollback\nevent\nfor rolling back to revision 2 is generated from Deployment controller.\nCheck if the rollback was successful and the Deployment is running as expected, run:\nkubectl get deployment nginx-deployment\nThe output is similar to this:\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           30m\nGet the description of the Deployment:\nkubectl describe deployment nginx-deployment\nThe output is similar to this:\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=4\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)\nEvents:\nType    Reason              Age   From                   Message\n----    ------              ----  ----                   -------\nNormal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1\nNormal  DeploymentRollback  15s   deployment-controller  Rolled back deployment \"nginx-deployment\" to revision 2\nNormal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0\nScaling a Deployment\nYou can scale a Deployment by using the following command:\nkubectl scale deployment/nginx-deployment --replicas\n=\n10\nThe output is similar to this:\ndeployment.apps/nginx-deployment scaled\nAssuming\nhorizontal Pod autoscaling\nis enabled\nin your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of\nPods you want to run based on the CPU utilization of your existing Pods.\nkubectl autoscale deployment/nginx-deployment --min\n=\n10\n--max\n=\n15\n--cpu-percent\n=\n80\nThe output is similar to this:\ndeployment.apps/nginx-deployment scaled\nProportional scaling\nRollingUpdate Deployments support running multiple versions of an application at the same time. When you\nor an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress\nor paused), the Deployment controller balances the additional replicas in the existing active\nReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called\nproportional scaling\n.\nFor example, you are running a Deployment with 10 replicas,\nmaxSurge\n=3, and\nmaxUnavailable\n=2.\nEnsure that the 10 replicas in your Deployment are running.\nkubectl get deploy\nThe output is similar to this:\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     10        10        10           10          50s\nYou update to a new image which happens to be unresolvable from inside the cluster.\nkubectl\nset\nimage deployment/nginx-deployment\nnginx\n=\nnginx:sometag\nThe output is similar to this:\ndeployment.apps/nginx-deployment image updated\nThe image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the\nmaxUnavailable\nrequirement that you mentioned above. Check out the rollout status:\nkubectl get rs\nThe output is similar to this:\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   5         5         0         9s\nnginx-deployment-618515232    8         8         8         1m\nThen a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas\nto 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using\nproportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you\nspread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the\nmost replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the\nReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.\nIn our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the\nnew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming\nthe new replicas become healthy. To confirm this, run:\nkubectl get deploy\nThe output is similar to this:\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     15        18        7            8           7m\nThe rollout status confirms how the replicas were added to each ReplicaSet.\nkubectl get rs\nThe output is similar to this:\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   7         7         0         7m\nnginx-deployment-618515232    11        11        11        7m\nPausing and Resuming a rollout of a Deployment\nWhen you update a Deployment, or plan to, you can pause rollouts\nfor that Deployment before you trigger one or more updates. When\nyou're ready to apply those changes, you resume rollouts for the\nDeployment. This approach allows you to\napply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.\nFor example, with a Deployment that was created:\nGet the Deployment details:\nkubectl get deploy\nThe output is similar to this:\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx     3         3         3            3           1m\nGet the rollout status:\nkubectl get rs\nThe output is similar to this:\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         1m\nPause by running the following command:\nkubectl rollout pause deployment/nginx-deployment\nThe output is similar to this:\ndeployment.apps/nginx-deployment paused\nThen update the image of the Deployment:\nkubectl\nset\nimage deployment/nginx-deployment\nnginx\n=\nnginx:1.16.1\nThe output is similar to this:\ndeployment.apps/nginx-deployment image updated\nNotice that no new rollout started:\nkubectl rollout\nhistory\ndeployment/nginx-deployment\nThe output is similar to this:\ndeployments \"nginx\"\nREVISION  CHANGE-CAUSE\n1   <none>\nGet the rollout status to verify that the existing ReplicaSet has not changed:\nkubectl get rs\nThe output is similar to this:\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         2m\nYou can make as many updates as you wish, for example, update the resources that will be used:\nkubectl\nset\nresources deployment/nginx-deployment -c\n=\nnginx --limits\n=\ncpu\n=\n200m,memory\n=\n512Mi\nThe output is similar to this:\ndeployment.apps/nginx-deployment resource requirements updated\nThe initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to\nthe Deployment will not have any effect as long as the Deployment rollout is paused.\nEventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:\nkubectl rollout resume deployment/nginx-deployment\nThe output is similar to this:\ndeployment.apps/nginx-deployment resumed\nWatch\nthe status of the rollout until it's done.\nkubectl get rs --watch\nThe output is similar to this:\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   2         2         2         2m\nnginx-3926361531   2         2         0         6s\nnginx-3926361531   2         2         1         18s\nnginx-2142116321   1         2         2         2m\nnginx-2142116321   1         2         2         2m\nnginx-3926361531   3         2         1         18s\nnginx-3926361531   3         2         1         18s\nnginx-2142116321   1         1         1         2m\nnginx-3926361531   3         3         1         18s\nnginx-3926361531   3         3         2         19s\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         20s\nGet the status of the latest rollout:\nkubectl get rs\nThe output is similar to this:\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         28s\nNote:\nYou cannot rollback a paused Deployment until you resume it.\nDeployment status\nA Deployment enters various states during its lifecycle. It can be\nprogressing\nwhile\nrolling out a new ReplicaSet, it can be\ncomplete\n, or it can\nfail to progress\n.\nProgressing Deployment\nKubernetes marks a Deployment as\nprogressing\nwhen one of the following tasks is performed:\nThe Deployment creates a new ReplicaSet.\nThe Deployment is scaling up its newest ReplicaSet.\nThe Deployment is scaling down its older ReplicaSet(s).\nNew Pods become ready or available (ready for at least\nMinReadySeconds\n).\nWhen the rollout becomes â€œprogressingâ€, the Deployment controller adds a condition with the following\nattributes to the Deployment's\n.status.conditions\n:\ntype: Progressing\nstatus: \"True\"\nreason: NewReplicaSetCreated\n|\nreason: FoundNewReplicaSet\n|\nreason: ReplicaSetUpdated\nYou can monitor the progress for a Deployment by using\nkubectl rollout status\n.\nComplete Deployment\nKubernetes marks a Deployment as\ncomplete\nwhen it has the following characteristics:\nAll of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any\nupdates you've requested have been completed.\nAll of the replicas associated with the Deployment are available.\nNo old replicas for the Deployment are running.\nWhen the rollout becomes â€œcompleteâ€, the Deployment controller sets a condition with the following\nattributes to the Deployment's\n.status.conditions\n:\ntype: Progressing\nstatus: \"True\"\nreason: NewReplicaSetAvailable\nThis\nProgressing\ncondition will retain a status value of\n\"True\"\nuntil a new rollout\nis initiated. The condition holds even when availability of replicas changes (which\ndoes instead affect the\nAvailable\ncondition).\nYou can check if a Deployment has completed by using\nkubectl rollout status\n. If the rollout completed\nsuccessfully,\nkubectl rollout status\nreturns a zero exit code.\nkubectl rollout status deployment/nginx-deployment\nThe output is similar to this:\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\ndeployment \"nginx-deployment\" successfully rolled out\nand the exit status from\nkubectl rollout\nis 0 (success):\necho\n$?\n0\nFailed Deployment\nYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur\ndue to some of the following factors:\nInsufficient quota\nReadiness probe failures\nImage pull errors\nInsufficient permissions\nLimit ranges\nApplication runtime misconfiguration\nOne way you can detect this condition is to specify a deadline parameter in your Deployment spec:\n(\n.spec.progressDeadlineSeconds\n).\n.spec.progressDeadlineSeconds\ndenotes the\nnumber of seconds the Deployment controller waits before indicating (in the Deployment status) that the\nDeployment progress has stalled.\nThe following\nkubectl\ncommand sets the spec with\nprogressDeadlineSeconds\nto make the controller report\nlack of progress of a rollout for a Deployment after 10 minutes:\nkubectl patch deployment/nginx-deployment -p\n'{\"spec\":{\"progressDeadlineSeconds\":600}}'\nThe output is similar to this:\ndeployment.apps/nginx-deployment patched\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following\nattributes to the Deployment's\n.status.conditions\n:\ntype: Progressing\nstatus: \"False\"\nreason: ProgressDeadlineExceeded\nThis condition can also fail early and is then set to status value of\n\"False\"\ndue to reasons as\nReplicaSetCreateError\n.\nAlso, the deadline is not taken into account anymore once the Deployment rollout completes.\nSee the\nKubernetes API conventions\nfor more information on status conditions.\nNote:\nKubernetes takes no action on a stalled Deployment other than to report a status condition with\nreason: ProgressDeadlineExceeded\n. Higher level orchestrators can take advantage of it and act accordingly, for\nexample, rollback the Deployment to its previous version.\nNote:\nIf you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.\nYou can safely pause a Deployment rollout in the middle of a rollout and resume without triggering\nthe condition for exceeding the deadline.\nYou may experience transient errors with your Deployments, either due to a low timeout that you have set or\ndue to any other kind of error that can be treated as transient. For example, let's suppose you have\ninsufficient quota. If you describe the Deployment you will notice the following section:\nkubectl describe deployment nginx-deployment\nThe output is similar to this:\n<...>\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     True    ReplicaSetUpdated\nReplicaFailure  True    FailedCreate\n<...>\nIf you run\nkubectl get deployment nginx-deployment -o yaml\n, the Deployment status is similar to this:\nstatus:\navailableReplicas: 2\nconditions:\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: Replica set \"nginx-deployment-4262182780\" is progressing.\nreason: ReplicaSetUpdated\nstatus: \"True\"\ntype: Progressing\n- lastTransitionTime: 2016-10-04T12:25:42Z\nlastUpdateTime: 2016-10-04T12:25:42Z\nmessage: Deployment has minimum availability.\nreason: MinimumReplicasAvailable\nstatus: \"True\"\ntype: Available\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\nobject-counts, requested: pods=1, used: pods=3, limited: pods=2'\nreason: FailedCreate\nstatus: \"True\"\ntype: ReplicaFailure\nobservedGeneration: 3\nreplicas: 2\nunavailableReplicas: 2\nEventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the\nreason for the Progressing condition:\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     False   ProgressDeadlineExceeded\nReplicaFailure  True    FailedCreate\nYou can address an issue of insufficient quota by scaling down your Deployment, by scaling down other\ncontrollers you may be running, or by increasing quota in your namespace. If you satisfy the quota\nconditions and the Deployment controller then completes the Deployment rollout, you'll see the\nDeployment's status update with a successful condition (\nstatus: \"True\"\nand\nreason: NewReplicaSetAvailable\n).\nConditions:\nType          Status  Reason\n----          ------  ------\nAvailable     True    MinimumReplicasAvailable\nProgressing   True    NewReplicaSetAvailable\ntype: Available\nwith\nstatus: \"True\"\nmeans that your Deployment has minimum availability. Minimum availability is dictated\nby the parameters specified in the deployment strategy.\ntype: Progressing\nwith\nstatus: \"True\"\nmeans that your Deployment\nis either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum\nrequired new replicas are available (see the Reason of the condition for the particulars - in our case\nreason: NewReplicaSetAvailable\nmeans that the Deployment is complete).\nYou can check if a Deployment has failed to progress by using\nkubectl rollout status\n.\nkubectl rollout status\nreturns a non-zero exit code if the Deployment has exceeded the progression deadline.\nkubectl rollout status deployment/nginx-deployment\nThe output is similar to this:\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nerror: deployment \"nginx\" exceeded its progress deadline\nand the exit status from\nkubectl rollout\nis 1 (indicating an error):\necho\n$?\n1\nOperating on a failed deployment\nAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back\nto a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.\nClean up Policy\nYou can set\n.spec.revisionHistoryLimit\nfield in a Deployment to specify how many old ReplicaSets for\nthis Deployment you want to retain. The rest will be garbage-collected in the background. By default,\nit is 10.\nNote:\nExplicitly setting this field to 0, will result in cleaning up all the history of your Deployment\nthus that Deployment will not be able to roll back.\nThe cleanup only starts\nafter\na Deployment reaches a\ncomplete state\n.\nIf you set\n.spec.revisionHistoryLimit\nto 0, any rollout nonetheless triggers creation of a new\nReplicaSet before Kubernetes removes the old one.\nEven with a non-zero revision history limit, you can have more ReplicaSets than the limit\nyou configure. For example, if pods are crash looping, and there are multiple rolling updates\nevents triggered over time, you might end up with more ReplicaSets than the\n.spec.revisionHistoryLimit\nbecause the Deployment never reaches a complete state.\nCanary Deployment\nIf you want to roll out releases to a subset of users or servers using the Deployment, you\ncan create multiple Deployments, one for each release, following the canary pattern described in\nmanaging resources\n.\nWriting a Deployment Spec\nAs with all other Kubernetes configs, a Deployment needs\n.apiVersion\n,\n.kind\n, and\n.metadata\nfields.\nFor general information about working with config files, see\ndeploying applications\n,\nconfiguring containers, and\nusing kubectl to manage resources\ndocuments.\nWhen the control plane creates new Pods for a Deployment, the\n.metadata.name\nof the\nDeployment is part of the basis for naming those Pods. The name of a Deployment must be a valid\nDNS subdomain\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\nDNS label\n.\nA Deployment also needs a\n.spec\nsection\n.\nPod Template\nThe\n.spec.template\nand\n.spec.selector\nare the only required fields of the\n.spec\n.\nThe\n.spec.template\nis a\nPod template\n. It has exactly the same schema as a\nPod\n, except it is nested and does not have an\napiVersion\nor\nkind\n.\nIn addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See\nselector\n.\nOnly a\n.spec.template.spec.restartPolicy\nequal to\nAlways\nis\nallowed, which is the default if not specified.\nReplicas\n.spec.replicas\nis an optional field that specifies the number of desired Pods. It defaults to 1.\nShould you manually scale a Deployment, example via\nkubectl scale deployment deployment --replicas=X\n, and then you update that Deployment based on a manifest\n(for example: by running\nkubectl apply -f deployment.yaml\n),\nthen applying that manifest overwrites the manual scaling that you previously did.\nIf a\nHorizontalPodAutoscaler\n(or any\nsimilar API for horizontal scaling) is managing scaling for a Deployment, don't set\n.spec.replicas\n.\nInstead, allow the Kubernetes\ncontrol plane\nto manage the\n.spec.replicas\nfield automatically.\nSelector\n.spec.selector\nis a required field that specifies a\nlabel selector\nfor the Pods targeted by this Deployment.\n.spec.selector\nmust match\n.spec.template.metadata.labels\n, or it will be rejected by the API.\nIn API version\napps/v1\n,\n.spec.selector\nand\n.metadata.labels\ndo not default to\n.spec.template.metadata.labels\nif not set. So they must be set explicitly. Also note that\n.spec.selector\nis immutable after creation of the Deployment in\napps/v1\n.\nA Deployment may terminate Pods whose labels match the selector if their template is different\nfrom\n.spec.template\nor if the total number of such Pods exceeds\n.spec.replicas\n. It brings up new\nPods with\n.spec.template\nif the number of Pods is less than the desired number.\nNote:\nYou should not create other Pods whose labels match this selector, either directly, by creating\nanother Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you\ndo so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.\nIf you have multiple controllers that have overlapping selectors, the controllers will fight with each\nother and won't behave correctly.\nStrategy\n.spec.strategy\nspecifies the strategy used to replace old Pods by new ones.\n.spec.strategy.type\ncan be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is\nthe default value.\nRecreate Deployment\nAll existing Pods are killed before new ones are created when\n.spec.strategy.type==Recreate\n.\nNote:\nThis will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods\nof the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new\nrevision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the\nreplacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an\n\"at most\" guarantee for your Pods, you should consider using a\nStatefulSet\n.\nRolling Update Deployment\nThe Deployment updates Pods in a rolling update\nfashion (gradually scale down the old ReplicaSets and scale up the new one) when\n.spec.strategy.type==RollingUpdate\n. You can specify\nmaxUnavailable\nand\nmaxSurge\nto control\nthe rolling update process.\nMax Unavailable\n.spec.strategy.rollingUpdate.maxUnavailable\nis an optional field that specifies the maximum number\nof Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)\nor a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by\nrounding down. The value cannot be 0 if\n.spec.strategy.rollingUpdate.maxSurge\nis 0. The default value is 25%.\nFor example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired\nPods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled\ndown further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available\nat all times during the update is at least 70% of the desired Pods.\nMax Surge\n.spec.strategy.rollingUpdate.maxSurge\nis an optional field that specifies the maximum number of Pods\nthat can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a\npercentage of desired Pods (for example, 10%). The value cannot be 0 if\nmaxUnavailable\nis 0. The absolute number\nis calculated from the percentage by rounding up. The default value is 25%.\nFor example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the\nrolling update starts, such that the total number of old and new Pods does not exceed 130% of desired\nPods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the\ntotal number of Pods running at any time during the update is at most 130% of desired Pods.\nHere are some Rolling Update Deployment examples that use the\nmaxUnavailable\nand\nmaxSurge\n:\nMax Unavailable\nMax Surge\nHybrid\napiVersion\n:\napps/v1\nkind\n:\nDeployment\nmetadata\n:\nname\n:\nnginx-deployment\nlabels\n:\napp\n:\nnginx\nspec\n:\nreplicas\n:\n3\nselector\n:\nmatchLabels\n:\napp\n:\nnginx\ntemplate\n:\nmetadata\n:\nlabels\n:\napp\n:\nnginx\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:1.14.2\nports\n:\n-\ncontainerPort\n:\n80\nstrategy\n:\ntype\n:\nRollingUpdate\nrollingUpdate\n:\nmaxUnavailable\n:\n1\napiVersion\n:\napps/v1\nkind\n:\nDeployment\nmetadata\n:\nname\n:\nnginx-deployment\nlabels\n:\napp\n:\nnginx\nspec\n:\nreplicas\n:\n3\nselector\n:\nmatchLabels\n:\napp\n:\nnginx\ntemplate\n:\nmetadata\n:\nlabels\n:\napp\n:\nnginx\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:1.14.2\nports\n:\n-\ncontainerPort\n:\n80\nstrategy\n:\ntype\n:\nRollingUpdate\nrollingUpdate\n:\nmaxSurge\n:\n1\napiVersion\n:\napps/v1\nkind\n:\nDeployment\nmetadata\n:\nname\n:\nnginx-deployment\nlabels\n:\napp\n:\nnginx\nspec\n:\nreplicas\n:\n3\nselector\n:\nmatchLabels\n:\napp\n:\nnginx\ntemplate\n:\nmetadata\n:\nlabels\n:\napp\n:\nnginx\nspec\n:\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nnginx:1.14.2\nports\n:\n-\ncontainerPort\n:\n80\nstrategy\n:\ntype\n:\nRollingUpdate\nrollingUpdate\n:\nmaxSurge\n:\n1\nmaxUnavailable\n:\n1\nProgress Deadline Seconds\n.spec.progressDeadlineSeconds\nis an optional field that specifies the number of seconds you want\nto wait for your Deployment to progress before the system reports back that the Deployment has\nfailed progressing\n- surfaced as a condition with\ntype: Progressing\n,\nstatus: \"False\"\n.\nand\nreason: ProgressDeadlineExceeded\nin the status of the resource. The Deployment controller will keep\nretrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment\ncontroller will roll back a Deployment as soon as it observes such a condition.\nIf specified, this field needs to be greater than\n.spec.minReadySeconds\n.\nMin Ready Seconds\n.spec.minReadySeconds\nis an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be ready without any of its containers crashing, for it to be considered available.\nThis defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see\nContainer Probes\n.\nTerminating Pods\nFEATURE STATE:\nKubernetes v1.33 [alpha]\n(disabled by default)\nYou can enable this feature by setting the\nDeploymentReplicaSetTerminatingReplicas\nfeature gate\non the\nAPI server\nand on the\nkube-controller-manager\nPods that become terminating due to deletion or scale down may take a long time to terminate, and may consume\nadditional resources during that period. As a result, the total number of all pods can temporarily exceed\n.spec.replicas\n. Terminating pods can be tracked using the\n.status.terminatingReplicas\nfield of the Deployment.\nRevision History Limit\nA Deployment's revision history is stored in the ReplicaSets it controls.\n.spec.revisionHistoryLimit\nis an optional field that specifies the number of old ReplicaSets to retain\nto allow rollback. These old ReplicaSets consume resources in\netcd\nand crowd the output of\nkubectl get rs\n. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.\nMore specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.\nIn this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.\nPaused\n.spec.paused\nis an optional boolean field for pausing and resuming a Deployment. The only difference between\na paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused\nDeployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when\nit is created.\nWhat's next\nLearn more about\nPods\n.\nRun a stateless application using a Deployment\n.\nRead the\nDeployment\nto understand the Deployment API.\nRead about\nPodDisruptionBudget\nand how\nyou can use it to manage application availability during disruptions.\nUse kubectl to\ncreate a Deployment\n.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 30, 2025 at 10:31 AM PST:\nDocument --record flag deprecation and tooling support (bd00633eb4)\nDeployment API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"}}
{"text": "StatefulSets | Kubernetes\nStatefulSets\nA StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing applications that need persistent storage or a stable, unique network identity.\nStatefulSet is the workload API object used to manage stateful applications.\nManages the deployment and scaling of a set of\nPods\n,\nand provides guarantees about the ordering and uniqueness\nof these Pods.\nLike a\nDeployment\n, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.\nIf you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.\nUsing StatefulSets\nStatefulSets are valuable for applications that require one or more of the\nfollowing:\nStable, unique network identifiers.\nStable, persistent storage.\nOrdered, graceful deployment and scaling.\nOrdered, automated rolling updates.\nIn the above, stable is synonymous with persistence across Pod (re)scheduling.\nIf an application doesn't require any stable identifiers or ordered deployment,\ndeletion, or scaling, you should deploy your application using a workload object\nthat provides a set of stateless replicas.\nDeployment\nor\nReplicaSet\nmay be better suited to your stateless needs.\nLimitations\nThe storage for a given Pod must either be provisioned by a\nPersistentVolume Provisioner\nbased on the requested\nstorage class\n, or pre-provisioned by an admin.\nDeleting and/or scaling a StatefulSet down will\nnot\ndelete the volumes associated with the\nStatefulSet. This is done to ensure data safety, which is generally more valuable than an\nautomatic purge of all related StatefulSet resources.\nStatefulSets currently require a\nHeadless Service\nto be responsible for the network identity of the Pods. You are responsible for creating this\nService.\nStatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is\ndeleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is\npossible to scale the StatefulSet down to 0 prior to deletion.\nWhen using\nRolling Updates\nwith the default\nPod Management Policy\n(\nOrderedReady\n),\nit's possible to get into a broken state that requires\nmanual intervention to repair\n.\nComponents\nThe example below demonstrates the components of a StatefulSet.\napiVersion\n:\nv1\nkind\n:\nService\nmetadata\n:\nname\n:\nnginx\nlabels\n:\napp\n:\nnginx\nspec\n:\nports\n:\n-\nport\n:\n80\nname\n:\nweb\nclusterIP\n:\nNone\nselector\n:\napp\n:\nnginx\n---\napiVersion\n:\napps/v1\nkind\n:\nStatefulSet\nmetadata\n:\nname\n:\nweb\nspec\n:\nselector\n:\nmatchLabels\n:\napp\n:\nnginx\n# has to match .spec.template.metadata.labels\nserviceName\n:\n\"nginx\"\nreplicas\n:\n3\n# by default is 1\nminReadySeconds\n:\n10\n# by default is 0\ntemplate\n:\nmetadata\n:\nlabels\n:\napp\n:\nnginx\n# has to match .spec.selector.matchLabels\nspec\n:\nterminationGracePeriodSeconds\n:\n10\ncontainers\n:\n-\nname\n:\nnginx\nimage\n:\nregistry.k8s.io/nginx-slim:0.24\nports\n:\n-\ncontainerPort\n:\n80\nname\n:\nweb\nvolumeMounts\n:\n-\nname\n:\nwww\nmountPath\n:\n/usr/share/nginx/html\nvolumeClaimTemplates\n:\n-\nmetadata\n:\nname\n:\nwww\nspec\n:\naccessModes\n:\n[\n\"ReadWriteOnce\"\n]\nstorageClassName\n:\n\"my-storage-class\"\nresources\n:\nrequests\n:\nstorage\n:\n1Gi\nNote:\nThis example uses the\nReadWriteOnce\naccess mode, for simplicity. For\nproduction use, the Kubernetes project recommends using the\nReadWriteOncePod\naccess mode instead.\nIn the above example:\nA Headless Service, named\nnginx\n, is used to control the network domain.\nThe StatefulSet, named\nweb\n, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.\nThe\nvolumeClaimTemplates\nwill provide stable storage using\nPersistentVolumes\nprovisioned by a\nPersistentVolume Provisioner.\nThe name of a StatefulSet object must be a valid\nDNS label\n.\nPod Selector\nYou must set the\n.spec.selector\nfield of a StatefulSet to match the labels of its\n.spec.template.metadata.labels\n. Failing to specify a matching Pod Selector will result in a\nvalidation error during StatefulSet creation.\nVolume Claim Templates\nYou can set the\n.spec.volumeClaimTemplates\nfield to create a\nPersistentVolumeClaim\n.\nThis will provide stable storage to the StatefulSet if either:\nThe StorageClass specified for the volume claim is set up to use\ndynamic\nprovisioning\n.\nThe cluster already contains a PersistentVolume with the correct StorageClass\nand sufficient available storage space.\nMinimum ready seconds\nFEATURE STATE:\nKubernetes v1.25 [stable]\n.spec.minReadySeconds\nis an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be running and ready without any of its containers crashing, for it to be considered available.\nThis is used to check progression of a rollout when using a\nRolling Update\nstrategy.\nThis field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see\nContainer Probes\n.\nPod Identity\nStatefulSet Pods have a unique identity that consists of an ordinal, a\nstable network identity, and stable storage. The identity sticks to the Pod,\nregardless of which node it's (re)scheduled on.\nOrdinal Index\nFor a StatefulSet with N\nreplicas\n, each Pod in the StatefulSet\nwill be assigned an integer ordinal, that is unique over the Set. By default,\npods will be assigned ordinals from 0 up through N-1. The StatefulSet controller\nwill also add a pod label with this index:\napps.kubernetes.io/pod-index\n.\nStart ordinal\nFEATURE STATE:\nKubernetes v1.31 [stable]\n(enabled by default)\n.spec.ordinals\nis an optional field that allows you to configure the integer\nordinals assigned to each Pod. It defaults to nil. Within the field, you can\nconfigure the following options:\n.spec.ordinals.start\n: If the\n.spec.ordinals.start\nfield is set, Pods will\nbe assigned ordinals from\n.spec.ordinals.start\nup through\n.spec.ordinals.start + .spec.replicas - 1\n.\nStable Network ID\nEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet\nand the ordinal of the Pod. The pattern for the constructed hostname\nis\n$(statefulset name)-$(ordinal)\n. The example above will create three Pods\nnamed\nweb-0,web-1,web-2\n.\nA StatefulSet can use a\nHeadless Service\nto control the domain of its Pods. The domain managed by this Service takes the form:\n$(service name).$(namespace).svc.cluster.local\n, where \"cluster.local\" is the\ncluster domain.\nAs each Pod is created, it gets a matching DNS subdomain, taking the form:\n$(podname).$(governing service domain)\n, where the governing service is defined\nby the\nserviceName\nfield on the StatefulSet.\nDepending on how DNS is configured in your cluster, you may not be able to look up the DNS\nname for a newly-run Pod immediately. This behavior can occur when other clients in the\ncluster have already sent queries for the hostname of the Pod before it was created.\nNegative caching (normal in DNS) means that the results of previous failed lookups are\nremembered and reused, even after the Pod is running, for at least a few seconds.\nIf you need to discover Pods promptly after they are created, you have a few options:\nQuery the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.\nDecrease the time of caching in your Kubernetes DNS provider (typically this means editing the\nconfig map for CoreDNS, which currently caches for 30 seconds).\nAs mentioned in the\nlimitations\nsection, you are responsible for\ncreating the\nHeadless Service\nresponsible for the network identity of the pods.\nHere are some examples of choices for Cluster Domain, Service name,\nStatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.\nCluster Domain\nService (ns/name)\nStatefulSet (ns/name)\nStatefulSet Domain\nPod DNS\nPod Hostname\ncluster.local\ndefault/nginx\ndefault/web\nnginx.default.svc.cluster.local\nweb-{0..N-1}.nginx.default.svc.cluster.local\nweb-{0..N-1}\ncluster.local\nfoo/nginx\nfoo/web\nnginx.foo.svc.cluster.local\nweb-{0..N-1}.nginx.foo.svc.cluster.local\nweb-{0..N-1}\nkube.local\nfoo/nginx\nfoo/web\nnginx.foo.svc.kube.local\nweb-{0..N-1}.nginx.foo.svc.kube.local\nweb-{0..N-1}\nNote:\nCluster Domain will be set to\ncluster.local\nunless\notherwise configured\n.\nStable Storage\nFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one\nPersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume\nwith a StorageClass of\nmy-storage-class\nand 1 GiB of provisioned storage. If no StorageClass\nis specified, then the default StorageClass will be used. When a Pod is (re)scheduled\nonto a node, its\nvolumeMounts\nmount the PersistentVolumes associated with its\nPersistentVolume Claims. Note that, the PersistentVolumes associated with the\nPods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.\nThis must be done manually.\nPod Name Label\nWhen the StatefulSet\ncontroller\ncreates a Pod,\nit adds a label,\nstatefulset.kubernetes.io/pod-name\n, that is set to the name of\nthe Pod. This label allows you to attach a Service to a specific Pod in\nthe StatefulSet.\nPod index label\nFEATURE STATE:\nKubernetes v1.32 [stable]\n(enabled by default)\nWhen the StatefulSet\ncontroller\ncreates a Pod,\nthe new Pod is labelled with\napps.kubernetes.io/pod-index\n. The value of this label is the ordinal index of\nthe Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics\nusing the pod index label, and more. Note the feature gate\nPodIndexLabel\nis enabled and locked by default for this\nfeature, in order to disable it, users will have to use server emulated version v1.31.\nDeployment and Scaling Guarantees\nFor a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.\nWhen Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\nBefore a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.\nBefore a Pod is terminated, all of its successors must be completely shutdown.\nThe StatefulSet should not specify a\npod.Spec.TerminationGracePeriodSeconds\nof 0. This practice\nis unsafe and strongly discouraged. For further explanation, please refer to\nforce deleting StatefulSet Pods\n.\nWhen the nginx example above is created, three Pods will be deployed in the order\nweb-0, web-1, web-2. web-1 will not be deployed before web-0 is\nRunning and Ready\n, and web-2 will not be deployed until\nweb-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before\nweb-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and\nbecomes Running and Ready.\nIf a user were to scale the deployed example by patching the StatefulSet such that\nreplicas=1\n, web-2 would be terminated first. web-1 would not be terminated until web-2\nis fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and\nis completely shutdown, but prior to web-1's termination, web-1 would not be terminated\nuntil web-0 is Running and Ready.\nPod Management Policies\nStatefulSet allows you to relax its ordering guarantees while\npreserving its uniqueness and identity guarantees via its\n.spec.podManagementPolicy\nfield.\nOrderedReady Pod Management\nOrderedReady\npod management is the default for StatefulSets. It implements the behavior\ndescribed in\nDeployment and Scaling Guarantees\n.\nParallel Pod Management\nParallel\npod management tells the StatefulSet controller to launch or\nterminate all Pods in parallel, and to not wait for Pods to become Running\nand Ready or completely terminated prior to launching or terminating another\nPod. This option only affects the behavior for scaling operations. Updates are not\naffected.\nUpdate strategies\nA StatefulSet's\n.spec.updateStrategy\nfield allows you to configure\nand disable automated rolling updates for containers, labels, resource request/limits, and\nannotations for the Pods in a StatefulSet. There are two possible values:\nOnDelete\nWhen a StatefulSet's\n.spec.updateStrategy.type\nis set to\nOnDelete\n,\nthe StatefulSet controller will not automatically update the Pods in a\nStatefulSet. Users must manually delete Pods to cause the controller to\ncreate new Pods that reflect modifications made to a StatefulSet's\n.spec.template\n.\nRollingUpdate\nThe\nRollingUpdate\nupdate strategy implements automated, rolling updates for the Pods in a\nStatefulSet. This is the default update strategy.\nRolling Updates\nWhen a StatefulSet's\n.spec.updateStrategy.type\nis set to\nRollingUpdate\n, the\nStatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed\nin the same order as Pod termination (from the largest ordinal to the smallest), updating\neach Pod one at a time.\nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior\nto updating its predecessor. If you have set\n.spec.minReadySeconds\n(see\nMinimum Ready Seconds\n), the control plane additionally waits that\namount of time after the Pod turns ready, before moving on.\nPartitioned rolling updates\nThe\nRollingUpdate\nupdate strategy can be partitioned, by specifying a\n.spec.updateStrategy.rollingUpdate.partition\n. If a partition is specified, all Pods with an\nordinal that is greater than or equal to the partition will be updated when the StatefulSet's\n.spec.template\nis updated. All Pods with an ordinal that is less than the partition will not\nbe updated, and, even if they are deleted, they will be recreated at the previous version. If a\nStatefulSet's\n.spec.updateStrategy.rollingUpdate.partition\nis greater than its\n.spec.replicas\n,\nupdates to its\n.spec.template\nwill not be propagated to its Pods.\nIn most cases you will not need to use a partition, but they are useful if you want to stage an\nupdate, roll out a canary, or perform a phased roll out.\nMaximum unavailable Pods\nFEATURE STATE:\nKubernetes v1.24 [alpha]\nYou can control the maximum number of Pods that can be unavailable during an update\nby specifying the\n.spec.updateStrategy.rollingUpdate.maxUnavailable\nfield.\nThe value can be an absolute number (for example,\n5\n) or a percentage of desired\nPods (for example,\n10%\n). Absolute number is calculated from the percentage value\nby rounding it up. This field cannot be 0. The default setting is 1.\nThis field applies to all Pods in the range\n0\nto\nreplicas - 1\n. If there is any\nunavailable Pod in the range\n0\nto\nreplicas - 1\n, it will be counted towards\nmaxUnavailable\n.\nNote:\nThe\nmaxUnavailable\nfield is in Alpha stage and it is honored only by API servers\nthat are running with the\nMaxUnavailableStatefulSet\nfeature gate\nenabled.\nForced rollback\nWhen using\nRolling Updates\nwith the default\nPod Management Policy\n(\nOrderedReady\n),\nit's possible to get into a broken state that requires manual intervention to repair.\nIf you update the Pod template to a configuration that never becomes Running and\nReady (for example, due to a bad binary or application-level configuration error),\nStatefulSet will stop the rollout and wait.\nIn this state, it's not enough to revert the Pod template to a good configuration.\nDue to a\nknown issue\n,\nStatefulSet will continue to wait for the broken Pod to become Ready\n(which never happens) before it will attempt to revert it back to the working\nconfiguration.\nAfter reverting the template, you must also delete any Pods that StatefulSet had\nalready attempted to run with the bad configuration.\nStatefulSet will then begin to recreate the Pods using the reverted template.\nRevision history\nControllerRevision is a Kubernetes API resource used by controllers, such as the StatefulSet controller, to track historical configuration changes.\nStatefulSets use ControllerRevisions to maintain a revision history, enabling rollbacks and version tracking.\nHow StatefulSets track changes using ControllerRevisions\nWhen you update a StatefulSet's Pod template (\nspec.template\n), the StatefulSet controller:\nPrepares a new ControllerRevision object\nStores a snapshot of the Pod template and metadata\nAssigns an incremental revision number\nKey Properties\nSee\nControllerRevision\nto learn more about key properties and other details.\nManaging Revision History\nControl retained revisions with\n.spec.revisionHistoryLimit\n:\napiVersion\n:\napps/v1\nkind\n:\nStatefulSet\nmetadata\n:\nname\n:\nwebapp\nspec\n:\nrevisionHistoryLimit\n:\n5\n# Keep last 5 revisions\n# ... other spec fields ...\nDefault\n: 10 revisions retained if unspecified\nCleanup\n: Oldest revisions are garbage-collected when exceeding the limit\nPerforming Rollbacks\nYou can revert to a previous configuration using:\n# View revision history\nkubectl rollout\nhistory\nstatefulset/webapp\n# Rollback to a specific revision\nkubectl rollout undo statefulset/webapp --to-revision\n=\n3\nThis will:\nApply the Pod template from revision 3\nCreate a new ControllerRevision with an updated revision number\nInspecting ControllerRevisions\nTo view associated ControllerRevisions:\n# List all revisions for the StatefulSet\nkubectl get controllerrevisions -l app.kubernetes.io/name\n=\nwebapp\n# View detailed configuration of a specific revision\nkubectl get controllerrevision/webapp-3 -o yaml\nBest Practices\nRetention Policy\nSet\nrevisionHistoryLimit\nbetween\n5â€“10\nfor most workloads.\nIncrease only if\ndeep rollback history\nis required.\nMonitoring\nRegularly check revisions with:\nkubectl get controllerrevisions\nAlert on\nrapid revision count growth\n.\nAvoid\nManual edits to ControllerRevision objects.\nUsing revisions as a backup mechanism (use actual backup tools).\nSetting\nrevisionHistoryLimit: 0\n(disables rollback capability).\nPersistentVolumeClaim retention\nFEATURE STATE:\nKubernetes v1.32 [stable]\n(enabled by default)\nThe optional\n.spec.persistentVolumeClaimRetentionPolicy\nfield controls if\nand how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the\nStatefulSetAutoDeletePVC\nfeature gate\non the API server and the controller manager to use this field.\nOnce enabled, there are two policies you can configure for each StatefulSet:\nwhenDeleted\nConfigures the volume retention behavior that applies when the StatefulSet is deleted.\nwhenScaled\nConfigures the volume retention behavior that applies when the replica count of\nthe StatefulSet is reduced; for example, when scaling down the set.\nFor each policy that you can configure, you can set the value to either\nDelete\nor\nRetain\n.\nDelete\nThe PVCs created from the StatefulSet\nvolumeClaimTemplate\nare deleted for each Pod\naffected by the policy. With the\nwhenDeleted\npolicy all PVCs from the\nvolumeClaimTemplate\nare deleted after their Pods have been deleted. With the\nwhenScaled\npolicy, only PVCs corresponding to Pod replicas being scaled down are\ndeleted, after their Pods have been deleted.\nRetain\n(default)\nPVCs from the\nvolumeClaimTemplate\nare not affected when their Pod is\ndeleted. This is the behavior before this new feature.\nBear in mind that these policies\nonly\napply when Pods are being removed due to the\nStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet\nfails due to node failure, and the control plane creates a replacement Pod, the StatefulSet\nretains the existing PVC. The existing volume is unaffected, and the cluster will attach it to\nthe node where the new Pod is about to launch.\nThe default for policies is\nRetain\n, matching the StatefulSet behavior before this new feature.\nHere is an example policy:\napiVersion\n:\napps/v1\nkind\n:\nStatefulSet\n...\nspec\n:\npersistentVolumeClaimRetentionPolicy\n:\nwhenDeleted\n:\nRetain\nwhenScaled\n:\nDelete\n...\nThe StatefulSet\ncontroller\nadds\nowner references\nto its PVCs, which are then deleted by the\ngarbage collector\nafter the Pod is terminated. This enables the Pod to\ncleanly unmount all volumes before the PVCs are deleted (and before the backing PV and\nvolume are deleted, depending on the retain policy). When you set the\nwhenDeleted\npolicy to\nDelete\n, an owner reference to the StatefulSet instance is placed on all PVCs\nassociated with that StatefulSet.\nThe\nwhenScaled\npolicy must delete PVCs only when a Pod is scaled down, and not when a\nPod is deleted for another reason. When reconciling, the StatefulSet controller compares\nits desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod\nwhose id greater than the replica count is condemned and marked for deletion. If the\nwhenScaled\npolicy is\nDelete\n, the condemned Pods are first set as owners to the\nassociated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs\nto be garbage collected after only the condemned Pods have terminated.\nThis means that if the controller crashes and restarts, no Pod will be deleted before its\nowner reference has been updated appropriate to the policy. If a condemned Pod is\nforce-deleted while the controller is down, the owner reference may or may not have been\nset up, depending on when the controller crashed. It may take several reconcile loops to\nupdate the owner references, so some condemned Pods may have set up owner references and\nothers may not. For this reason we recommend waiting for the controller to come back up,\nwhich will verify owner references before terminating Pods. If that is not possible, the\noperator should verify the owner references on PVCs to ensure the expected objects are\ndeleted when Pods are force-deleted.\nReplicas\n.spec.replicas\nis an optional field that specifies the number of desired Pods. It defaults to 1.\nShould you manually scale a deployment, example via\nkubectl scale statefulset statefulset --replicas=X\n, and then you update that StatefulSet\nbased on a manifest (for example: by running\nkubectl apply -f statefulset.yaml\n), then applying that manifest overwrites the manual scaling\nthat you previously did.\nIf a\nHorizontalPodAutoscaler\n(or any similar API for horizontal scaling) is managing scaling for a\nStatefulset, don't set\n.spec.replicas\n. Instead, allow the Kubernetes\ncontrol plane\nto manage\nthe\n.spec.replicas\nfield automatically.\nWhat's next\nLearn about\nPods\n.\nFind out how to use StatefulSets\nFollow an example of\ndeploying a stateful application\n.\nFollow an example of\ndeploying Cassandra with Stateful Sets\n.\nFollow an example of\nrunning a replicated stateful application\n.\nLearn how to\nscale a StatefulSet\n.\nLearn what's involved when you\ndelete a StatefulSet\n.\nLearn how to\nconfigure a Pod to use a volume for storage\n.\nLearn how to\nconfigure a Pod to use a PersistentVolume for storage\n.\nStatefulSet\nis a top-level resource in the Kubernetes REST API.\nRead the\nStatefulSet\nobject definition to understand the API for stateful sets.\nRead about\nPodDisruptionBudget\nand how\nyou can use it to manage application availability during disruptions.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 30, 2025 at 11:00 PM PST:\nfix broken link in statefulset.md (d8c5f3484d)\nStatefulSet API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"}}
{"text": "DaemonSet | Kubernetes\nDaemonSet\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\nA\nDaemonSet\nensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the\ncluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage\ncollected. Deleting a DaemonSet will clean up the Pods it created.\nSome typical uses of a DaemonSet are:\nrunning a cluster storage daemon on every node\nrunning a logs collection daemon on every node\nrunning a node monitoring daemon on every node\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\ndifferent flags and/or different memory and cpu requests for different hardware types.\nWriting a DaemonSet Spec\nCreate a DaemonSet\nYou can describe a DaemonSet in a YAML file. For example, the\ndaemonset.yaml\nfile below\ndescribes a DaemonSet that runs the fluentd-elasticsearch Docker image:\ncontrollers/daemonset.yaml\napiVersion\n:\napps/v1\nkind\n:\nDaemonSet\nmetadata\n:\nname\n:\nfluentd-elasticsearch\nnamespace\n:\nkube-system\nlabels\n:\nk8s-app\n:\nfluentd-logging\nspec\n:\nselector\n:\nmatchLabels\n:\nname\n:\nfluentd-elasticsearch\ntemplate\n:\nmetadata\n:\nlabels\n:\nname\n:\nfluentd-elasticsearch\nspec\n:\ntolerations\n:\n# these tolerations are to have the daemonset runnable on control plane nodes\n# remove them if your control plane nodes should not run pods\n-\nkey\n:\nnode-role.kubernetes.io/control-plane\noperator\n:\nExists\neffect\n:\nNoSchedule\n-\nkey\n:\nnode-role.kubernetes.io/master\noperator\n:\nExists\neffect\n:\nNoSchedule\ncontainers\n:\n-\nname\n:\nfluentd-elasticsearch\nimage\n:\nquay.io/fluentd_elasticsearch/fluentd:v5.0.1\nresources\n:\nlimits\n:\nmemory\n:\n200Mi\nrequests\n:\ncpu\n:\n100m\nmemory\n:\n200Mi\nvolumeMounts\n:\n-\nname\n:\nvarlog\nmountPath\n:\n/var/log\n# it may be desirable to set a high priority class to ensure that a DaemonSet Pod\n# preempts running Pods\n# priorityClassName: important\nterminationGracePeriodSeconds\n:\n30\nvolumes\n:\n-\nname\n:\nvarlog\nhostPath\n:\npath\n:\n/var/log\nCreate a DaemonSet based on the YAML file:\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\nRequired Fields\nAs with all other Kubernetes config, a DaemonSet needs\napiVersion\n,\nkind\n, and\nmetadata\nfields. For\ngeneral information about working with config files, see\nrunning stateless applications\nand\nobject management using kubectl\n.\nThe name of a DaemonSet object must be a valid\nDNS subdomain name\n.\nA DaemonSet also needs a\n.spec\nsection.\nPod Template\nThe\n.spec.template\nis one of the required fields in\n.spec\n.\nThe\n.spec.template\nis a\npod template\n.\nIt has exactly the same schema as a\nPod\n,\nexcept it is nested and does not have an\napiVersion\nor\nkind\n.\nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate\nlabels (see\npod selector\n).\nA Pod Template in a DaemonSet must have a\nRestartPolicy\nequal to\nAlways\n, or be unspecified, which defaults to\nAlways\n.\nPod Selector\nThe\n.spec.selector\nfield is a pod selector. It works the same as the\n.spec.selector\nof\na\nJob\n.\nYou must specify a pod selector that matches the labels of the\n.spec.template\n.\nAlso, once a DaemonSet is created,\nits\n.spec.selector\ncan not be mutated. Mutating the pod selector can lead to the\nunintentional orphaning of Pods, and it was found to be confusing to users.\nThe\n.spec.selector\nis an object consisting of two fields:\nmatchLabels\n- works the same as the\n.spec.selector\nof a\nReplicationController\n.\nmatchExpressions\n- allows to build more sophisticated selectors by specifying key,\nlist of values and an operator that relates the key and values.\nWhen the two are specified the result is ANDed.\nThe\n.spec.selector\nmust match the\n.spec.template.metadata.labels\n.\nConfig with these two not matching will be rejected by the API.\nRunning Pods on select Nodes\nIf you specify a\n.spec.template.spec.nodeSelector\n, then the DaemonSet controller will\ncreate Pods on nodes which match that\nnode selector\n.\nLikewise if you specify a\n.spec.template.spec.affinity\n,\nthen DaemonSet controller will create Pods on nodes which match that\nnode affinity\n.\nIf you do not specify either, then the DaemonSet controller will create Pods on all nodes.\nHow Daemon Pods are scheduled\nA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod.\nThe DaemonSet controller creates a Pod for each eligible node and adds the\nspec.affinity.nodeAffinity\nfield of the Pod to match the target host. After\nthe Pod is created, the default scheduler typically takes over and then binds\nthe Pod to the target host by setting the\n.spec.nodeName\nfield. If the new\nPod cannot fit on the node, the default scheduler may preempt (evict) some of\nthe existing Pods based on the\npriority\nof the new Pod.\nNote:\nIf it's important that the DaemonSet pod run on each node, it's often desirable\nto set the\n.spec.template.spec.priorityClassName\nof the DaemonSet to a\nPriorityClass\nwith a higher priority to ensure that this eviction occurs.\nThe user can specify a different scheduler for the Pods of the DaemonSet, by\nsetting the\n.spec.template.spec.schedulerName\nfield of the DaemonSet.\nThe original node affinity specified at the\n.spec.template.spec.affinity.nodeAffinity\nfield (if specified) is taken into\nconsideration by the DaemonSet controller when evaluating the eligible nodes,\nbut is replaced on the created Pod with the node affinity that matches the name\nof the eligible node.\nnodeAffinity\n:\nrequiredDuringSchedulingIgnoredDuringExecution\n:\nnodeSelectorTerms\n:\n-\nmatchFields\n:\n-\nkey\n:\nmetadata.name\noperator\n:\nIn\nvalues\n:\n- target-host-name\nTaints and tolerations\nThe DaemonSet controller automatically adds a set of\ntolerations\nto DaemonSet Pods:\nTolerations for DaemonSet pods\nToleration key\nEffect\nDetails\nnode.kubernetes.io/not-ready\nNoExecute\nDaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted.\nnode.kubernetes.io/unreachable\nNoExecute\nDaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted.\nnode.kubernetes.io/disk-pressure\nNoSchedule\nDaemonSet Pods can be scheduled onto nodes with disk pressure issues.\nnode.kubernetes.io/memory-pressure\nNoSchedule\nDaemonSet Pods can be scheduled onto nodes with memory pressure issues.\nnode.kubernetes.io/pid-pressure\nNoSchedule\nDaemonSet Pods can be scheduled onto nodes with process pressure issues.\nnode.kubernetes.io/unschedulable\nNoSchedule\nDaemonSet Pods can be scheduled onto nodes that are unschedulable.\nnode.kubernetes.io/network-unavailable\nNoSchedule\nOnly added for DaemonSet Pods that request host networking\n, i.e., Pods having\nspec.hostNetwork: true\n. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.\nYou can add your own tolerations to the Pods of a DaemonSet as well, by\ndefining these in the Pod template of the DaemonSet.\nBecause the DaemonSet controller sets the\nnode.kubernetes.io/unschedulable:NoSchedule\ntoleration automatically,\nKubernetes can run DaemonSet Pods on nodes that are marked as\nunschedulable\n.\nIf you use a DaemonSet to provide an important node-level function, such as\ncluster networking\n, it is\nhelpful that Kubernetes places DaemonSet Pods on nodes before they are ready.\nFor example, without that special toleration, you could end up in a deadlock\nsituation where the node is not marked as ready because the network plugin is\nnot running there, and at the same time the network plugin is not running on\nthat node because the node is not yet ready.\nCommunicating with Daemon Pods\nSome possible patterns for communicating with Pods in a DaemonSet are:\nPush\n: Pods in the DaemonSet are configured to send updates to another service, such\nas a stats database. They do not have clients.\nNodeIP and Known Port\n: Pods in the DaemonSet can use a\nhostPort\n, so that the pods\nare reachable via the node IPs.\nClients know the list of node IPs somehow, and know the port by convention.\nDNS\n: Create a\nheadless service\nwith the same pod selector, and then discover DaemonSets using the\nendpoints\nresource or retrieve multiple A records from DNS.\nService\n: Create a service with the same Pod selector, and use the service to reach a\ndaemon on a random node. Use\nService Internal Traffic Policy\nto limit to pods on the same node.\nUpdating a DaemonSet\nIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete\nPods from newly not-matching nodes.\nYou can modify the Pods that a DaemonSet creates. However, Pods do not allow all\nfields to be updated. Also, the DaemonSet controller will use the original template the next\ntime a node (even with the same name) is created.\nYou can delete a DaemonSet. If you specify\n--cascade=orphan\nwith\nkubectl\n, then the Pods\nwill be left on the nodes. If you subsequently create a new DaemonSet with the same selector,\nthe new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces\nthem according to its\nupdateStrategy\n.\nYou can\nperform a rolling update\non a DaemonSet.\nAlternatives to DaemonSet\nInit scripts\nIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using\ninit\n,\nupstartd\n, or\nsystemd\n). This is perfectly fine. However, there are several advantages to\nrunning such processes via a DaemonSet:\nAbility to monitor and manage logs for daemons in the same way as applications.\nSame config language and tools (e.g. Pod templates,\nkubectl\n) for daemons and applications.\nRunning daemons in containers with resource limits increases isolation between daemons from app\ncontainers. However, this can also be accomplished by running the daemons in a container but not in a Pod.\nBare Pods\nIt is possible to create Pods directly which specify a particular node to run on. However,\na DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of\nnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should\nuse a DaemonSet rather than creating individual Pods.\nStatic Pods\nIt is possible to create Pods by writing a file to a certain directory watched by Kubelet. These\nare called\nstatic pods\n.\nUnlike DaemonSet, static Pods cannot be managed with kubectl\nor other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful\nin cluster bootstrapping cases. Also, static Pods may be deprecated in the future.\nDeployments\nDaemonSets are similar to\nDeployments\nin that\nthey both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,\nstorage servers).\nUse a Deployment for stateless services, like frontends, where scaling up and down the\nnumber of replicas and rolling out updates are more important than controlling exactly which host\nthe Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on\nall or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.\nFor example,\nnetwork plugins\noften include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.\nWhat's next\nLearn about\nPods\n:\nLearn about\nstatic Pods\n, which are useful for running Kubernetes\ncontrol plane\ncomponents.\nFind out how to use DaemonSets:\nPerform a rolling update on a DaemonSet\n.\nPerform a rollback on a DaemonSet\n(for example, if a roll out didn't work how you expected).\nUnderstand\nhow Kubernetes assigns Pods to Nodes\n.\nLearn about\ndevice plugins\nand\nadd ons\n, which often run as DaemonSets.\nDaemonSet\nis a top-level resource in the Kubernetes REST API.\nRead the\nDaemonSet\nobject definition to understand the API for daemon sets.\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 20, 2025 at 7:13 PM PST:\nfix typo in workloads/controllers/daemonset.md (0dc80c3525)\nDaemonSet API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"}}
{"text": "Persistent Volumes | Kubernetes\nPersistent Volumes\nThis document describes\npersistent volumes\nin Kubernetes. Familiarity with\nvolumes\n,\nStorageClasses\nand\nVolumeAttributesClasses\nis suggested.\nIntroduction\nManaging storage is a distinct problem from managing compute instances.\nThe PersistentVolume subsystem provides an API for users and administrators\nthat abstracts details of how storage is provided from how it is consumed.\nTo do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.\nA\nPersistentVolume\n(PV) is a piece of storage in the cluster that has been\nprovisioned by an administrator or dynamically provisioned using\nStorage Classes\n. It is a resource in\nthe cluster just like a node is a cluster resource. PVs are volume plugins like\nVolumes, but have a lifecycle independent of any individual Pod that uses the PV.\nThis API object captures the details of the implementation of the storage, be that\nNFS, iSCSI, or a cloud-provider-specific storage system.\nA\nPersistentVolumeClaim\n(PVC) is a request for storage by a user. It is similar\nto a Pod. Pods consume node resources and PVCs consume PV resources. Pods can\nrequest specific levels of resources (CPU and Memory). Claims can request specific\nsize and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany,\nReadWriteMany, or ReadWriteOncePod, see\nAccessModes\n).\nWhile PersistentVolumeClaims allow a user to consume abstract storage resources,\nit is common that users need PersistentVolumes with varying properties, such as\nperformance, for different problems. Cluster administrators need to be able to\noffer a variety of PersistentVolumes that differ in more ways than size and access\nmodes, without exposing users to the details of how those volumes are implemented.\nFor these needs, there is the\nStorageClass\nresource.\nSee the\ndetailed walkthrough with working examples\n.\nLifecycle of a volume and claim\nPVs are resources in the cluster. PVCs are requests for those resources and also act\nas claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:\nProvisioning\nThere are two ways PVs may be provisioned: statically or dynamically.\nStatic\nA cluster administrator creates a number of PVs. They carry the details of the\nreal storage, which is available for use by cluster users. They exist in the\nKubernetes API and are available for consumption.\nDynamic\nWhen none of the static PVs the administrator created match a user's PersistentVolumeClaim,\nthe cluster may try to dynamically provision a volume specially for the PVC.\nThis provisioning is based on StorageClasses: the PVC must request a\nstorage class\nand\nthe administrator must have created and configured that class for dynamic\nprovisioning to occur. Claims that request the class\n\"\"\neffectively disable\ndynamic provisioning for themselves.\nTo enable dynamic storage provisioning based on storage class, the cluster administrator\nneeds to enable the\nDefaultStorageClass\nadmission controller\non the API server. This can be done, for example, by ensuring that\nDefaultStorageClass\nis\namong the comma-delimited, ordered list of values for the\n--enable-admission-plugins\nflag of\nthe API server component. For more information on API server command-line flags,\ncheck\nkube-apiserver\ndocumentation.\nBinding\nA user creates, or in the case of dynamic provisioning, has already created,\na PersistentVolumeClaim with a specific amount of storage requested and with\ncertain access modes. A control loop in the control plane watches for new PVCs, finds\na matching PV (if possible), and binds them together. If a PV was dynamically\nprovisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise,\nthe user will always get at least what they asked for, but the volume may be in\nexcess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive,\nregardless of how they were bound. A PVC to PV binding is a one-to-one mapping,\nusing a ClaimRef which is a bi-directional binding between the PersistentVolume\nand the PersistentVolumeClaim.\nClaims will remain unbound indefinitely if a matching volume does not exist.\nClaims will be bound as matching volumes become available. For example, a\ncluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi.\nThe PVC can be bound when a 100Gi PV is added to the cluster.\nUsing\nPods use claims as volumes. The cluster inspects the claim to find the bound\nvolume and mounts that volume for a Pod. For volumes that support multiple\naccess modes, the user specifies which mode is desired when using their claim\nas a volume in a Pod.\nOnce a user has a claim and that claim is bound, the bound PV belongs to the\nuser for as long as they need it. Users schedule Pods and access their claimed\nPVs by including a\npersistentVolumeClaim\nsection in a Pod's\nvolumes\nblock.\nSee\nClaims As Volumes\nfor more details on this.\nStorage Object in Use Protection\nThe purpose of the Storage Object in Use Protection feature is to ensure that\nPersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs)\nthat are bound to PVCs are not removed from the system, as this may result in data loss.\nNote:\nPVC is in active use by a Pod when a Pod object exists that is using the PVC.\nIf a user deletes a PVC in active use by a Pod, the PVC is not removed immediately.\nPVC removal is postponed until the PVC is no longer actively used by any Pods. Also,\nif an admin deletes a PV that is bound to a PVC, the PV is not removed immediately.\nPV removal is postponed until the PV is no longer bound to a PVC.\nYou can see that a PVC is protected when the PVC's status is\nTerminating\nand the\nFinalizers\nlist includes\nkubernetes.io/pvc-protection\n:\nkubectl describe pvc hostpath\nName:          hostpath\nNamespace:     default\nStorageClass:  example-hostpath\nStatus:        Terminating\nVolume:\nLabels:        <none>\nAnnotations:   volume.beta.kubernetes.io/storage-class\n=\nexample-hostpath\nvolume.beta.kubernetes.io/storage-provisioner\n=\nexample.com/hostpath\nFinalizers:\n[\nkubernetes.io/pvc-protection\n]\n...\nYou can see that a PV is protected when the PV's status is\nTerminating\nand\nthe\nFinalizers\nlist includes\nkubernetes.io/pv-protection\ntoo:\nkubectl describe pv task-pv-volume\nName:            task-pv-volume\nLabels:\ntype\n=\nlocal\nAnnotations:     <none>\nFinalizers:\n[\nkubernetes.io/pv-protection\n]\nStorageClass:    standard\nStatus:          Terminating\nClaim:\nReclaim Policy:  Delete\nAccess Modes:    RWO\nCapacity:        1Gi\nMessage:\nSource:\nType:          HostPath\n(\nbare host directory volume\n)\nPath:          /tmp/data\nHostPathType:\nEvents:            <none>\nReclaiming\nWhen a user is done with their volume, they can delete the PVC objects from the\nAPI that allows reclamation of the resource. The reclaim policy for a PersistentVolume\ntells the cluster what to do with the volume after it has been released of its claim.\nCurrently, volumes can either be Retained, Recycled, or Deleted.\nRetain\nThe\nRetain\nreclaim policy allows for manual reclamation of the resource.\nWhen the PersistentVolumeClaim is deleted, the PersistentVolume still exists\nand the volume is considered \"released\". But it is not yet available for\nanother claim because the previous claimant's data remains on the volume.\nAn administrator can manually reclaim the volume with the following steps.\nDelete the PersistentVolume. The associated storage asset in external infrastructure\nstill exists after the PV is deleted.\nManually clean up the data on the associated storage asset accordingly.\nManually delete the associated storage asset.\nIf you want to reuse the same storage asset, create a new PersistentVolume with\nthe same storage asset definition.\nDelete\nFor volume plugins that support the\nDelete\nreclaim policy, deletion removes\nboth the PersistentVolume object from Kubernetes, as well as the associated\nstorage asset in the external infrastructure. Volumes that were dynamically provisioned\ninherit the\nreclaim policy of their StorageClass\n, which\ndefaults to\nDelete\n. The administrator should configure the StorageClass\naccording to users' expectations; otherwise, the PV must be edited or\npatched after it is created. See\nChange the Reclaim Policy of a PersistentVolume\n.\nRecycle\nWarning:\nThe\nRecycle\nreclaim policy is deprecated. Instead, the recommended approach\nis to use dynamic provisioning.\nIf supported by the underlying volume plugin, the\nRecycle\nreclaim policy performs\na basic scrub (\nrm -rf /thevolume/*\n) on the volume and makes it available again for a new claim.\nHowever, an administrator can configure a custom recycler Pod template using\nthe Kubernetes controller manager command line arguments as described in the\nreference\n.\nThe custom recycler Pod template must contain a\nvolumes\nspecification, as\nshown in the example below:\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\npv-recycler\nnamespace\n:\ndefault\nspec\n:\nrestartPolicy\n:\nNever\nvolumes\n:\n-\nname\n:\nvol\nhostPath\n:\npath\n:\n/any/path/it/will/be/replaced\ncontainers\n:\n-\nname\n:\npv-recycler\nimage\n:\n\"registry.k8s.io/busybox\"\ncommand\n:\n[\n\"/bin/sh\"\n,\n\"-c\"\n,\n\"test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  && test -z \\\"$(ls -A /scrub)\\\" || exit 1\"\n]\nvolumeMounts\n:\n-\nname\n:\nvol\nmountPath\n:\n/scrub\nHowever, the particular path specified in the custom recycler Pod template in the\nvolumes\npart is replaced with the particular path of the volume that is being recycled.\nPersistentVolume deletion protection finalizer\nFEATURE STATE:\nKubernetes v1.33 [stable]\n(enabled by default)\nFinalizers can be added on a PersistentVolume to ensure that PersistentVolumes\nhaving\nDelete\nreclaim policy are deleted only after the backing storage are deleted.\nThe finalizer\nexternal-provisioner.volume.kubernetes.io/finalizer\n(introduced\nin v1.31) is added to both dynamically provisioned and statically provisioned\nCSI volumes.\nThe finalizer\nkubernetes.io/pv-controller\n(introduced in v1.31) is added to\ndynamically provisioned in-tree plugin volumes and skipped for statically\nprovisioned in-tree plugin volumes.\nThe following is an example of dynamically provisioned in-tree plugin volume:\nkubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\nName:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\nLabels:          <none>\nAnnotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner\npv.kubernetes.io/bound-by-controller: yes\npv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume\nFinalizers:\n[\nkubernetes.io/pv-protection kubernetes.io/pv-controller\n]\nStorageClass:    vcp-sc\nStatus:          Bound\nClaim:           default/vcp-pvc-1\nReclaim Policy:  Delete\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        1Gi\nNode Affinity:   <none>\nMessage:\nSource:\nType:               vSphereVolume\n(\na Persistent Disk resource in vSphere\n)\nVolumePath:\n[\nvsanDatastore\n]\nd49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk\nFSType:             ext4\nStoragePolicyName:  vSAN Default Storage Policy\nEvents:                 <none>\nThe finalizer\nexternal-provisioner.volume.kubernetes.io/finalizer\nis added for CSI volumes.\nThe following is an example:\nName:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d\nLabels:          <none>\nAnnotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com\nFinalizers:\n[\nkubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer\n]\nStorageClass:    fast\nStatus:          Bound\nClaim:           demo-app/nginx-logs\nReclaim Policy:  Delete\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        200Mi\nNode Affinity:   <none>\nMessage:\nSource:\nType:              CSI\n(\na Container Storage Interface\n(\nCSI\n)\nvolume\nsource\n)\nDriver:            csi.vsphere.vmware.com\nFSType:            ext4\nVolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd\nReadOnly:\nfalse\nVolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity\n=\n1648442357185-8081-csi.vsphere.vmware.com\ntype\n=\nvSphere CNS Block Volume\nEvents:                <none>\nWhen the\nCSIMigration{provider}\nfeature flag is enabled for a specific in-tree volume plugin,\nthe\nkubernetes.io/pv-controller\nfinalizer is replaced by the\nexternal-provisioner.volume.kubernetes.io/finalizer\nfinalizer.\nThe finalizers ensure that the PV object is removed only after the volume is deleted\nfrom the storage backend provided the reclaim policy of the PV is\nDelete\n. This\nalso ensures that the volume is deleted from storage backend irrespective of the\norder of deletion of PV and PVC.\nReserving a PersistentVolume\nThe control plane can\nbind PersistentVolumeClaims to matching PersistentVolumes\nin the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.\nBy specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding\nbetween that specific PV and PVC. If the PersistentVolume exists and has not reserved\nPersistentVolumeClaims through its\nclaimRef\nfield, then the PersistentVolume and\nPersistentVolumeClaim will be bound.\nThe binding happens regardless of some volume matching criteria, including node affinity.\nThe control plane still checks that\nstorage class\n,\naccess modes, and requested storage size are valid.\napiVersion\n:\nv1\nkind\n:\nPersistentVolumeClaim\nmetadata\n:\nname\n:\nfoo-pvc\nnamespace\n:\nfoo\nspec\n:\nstorageClassName\n:\n\"\"\n# Empty string must be explicitly set otherwise default StorageClass will be set\nvolumeName\n:\nfoo-pv\n...\nThis method does not guarantee any binding privileges to the PersistentVolume.\nIf other PersistentVolumeClaims could use the PV that you specify, you first\nneed to reserve that storage volume. Specify the relevant PersistentVolumeClaim\nin the\nclaimRef\nfield of the PV so that other PVCs can not bind to it.\napiVersion\n:\nv1\nkind\n:\nPersistentVolume\nmetadata\n:\nname\n:\nfoo-pv\nspec\n:\nstorageClassName\n:\n\"\"\nclaimRef\n:\nname\n:\nfoo-pvc\nnamespace\n:\nfoo\n...\nThis is useful if you want to consume PersistentVolumes that have their\npersistentVolumeReclaimPolicy\nset\nto\nRetain\n, including cases where you are reusing an existing PV.\nExpanding Persistent Volumes Claims\nFEATURE STATE:\nKubernetes v1.24 [stable]\nSupport for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand\nthe following types of volumes:\ncsi\n(including some CSI migrated\nvolme types)\nflexVolume (deprecated)\nportworxVolume (deprecated)\nYou can only expand a PVC if its storage class's\nallowVolumeExpansion\nfield is set to true.\napiVersion\n:\nstorage.k8s.io/v1\nkind\n:\nStorageClass\nmetadata\n:\nname\n:\nexample-vol-default\nprovisioner\n:\nvendor-name.example/magicstorage\nparameters\n:\nresturl\n:\n\"http://192.168.10.100:8080\"\nrestuser\n:\n\"\"\nsecretNamespace\n:\n\"\"\nsecretName\n:\n\"\"\nallowVolumeExpansion\n:\ntrue\nTo request a larger volume for a PVC, edit the PVC object and specify a larger\nsize. This triggers expansion of the volume that backs the underlying PersistentVolume. A\nnew PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.\nWarning:\nDirectly editing the size of a PersistentVolume can prevent an automatic resize of that volume.\nIf you edit the capacity of a PersistentVolume, and then edit the\n.spec\nof a matching\nPersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,\nthen no storage resize happens.\nThe Kubernetes control plane will see that the desired state of both resources matches,\nconclude that the backing volume size has been manually\nincreased and that no resize is necessary.\nCSI Volume expansion\nFEATURE STATE:\nKubernetes v1.24 [stable]\nSupport for expanding CSI volumes is enabled by default but it also requires a\nspecific CSI driver to support volume expansion. Refer to documentation of the\nspecific CSI driver for more information.\nResizing a volume containing a file system\nYou can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.\nWhen a volume contains a file system, the file system is only resized when a new Pod is using\nthe PersistentVolumeClaim in\nReadWrite\nmode. File system expansion is either done when a Pod is starting up\nor when a Pod is running and the underlying file system supports online expansion.\nFlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the\nRequiresFSResize\ncapability to\ntrue\n. The FlexVolume can be resized on Pod restart.\nResizing an in-use PersistentVolumeClaim\nFEATURE STATE:\nKubernetes v1.24 [stable]\nIn this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.\nAny in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.\nThis feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that\nuses the PVC before the expansion can complete.\nSimilar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.\nNote:\nFlexVolume resize is possible only when the underlying driver supports resize.\nRecovering from Failure when Expanding Volumes\nIf a user specifies a new size that is too big to be satisfied by underlying\nstorage system, expansion of PVC will be continuously retried until user or\ncluster administrator takes some action. This can be undesirable and hence\nKubernetes provides following methods of recovering from such failures.\nManually with Cluster Administrator access\nBy requesting expansion to smaller size\nIf expanding underlying storage fails, the cluster administrator can manually\nrecover the Persistent Volume Claim (PVC) state and cancel the resize requests.\nOtherwise, the resize requests are continuously retried by the controller without\nadministrator intervention.\nMark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC)\nwith\nRetain\nreclaim policy.\nDelete the PVC. Since PV has\nRetain\nreclaim policy - we will not lose any data\nwhen we recreate the PVC.\nDelete the\nclaimRef\nentry from PV specs, so as new PVC can bind to it.\nThis should make the PV\nAvailable\n.\nRe-create the PVC with smaller size than PV and set\nvolumeName\nfield of the\nPVC to the name of the PV. This should bind new PVC to existing PV.\nDon't forget to restore the reclaim policy of the PV.\nIf expansion has failed for a PVC, you can retry expansion with a\nsmaller size than the previously requested value. To request a new expansion attempt with a\nsmaller proposed size, edit\n.spec.resources\nfor that PVC and choose a value that is less than the\nvalue you previously tried.\nThis is useful if expansion to a higher value did not succeed because of capacity constraint.\nIf that has happened, or you suspect that it might have, you can retry expansion by specifying a\nsize that is within the capacity limits of underlying storage provider. You can monitor status of\nresize operation by watching\n.status.allocatedResourceStatuses\nand events on the PVC.\nNote that,\nalthough you can specify a lower amount of storage than what was requested previously,\nthe new value must still be higher than\n.status.capacity\n.\nKubernetes does not support shrinking a PVC to less than its current size.\nTypes of Persistent Volumes\nPersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:\ncsi\n- Container Storage Interface (CSI)\nfc\n- Fibre Channel (FC) storage\nhostPath\n- HostPath volume\n(for single node testing only; WILL NOT WORK in a multi-node cluster;\nconsider using\nlocal\nvolume instead)\niscsi\n- iSCSI (SCSI over IP) storage\nlocal\n- local storage devices\nmounted on nodes.\nnfs\n- Network File System (NFS) storage\nThe following types of PersistentVolume are deprecated but still available.\nIf you are using these volume types except for\nflexVolume\n,\ncephfs\nand\nrbd\n,\nplease install corresponding CSI drivers.\nawsElasticBlockStore\n- AWS Elastic Block Store (EBS)\n(\nmigration on by default\nstarting v1.23)\nazureDisk\n- Azure Disk\n(\nmigration on by default\nstarting v1.23)\nazureFile\n- Azure File\n(\nmigration on by default\nstarting v1.24)\ncinder\n- Cinder (OpenStack block storage)\n(\nmigration on by default\nstarting v1.21)\nflexVolume\n- FlexVolume\n(\ndeprecated\nstarting v1.23, no migration plan and no plan to remove support)\ngcePersistentDisk\n- GCE Persistent Disk\n(\nmigration on by default\nstarting v1.23)\nportworxVolume\n- Portworx volume\n(\nmigration on by default\nstarting v1.31)\nvsphereVolume\n- vSphere VMDK volume\n(\nmigration on by default\nstarting v1.25)\nOlder versions of Kubernetes also supported the following in-tree PersistentVolume types:\ncephfs\n(\nnot available\nstarting v1.31)\nflocker\n- Flocker storage.\n(\nnot available\nstarting v1.25)\nglusterfs\n- GlusterFS storage.\n(\nnot available\nstarting v1.26)\nphotonPersistentDisk\n- Photon controller persistent disk.\n(\nnot available\nstarting v1.15)\nquobyte\n- Quobyte volume.\n(\nnot available\nstarting v1.25)\nrbd\n- Rados Block Device (RBD) volume\n(\nnot available\nstarting v1.31)\nscaleIO\n- ScaleIO volume.\n(\nnot available\nstarting v1.21)\nstorageos\n- StorageOS volume.\n(\nnot available\nstarting v1.25)\nPersistent Volumes\nEach PV contains a spec and status, which is the specification and status of the volume.\nThe name of a PersistentVolume object must be a valid\nDNS subdomain name\n.\napiVersion\n:\nv1\nkind\n:\nPersistentVolume\nmetadata\n:\nname\n:\npv0003\nspec\n:\ncapacity\n:\nstorage\n:\n5Gi\nvolumeMode\n:\nFilesystem\naccessModes\n:\n- ReadWriteOnce\npersistentVolumeReclaimPolicy\n:\nRecycle\nstorageClassName\n:\nslow\nmountOptions\n:\n- hard\n- nfsvers=4.1\nnfs\n:\npath\n:\n/tmp\nserver\n:\n172.17.0.2\nNote:\nHelper programs relating to the volume type may be required for consumption of\na PersistentVolume within a cluster. In this example, the PersistentVolume is\nof type NFS and the helper program /sbin/mount.nfs is required to support the\nmounting of NFS filesystems.\nCapacity\nGenerally, a PV will have a specific storage capacity. This is set using the PV's\ncapacity\nattribute which is a\nQuantity\nvalue.\nCurrently, storage size is the only resource that can be set or requested.\nFuture attributes may include IOPS, throughput, etc.\nVolume Mode\nFEATURE STATE:\nKubernetes v1.18 [stable]\nKubernetes supports two\nvolumeModes\nof PersistentVolumes:\nFilesystem\nand\nBlock\n.\nvolumeMode\nis an optional API parameter.\nFilesystem\nis the default mode used when\nvolumeMode\nparameter is omitted.\nA volume with\nvolumeMode: Filesystem\nis\nmounted\ninto Pods into a directory. If the volume\nis backed by a block device and the device is empty, Kubernetes creates a filesystem\non the device before mounting it for the first time.\nYou can set the value of\nvolumeMode\nto\nBlock\nto use a volume as a raw block device.\nSuch volume is presented into a Pod as a block device, without any filesystem on it.\nThis mode is useful to provide a Pod the fastest possible way to access a volume, without\nany filesystem layer between the Pod and the volume. On the other hand, the application\nrunning in the Pod must know how to handle a raw block device.\nSee\nRaw Block Volume Support\nfor an example on how to use a volume with\nvolumeMode: Block\nin a Pod.\nAccess Modes\nA PersistentVolume can be mounted on a host in any way supported by the resource\nprovider. As shown in the table below, providers will have different capabilities\nand each PV's access modes are set to the specific modes supported by that particular\nvolume. For example, NFS can support multiple read/write clients, but a specific\nNFS PV might be exported on the server as read-only. Each PV gets its own set of\naccess modes describing that specific PV's capabilities.\nThe access modes are:\nReadWriteOnce\nthe volume can be mounted as read-write by a single node. ReadWriteOnce access\nmode still can allow multiple pods to access (read from or write to) that volume when the pods are\nrunning on the same node. For single pod access, please see ReadWriteOncePod.\nReadOnlyMany\nthe volume can be mounted as read-only by many nodes.\nReadWriteMany\nthe volume can be mounted as read-write by many nodes.\nReadWriteOncePod\nFEATURE STATE:\nKubernetes v1.29 [stable]\nthe volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod\naccess mode if you want to ensure that only one pod across the whole cluster can\nread that PVC or write to it.\nNote:\nThe\nReadWriteOncePod\naccess mode is only supported for\nCSI\nvolumes and Kubernetes version\n1.22+. To use this feature you will need to update the following\nCSI sidecars\nto these versions or greater:\ncsi-provisioner:v3.0.0+\ncsi-attacher:v3.3.0+\ncsi-resizer:v1.3.0+\nIn the CLI, the access modes are abbreviated to:\nRWO - ReadWriteOnce\nROX - ReadOnlyMany\nRWX - ReadWriteMany\nRWOP - ReadWriteOncePod\nNote:\nKubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes.\nIn some cases, the volume access modes also constrain where the PersistentVolume can be mounted.\nVolume access modes do\nnot\nenforce write protection once the storage has been mounted.\nEven if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany,\nthey don't set any constraints on the volume. For example, even if a PersistentVolume is\ncreated as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes\nare specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.\nImportant!\nA volume can only be mounted using one access mode at a time,\neven if it supports many.\nVolume Plugin\nReadWriteOnce\nReadOnlyMany\nReadWriteMany\nReadWriteOncePod\nAzureFile\nâœ“\nâœ“\nâœ“\n-\nCephFS\nâœ“\nâœ“\nâœ“\n-\nCSI\ndepends on the driver\ndepends on the driver\ndepends on the driver\ndepends on the driver\nFC\nâœ“\nâœ“\n-\n-\nFlexVolume\nâœ“\nâœ“\ndepends on the driver\n-\nHostPath\nâœ“\n-\n-\n-\niSCSI\nâœ“\nâœ“\n-\n-\nNFS\nâœ“\nâœ“\nâœ“\n-\nRBD\nâœ“\nâœ“\n-\n-\nVsphereVolume\nâœ“\n-\n- (works when Pods are collocated)\n-\nPortworxVolume\nâœ“\n-\nâœ“\n-\nClass\nA PV can have a class, which is specified by setting the\nstorageClassName\nattribute to the name of a\nStorageClass\n.\nA PV of a particular class can only be bound to PVCs requesting\nthat class. A PV with no\nstorageClassName\nhas no class and can only be bound\nto PVCs that request no particular class.\nIn the past, the annotation\nvolume.beta.kubernetes.io/storage-class\nwas used instead\nof the\nstorageClassName\nattribute. This annotation is still working; however,\nit will become fully deprecated in a future Kubernetes release.\nReclaim Policy\nCurrent reclaim policies are:\nRetain -- manual reclamation\nRecycle -- basic scrub (\nrm -rf /thevolume/*\n)\nDelete -- delete the volume\nFor Kubernetes 1.34, only\nnfs\nand\nhostPath\nvolume types support recycling.\nMount Options\nA Kubernetes administrator can specify additional mount options for when a\nPersistent Volume is mounted on a node.\nNote:\nNot all Persistent Volume types support mount options.\nThe following volume types support mount options:\ncsi\n(including CSI migrated volume types)\niscsi\nnfs\nMount options are not validated. If a mount option is invalid, the mount fails.\nIn the past, the annotation\nvolume.beta.kubernetes.io/mount-options\nwas used instead\nof the\nmountOptions\nattribute. This annotation is still working; however,\nit will become fully deprecated in a future Kubernetes release.\nNode Affinity\nNote:\nFor most volume types, you do not need to set this field.\nYou need to explicitly set this for\nlocal\nvolumes.\nA PV can specify node affinity to define constraints that limit what nodes this\nvolume can be accessed from. Pods that use a PV will only be scheduled to nodes\nthat are selected by the node affinity. To specify node affinity, set\nnodeAffinity\nin the\n.spec\nof a PV. The\nPersistentVolume\nAPI reference has more details on this field.\nPhase\nA PersistentVolume will be in one of the following phases:\nAvailable\na free resource that is not yet bound to a claim\nBound\nthe volume is bound to a claim\nReleased\nthe claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster\nFailed\nthe volume has failed its (automated) reclamation\nYou can see the name of the PVC bound to the PV using\nkubectl describe persistentvolume <name>\n.\nPhase transition timestamp\nFEATURE STATE:\nKubernetes v1.31 [stable]\n(enabled by default)\nThe\n.status\nfield for a PersistentVolume can include an alpha\nlastPhaseTransitionTime\nfield. This field records\nthe timestamp of when the volume last transitioned its phase. For newly created\nvolumes the phase is set to\nPending\nand\nlastPhaseTransitionTime\nis set to\nthe current time.\nPersistentVolumeClaims\nEach PVC contains a spec and status, which is the specification and status of the claim.\nThe name of a PersistentVolumeClaim object must be a valid\nDNS subdomain name\n.\napiVersion\n:\nv1\nkind\n:\nPersistentVolumeClaim\nmetadata\n:\nname\n:\nmyclaim\nspec\n:\naccessModes\n:\n- ReadWriteOnce\nvolumeMode\n:\nFilesystem\nresources\n:\nrequests\n:\nstorage\n:\n8Gi\nstorageClassName\n:\nslow\nselector\n:\nmatchLabels\n:\nrelease\n:\n\"stable\"\nmatchExpressions\n:\n- {\nkey: environment, operator: In, values\n:\n[dev]}\nAccess Modes\nClaims use\nthe same conventions as volumes\nwhen requesting\nstorage with specific access modes.\nVolume Modes\nClaims use\nthe same convention as volumes\nto indicate the\nconsumption of the volume as either a filesystem or block device.\nVolume Name\nClaims can use the\nvolumeName\nfield to explicitly bind to a specific PersistentVolume. You can also leave\nvolumeName\nunset, indicating that you'd like Kubernetes to set up a new PersistentVolume\nthat matches the claim.\nIf the specified PV is already bound to another PVC, the binding will be stuck\nin a pending state.\nResources\nClaims, like Pods, can request specific quantities of a resource. In this case,\nthe request is for storage. The same\nresource model\napplies to both volumes and claims.\nNote:\nFor\nFilesystem\nvolumes, the storage request refers to the \"outer\" volume size\n(i.e. the allocated size from the storage backend).\nThis means that the writeable size may be slightly lower for providers that\nbuild a filesystem on top of a block device, due to filesystem overhead.\nThis is especially visible with XFS, where many metadata features are enabled by default.\nSelector\nClaims can specify a\nlabel selector\nto further filter the set of volumes.\nOnly the volumes whose labels match the selector can be bound to the claim.\nThe selector can consist of two fields:\nmatchLabels\n- the volume must have a label with this value\nmatchExpressions\n- a list of requirements made by specifying key, list of values,\nand operator that relates the key and values.\nValid operators include\nIn\n,\nNotIn\n,\nExists\n, and\nDoesNotExist\n.\nAll of the requirements, from both\nmatchLabels\nand\nmatchExpressions\n, are\nANDed together â€“ they must all be satisfied in order to match.\nClass\nA claim can request a particular class by specifying the name of a\nStorageClass\nusing the attribute\nstorageClassName\n.\nOnly PVs of the requested class, ones with the same\nstorageClassName\nas the PVC,\ncan be bound to the PVC.\nPVCs don't necessarily have to request a class. A PVC with its\nstorageClassName\nset\nequal to\n\"\"\nis always interpreted to be requesting a PV with no class, so it\ncan only be bound to PVs with no class (no annotation or one set equal to\n\"\"\n).\nA PVC with no\nstorageClassName\nis not quite the same and is treated differently\nby the cluster, depending on whether the\nDefaultStorageClass\nadmission plugin\nis turned on.\nIf the admission plugin is turned on, the administrator may specify a default StorageClass.\nAll PVCs that have no\nstorageClassName\ncan be bound only to PVs of that default.\nSpecifying a default StorageClass is done by setting the annotation\nstorageclass.kubernetes.io/is-default-class\nequal to\ntrue\nin a StorageClass object.\nIf the administrator does not specify a default, the cluster responds to PVC creation\nas if the admission plugin were turned off.\nIf more than one default StorageClass is specified, the newest default is used when\nthe PVC is dynamically provisioned.\nIf the admission plugin is turned off, there is no notion of a default StorageClass.\nAll PVCs that have\nstorageClassName\nset to\n\"\"\ncan be bound only to PVs\nthat have\nstorageClassName\nalso set to\n\"\"\n.\nHowever, PVCs with missing\nstorageClassName\ncan be updated later once default StorageClass becomes available.\nIf the PVC gets updated it will no longer bind to PVs that have\nstorageClassName\nalso set to\n\"\"\n.\nSee\nretroactive default StorageClass assignment\nfor more details.\nDepending on installation method, a default StorageClass may be deployed\nto a Kubernetes cluster by addon manager during installation.\nWhen a PVC specifies a\nselector\nin addition to requesting a StorageClass,\nthe requirements are ANDed together: only a PV of the requested class and with\nthe requested labels may be bound to the PVC.\nNote:\nCurrently, a PVC with a non-empty\nselector\ncan't have a PV dynamically provisioned for it.\nIn the past, the annotation\nvolume.beta.kubernetes.io/storage-class\nwas used instead\nof\nstorageClassName\nattribute. This annotation is still working; however,\nit won't be supported in a future Kubernetes release.\nRetroactive default StorageClass assignment\nFEATURE STATE:\nKubernetes v1.28 [stable]\nYou can create a PersistentVolumeClaim without specifying a\nstorageClassName\nfor the new PVC, and you can do so even when no default StorageClass exists\nin your cluster. In this case, the new PVC creates as you defined it, and the\nstorageClassName\nof that PVC remains unset until default becomes available.\nWhen a default StorageClass becomes available, the control plane identifies any\nexisting PVCs without\nstorageClassName\n. For the PVCs that either have an empty\nvalue for\nstorageClassName\nor do not have this key, the control plane then\nupdates those PVCs to set\nstorageClassName\nto match the new default StorageClass.\nIf you have an existing PVC where the\nstorageClassName\nis\n\"\"\n, and you configure\na default StorageClass, then this PVC will not get updated.\nIn order to keep binding to PVs with\nstorageClassName\nset to\n\"\"\n(while a default StorageClass is present), you need to set the\nstorageClassName\nof the associated PVC to\n\"\"\n.\nThis behavior helps administrators change default StorageClass by removing the\nold one first and then creating or setting another one. This brief window while\nthere is no default causes PVCs without\nstorageClassName\ncreated at that time\nto not have any default, but due to the retroactive default StorageClass\nassignment this way of changing defaults is safe.\nClaims As Volumes\nPods access storage by using the claim as a volume. Claims must exist in the\nsame namespace as the Pod using the claim. The cluster finds the claim in the\nPod's namespace and uses it to get the PersistentVolume backing the claim.\nThe volume is then mounted to the host and into the Pod.\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\nmypod\nspec\n:\ncontainers\n:\n-\nname\n:\nmyfrontend\nimage\n:\nnginx\nvolumeMounts\n:\n-\nmountPath\n:\n\"/var/www/html\"\nname\n:\nmypd\nvolumes\n:\n-\nname\n:\nmypd\npersistentVolumeClaim\n:\nclaimName\n:\nmyclaim\nA Note on Namespaces\nPersistentVolumes binds are exclusive, and since PersistentVolumeClaims are\nnamespaced objects, mounting claims with \"Many\" modes (\nROX\n,\nRWX\n) is only\npossible within one namespace.\nPersistentVolumes typed\nhostPath\nA\nhostPath\nPersistentVolume uses a file or directory on the Node to emulate\nnetwork-attached storage. See\nan example of\nhostPath\ntyped volume\n.\nRaw Block Volume Support\nFEATURE STATE:\nKubernetes v1.18 [stable]\nThe following volume plugins support raw block volumes, including dynamic provisioning where\napplicable:\nCSI (including some CSI migrated volume types)\nFC (Fibre Channel)\niSCSI\nLocal volume\nPersistentVolume using a Raw Block Volume\napiVersion\n:\nv1\nkind\n:\nPersistentVolume\nmetadata\n:\nname\n:\nblock-pv\nspec\n:\ncapacity\n:\nstorage\n:\n10Gi\naccessModes\n:\n- ReadWriteOnce\nvolumeMode\n:\nBlock\npersistentVolumeReclaimPolicy\n:\nRetain\nfc\n:\ntargetWWNs\n:\n[\n\"50060e801049cfd1\"\n]\nlun\n:\n0\nreadOnly\n:\nfalse\nPersistentVolumeClaim requesting a Raw Block Volume\napiVersion\n:\nv1\nkind\n:\nPersistentVolumeClaim\nmetadata\n:\nname\n:\nblock-pvc\nspec\n:\naccessModes\n:\n- ReadWriteOnce\nvolumeMode\n:\nBlock\nresources\n:\nrequests\n:\nstorage\n:\n10Gi\nPod specification adding Raw Block Device path in container\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\npod-with-block-volume\nspec\n:\ncontainers\n:\n-\nname\n:\nfc-container\nimage\n:\nfedora:26\ncommand\n:\n[\n\"/bin/sh\"\n,\n\"-c\"\n]\nargs\n:\n[\n\"tail -f /dev/null\"\n]\nvolumeDevices\n:\n-\nname\n:\ndata\ndevicePath\n:\n/dev/xvda\nvolumes\n:\n-\nname\n:\ndata\npersistentVolumeClaim\n:\nclaimName\n:\nblock-pvc\nNote:\nWhen adding a raw block device for a Pod, you specify the device path in the\ncontainer instead of a mount path.\nBinding Block Volumes\nIf a user requests a raw block volume by indicating this using the\nvolumeMode\nfield in the PersistentVolumeClaim spec, the binding rules differ slightly from\nprevious releases that didn't consider this mode as part of the spec.\nListed is a table of possible combinations the user and admin might specify for\nrequesting a raw block device. The table indicates if the volume will be bound or\nnot given the combinations: Volume binding matrix for statically provisioned volumes:\nPV volumeMode\nPVC volumeMode\nResult\nunspecified\nunspecified\nBIND\nunspecified\nBlock\nNO BIND\nunspecified\nFilesystem\nBIND\nBlock\nunspecified\nNO BIND\nBlock\nBlock\nBIND\nBlock\nFilesystem\nNO BIND\nFilesystem\nFilesystem\nBIND\nFilesystem\nBlock\nNO BIND\nFilesystem\nunspecified\nBIND\nNote:\nOnly statically provisioned volumes are supported for alpha release. Administrators\nshould take care to consider these values when working with raw block devices.\nVolume Snapshot and Restore Volume from Snapshot Support\nFEATURE STATE:\nKubernetes v1.20 [stable]\nVolume snapshots only support the out-of-tree CSI volume plugins.\nFor details, see\nVolume Snapshots\n.\nIn-tree volume plugins are deprecated. You can read about the deprecated volume\nplugins in the\nVolume Plugin FAQ\n.\nCreate a PersistentVolumeClaim from a Volume Snapshot\napiVersion\n:\nv1\nkind\n:\nPersistentVolumeClaim\nmetadata\n:\nname\n:\nrestore-pvc\nspec\n:\nstorageClassName\n:\ncsi-hostpath-sc\ndataSource\n:\nname\n:\nnew-snapshot-test\nkind\n:\nVolumeSnapshot\napiGroup\n:\nsnapshot.storage.k8s.io\naccessModes\n:\n- ReadWriteOnce\nresources\n:\nrequests\n:\nstorage\n:\n10Gi\nVolume Cloning\nVolume Cloning\nonly available for CSI volume plugins.\nCreate PersistentVolumeClaim from an existing PVC\napiVersion\n:\nv1\nkind\n:\nPersistentVolumeClaim\nmetadata\n:\nname\n:\ncloned-pvc\nspec\n:\nstorageClassName\n:\nmy-csi-plugin\ndataSource\n:\nname\n:\nexisting-src-pvc-name\nkind\n:\nPersistentVolumeClaim\naccessModes\n:\n- ReadWriteOnce\nresources\n:\nrequests\n:\nstorage\n:\n10Gi\nVolume populators and data sources\nFEATURE STATE:\nKubernetes v1.24 [beta]\nKubernetes supports custom volume populators.\nTo use custom volume populators, you must enable the\nAnyVolumeDataSource\nfeature gate\nfor\nthe kube-apiserver and kube-controller-manager.\nVolume populators take advantage of a PVC spec field called\ndataSourceRef\n. Unlike the\ndataSource\nfield, which can only contain either a reference to another PersistentVolumeClaim\nor to a VolumeSnapshot, the\ndataSourceRef\nfield can contain a reference to any object in the\nsame namespace, except for core objects other than PVCs. For clusters that have the feature\ngate enabled, use of the\ndataSourceRef\nis preferred over\ndataSource\n.\nCross namespace data sources\nFEATURE STATE:\nKubernetes v1.26 [alpha]\nKubernetes supports cross namespace volume data sources.\nTo use cross namespace volume data sources, you must enable the\nAnyVolumeDataSource\nand\nCrossNamespaceVolumeDataSource\nfeature gates\nfor\nthe kube-apiserver and kube-controller-manager.\nAlso, you must enable the\nCrossNamespaceVolumeDataSource\nfeature gate for the csi-provisioner.\nEnabling the\nCrossNamespaceVolumeDataSource\nfeature gate allows you to specify\na namespace in the dataSourceRef field.\nNote:\nWhen you specify a namespace for a volume data source, Kubernetes checks for a\nReferenceGrant in the other namespace before accepting the reference.\nReferenceGrant is part of the\ngateway.networking.k8s.io\nextension APIs.\nSee\nReferenceGrant\nin the Gateway API documentation for details.\nThis means that you must extend your Kubernetes cluster with at least ReferenceGrant from the\nGateway API before you can use this mechanism.\nData source references\nThe\ndataSourceRef\nfield behaves almost the same as the\ndataSource\nfield. If one is\nspecified while the other is not, the API server will give both fields the same value. Neither\nfield can be changed after creation, and attempting to specify different values for the two\nfields will result in a validation error. Therefore the two fields will always have the same\ncontents.\nThere are two differences between the\ndataSourceRef\nfield and the\ndataSource\nfield that\nusers should be aware of:\nThe\ndataSource\nfield ignores invalid values (as if the field was blank) while the\ndataSourceRef\nfield never ignores values and will cause an error if an invalid value is\nused. Invalid values are any core object (objects with no apiGroup) except for PVCs.\nThe\ndataSourceRef\nfield may contain different types of objects, while the\ndataSource\nfield\nonly allows PVCs and VolumeSnapshots.\nWhen the\nCrossNamespaceVolumeDataSource\nfeature is enabled, there are additional differences:\nThe\ndataSource\nfield only allows local objects, while the\ndataSourceRef\nfield allows\nobjects in any namespaces.\nWhen namespace is specified,\ndataSource\nand\ndataSourceRef\nare not synced.\nUsers should always use\ndataSourceRef\non clusters that have the feature gate enabled, and\nfall back to\ndataSource\non clusters that do not. It is not necessary to look at both fields\nunder any circumstance. The duplicated values with slightly different semantics exist only for\nbackwards compatibility. In particular, a mixture of older and newer controllers are able to\ninteroperate because the fields are the same.\nUsing volume populators\nVolume populators are\ncontrollers\nthat can\ncreate non-empty volumes, where the contents of the volume are determined by a Custom Resource.\nUsers create a populated volume by referring to a Custom Resource using the\ndataSourceRef\nfield:\napiVersion\n:\nv1\nkind\n:\nPersistentVolumeClaim\nmetadata\n:\nname\n:\npopulated-pvc\nspec\n:\ndataSourceRef\n:\nname\n:\nexample-name\nkind\n:\nExampleDataSource\napiGroup\n:\nexample.storage.k8s.io\naccessModes\n:\n- ReadWriteOnce\nresources\n:\nrequests\n:\nstorage\n:\n10Gi\nBecause volume populators are external components, attempts to create a PVC that uses one\ncan fail if not all the correct components are installed. External controllers should generate\nevents on the PVC to provide feedback on the status of the creation, including warnings if\nthe PVC cannot be created due to some missing component.\nYou can install the alpha\nvolume data source validator\ncontroller into your cluster. That controller generates warning Events on a PVC in the case that no populator\nis registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the\nresponsibility of that populator controller to report Events that relate to volume creation and issues during\nthe process.\nUsing a cross-namespace volume data source\nFEATURE STATE:\nKubernetes v1.26 [alpha]\nCreate a ReferenceGrant to allow the namespace owner to accept the reference.\nYou define a populated volume by specifying a cross namespace volume data source\nusing the\ndataSourceRef\nfield. You must already have a valid ReferenceGrant\nin the source namespace:\napiVersion\n:\ngateway.networking.k8s.io/v1beta1\nkind\n:\nReferenceGrant\nmetadata\n:\nname\n:\nallow-ns1-pvc\nnamespace\n:\ndefault\nspec\n:\nfrom\n:\n-\ngroup\n:\n\"\"\nkind\n:\nPersistentVolumeClaim\nnamespace\n:\nns1\nto\n:\n-\ngroup\n:\nsnapshot.storage.k8s.io\nkind\n:\nVolumeSnapshot\nname\n:\nnew-snapshot-demo\napiVersion\n:\nv1\nkind\n:\nPersistentVolumeClaim\nmetadata\n:\nname\n:\nfoo-pvc\nnamespace\n:\nns1\nspec\n:\nstorageClassName\n:\nexample\naccessModes\n:\n- ReadWriteOnce\nresources\n:\nrequests\n:\nstorage\n:\n1Gi\ndataSourceRef\n:\napiGroup\n:\nsnapshot.storage.k8s.io\nkind\n:\nVolumeSnapshot\nname\n:\nnew-snapshot-demo\nnamespace\n:\ndefault\nvolumeMode\n:\nFilesystem\nWriting Portable Configuration\nIf you're writing configuration templates or examples that run on a wide range of clusters\nand need persistent storage, it is recommended that you use the following pattern:\nInclude PersistentVolumeClaim objects in your bundle of config (alongside\nDeployments, ConfigMaps, etc).\nDo not include PersistentVolume objects in the config, since the user instantiating\nthe config may not have permission to create PersistentVolumes.\nGive the user the option of providing a storage class name when instantiating\nthe template.\nIf the user provides a storage class name, put that value into the\npersistentVolumeClaim.storageClassName\nfield.\nThis will cause the PVC to match the right storage\nclass if the cluster has StorageClasses enabled by the admin.\nIf the user does not provide a storage class name, leave the\npersistentVolumeClaim.storageClassName\nfield as nil. This will cause a\nPV to be automatically provisioned for the user with the default StorageClass\nin the cluster. Many cluster environments have a default StorageClass installed,\nor administrators can create their own default StorageClass.\nIn your tooling, watch for PVCs that are not getting bound after some time\nand surface this to the user, as this may indicate that the cluster has no\ndynamic storage support (in which case the user should create a matching PV)\nor the cluster has no storage system (in which case the user cannot deploy\nconfig requiring PVCs).\nWhat's next\nLearn more about\nCreating a PersistentVolume\n.\nLearn more about\nCreating a PersistentVolumeClaim\n.\nRead the\nPersistent Storage design document\n.\nAPI references\nRead about the APIs described in this page:\nPersistentVolume\nPersistentVolumeClaim\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified August 05, 2025 at 9:52 AM PST:\nUpdate docs for recover from expansion failure feature (4cb930c989)\nPersistentVolume API reference\nPersistentVolumeClaim API reference\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/concepts/storage/persistent-volumes/"}}
{"text": "Configure Liveness, Readiness and Startup Probes | Kubernetes\nConfigure Liveness, Readiness and Startup Probes\nThis page shows how to configure liveness, readiness and startup probes for containers.\nFor more information about probes, see\nLiveness, Readiness and Startup Probes\nThe\nkubelet\nuses\nliveness probes to know when to restart a container. For example, liveness\nprobes could catch a deadlock, where an application is running, but unable to\nmake progress. Restarting a container in such a state can help to make the\napplication more available despite bugs.\nA common pattern for liveness probes is to use the same low-cost HTTP endpoint\nas for readiness probes, but with a higher failureThreshold. This ensures that the pod\nis observed as not-ready for some period of time before it is hard killed.\nThe kubelet uses readiness probes to know when a container is ready to start\naccepting traffic. One use of this signal is to control which Pods are used as\nbackends for Services. A Pod is considered ready when its\nReady\ncondition\nis true. When a Pod is not ready, it is removed from Service load balancers.\nA Pod's\nReady\ncondition is false when its Node's\nReady\ncondition is not true,\nwhen one of the Pod's\nreadinessGates\nis false, or when at least one of its containers\nis not ready.\nThe kubelet uses startup probes to know when a container application has started.\nIf such a probe is configured, liveness and readiness probes do not start until\nit succeeds, making sure those probes don't interfere with the application startup.\nThis can be used to adopt liveness checks on slow starting containers, avoiding them\ngetting killed by the kubelet before they are up and running.\nCaution:\nLiveness probes can be a powerful way to recover from application failures, but\nthey should be used with caution. Liveness probes must be configured carefully\nto ensure that they truly indicate unrecoverable application failure, for example a deadlock.\nNote:\nIncorrect implementation of liveness probes can lead to cascading failures. This results in\nrestarting of container under high load; failed client requests as your application became less\nscalable; and increased workload on remaining pods due to some failed pods.\nUnderstand the difference between readiness and liveness probes and when to apply them for your app.\nBefore you begin\nYou need to have a Kubernetes cluster, and the kubectl command-line tool must\nbe configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a\ncluster, you can create one by using\nminikube\nor you can use one of these Kubernetes playgrounds:\niximiuz Labs\nKillercoda\nKodeKloud\nPlay with Kubernetes\nDefine a liveness command\nMany applications running for long periods of time eventually transition to\nbroken states, and cannot recover except by being restarted. Kubernetes provides\nliveness probes to detect and remedy such situations.\nIn this exercise, you create a Pod that runs a container based on the\nregistry.k8s.io/busybox:1.27.2\nimage. Here is the configuration file for the Pod:\npods/probe/exec-liveness.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nlabels\n:\ntest\n:\nliveness\nname\n:\nliveness-exec\nspec\n:\ncontainers\n:\n-\nname\n:\nliveness\nimage\n:\nregistry.k8s.io/busybox:1.27.2\nargs\n:\n- /bin/sh\n- -c\n- touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600\nlivenessProbe\n:\nexec\n:\ncommand\n:\n- cat\n- /tmp/healthy\ninitialDelaySeconds\n:\n5\nperiodSeconds\n:\n5\nIn the configuration file, you can see that the Pod has a single\nContainer\n.\nThe\nperiodSeconds\nfield specifies that the kubelet should perform a liveness\nprobe every 5 seconds. The\ninitialDelaySeconds\nfield tells the kubelet that it\nshould wait 5 seconds before performing the first probe. To perform a probe, the\nkubelet executes the command\ncat /tmp/healthy\nin the target container. If the\ncommand succeeds, it returns 0, and the kubelet considers the container to be alive and\nhealthy. If the command returns a non-zero value, the kubelet kills the container\nand restarts it.\nWhen the container starts, it executes this command:\n/bin/sh -c\n\"touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600\"\nFor the first 30 seconds of the container's life, there is a\n/tmp/healthy\nfile.\nSo during the first 30 seconds, the command\ncat /tmp/healthy\nreturns a success\ncode. After 30 seconds,\ncat /tmp/healthy\nreturns a failure code.\nCreate the Pod:\nkubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yaml\nWithin 30 seconds, view the Pod events:\nkubectl describe pod liveness-exec\nThe output indicates that no liveness probes have failed yet:\nType    Reason     Age   From               Message\n----    ------     ----  ----               -------\nNormal  Scheduled  11s   default-scheduler  Successfully assigned default/liveness-exec to node01\nNormal  Pulling    9s    kubelet, node01    Pulling image \"registry.k8s.io/busybox:1.27.2\"\nNormal  Pulled     7s    kubelet, node01    Successfully pulled image \"registry.k8s.io/busybox:1.27.2\"\nNormal  Created    7s    kubelet, node01    Created container liveness\nNormal  Started    7s    kubelet, node01    Started container liveness\nAfter 35 seconds, view the Pod events again:\nkubectl describe pod liveness-exec\nAt the bottom of the output, there are messages indicating that the liveness\nprobes have failed, and the failed containers have been killed and recreated.\nType     Reason     Age                From               Message\n----     ------     ----               ----               -------\nNormal   Scheduled  57s                default-scheduler  Successfully assigned default/liveness-exec to node01\nNormal   Pulling    55s                kubelet, node01    Pulling image \"registry.k8s.io/busybox:1.27.2\"\nNormal   Pulled     53s                kubelet, node01    Successfully pulled image \"registry.k8s.io/busybox:1.27.2\"\nNormal   Created    53s                kubelet, node01    Created container liveness\nNormal   Started    53s                kubelet, node01    Started container liveness\nWarning  Unhealthy  10s (x3 over 20s)  kubelet, node01    Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory\nNormal   Killing    10s                kubelet, node01    Container liveness failed liveness probe, will be restarted\nWait another 30 seconds, and verify that the container has been restarted:\nkubectl get pod liveness-exec\nThe output shows that\nRESTARTS\nhas been incremented. Note that the\nRESTARTS\ncounter\nincrements as soon as a failed container comes back to the running state:\nNAME            READY     STATUS    RESTARTS   AGE\nliveness-exec   1/1       Running   1          1m\nDefine a liveness HTTP request\nAnother kind of liveness probe uses an HTTP GET request. Here is the configuration\nfile for a Pod that runs a container based on the\nregistry.k8s.io/e2e-test-images/agnhost\nimage.\npods/probe/http-liveness.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nlabels\n:\ntest\n:\nliveness\nname\n:\nliveness-http\nspec\n:\ncontainers\n:\n-\nname\n:\nliveness\nimage\n:\nregistry.k8s.io/e2e-test-images/agnhost:2.40\nargs\n:\n- liveness\nlivenessProbe\n:\nhttpGet\n:\npath\n:\n/healthz\nport\n:\n8080\nhttpHeaders\n:\n-\nname\n:\nCustom-Header\nvalue\n:\nAwesome\ninitialDelaySeconds\n:\n3\nperiodSeconds\n:\n3\nIn the configuration file, you can see that the Pod has a single container.\nThe\nperiodSeconds\nfield specifies that the kubelet should perform a liveness\nprobe every 3 seconds. The\ninitialDelaySeconds\nfield tells the kubelet that it\nshould wait 3 seconds before performing the first probe. To perform a probe, the\nkubelet sends an HTTP GET request to the server that is running in the container\nand listening on port 8080. If the handler for the server's\n/healthz\npath\nreturns a success code, the kubelet considers the container to be alive and\nhealthy. If the handler returns a failure code, the kubelet kills the container\nand restarts it.\nAny code greater than or equal to 200 and less than 400 indicates success. Any\nother code indicates failure.\nYou can see the source code for the server in\nserver.go\n.\nFor the first 10 seconds that the container is alive, the\n/healthz\nhandler\nreturns a status of 200. After that, the handler returns a status of 500.\nhttp.\nHandleFunc\n(\n\"/healthz\"\n,\nfunc\n(w http.ResponseWriter, r\n*\nhttp.Request) {\nduration\n:=\ntime.\nNow\n().\nSub\n(started)\nif\nduration.\nSeconds\n() >\n10\n{\nw.\nWriteHeader\n(\n500\n)\nw.\nWrite\n([]\nbyte\n(fmt.\nSprintf\n(\n\"error: %v\"\n, duration.\nSeconds\n())))\n}\nelse\n{\nw.\nWriteHeader\n(\n200\n)\nw.\nWrite\n([]\nbyte\n(\n\"ok\"\n))\n}\n})\nThe kubelet starts performing health checks 3 seconds after the container starts.\nSo the first couple of health checks will succeed. But after 10 seconds, the health\nchecks will fail, and the kubelet will kill and restart the container.\nTo try the HTTP liveness check, create a Pod:\nkubectl apply -f https://k8s.io/examples/pods/probe/http-liveness.yaml\nAfter 10 seconds, view Pod events to verify that liveness probes have failed and\nthe container has been restarted:\nkubectl describe pod liveness-http\nIn releases after v1.13, local HTTP proxy environment variable settings do not\naffect the HTTP liveness probe.\nDefine a TCP liveness probe\nA third type of liveness probe uses a TCP socket. With this configuration, the\nkubelet will attempt to open a socket to your container on the specified port.\nIf it can establish a connection, the container is considered healthy, if it\ncan't it is considered a failure.\npods/probe/tcp-liveness-readiness.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\ngoproxy\nlabels\n:\napp\n:\ngoproxy\nspec\n:\ncontainers\n:\n-\nname\n:\ngoproxy\nimage\n:\nregistry.k8s.io/goproxy:0.1\nports\n:\n-\ncontainerPort\n:\n8080\nreadinessProbe\n:\ntcpSocket\n:\nport\n:\n8080\ninitialDelaySeconds\n:\n15\nperiodSeconds\n:\n10\nlivenessProbe\n:\ntcpSocket\n:\nport\n:\n8080\ninitialDelaySeconds\n:\n15\nperiodSeconds\n:\n10\nAs you can see, configuration for a TCP check is quite similar to an HTTP check.\nThis example uses both readiness and liveness probes. The kubelet will run the\nfirst liveness probe 15 seconds after the container starts. This will attempt to\nconnect to the\ngoproxy\ncontainer on port 8080. If the liveness probe fails,\nthe container will be restarted. The kubelet will continue to run this check\nevery 10 seconds.\nIn addition to the liveness probe, this configuration includes a readiness\nprobe. The kubelet will run the first readiness probe 15 seconds after the\ncontainer starts. Similar to the liveness probe, this will attempt to connect to\nthe\ngoproxy\ncontainer on port 8080. If the probe succeeds, the Pod will be\nmarked as ready and will receive traffic from services. If the readiness probe\nfails, the pod will be marked unready and will not receive traffic from any\nservices.\nTo try the TCP liveness check, create a Pod:\nkubectl apply -f https://k8s.io/examples/pods/probe/tcp-liveness-readiness.yaml\nAfter 15 seconds, view Pod events to verify that liveness probes:\nkubectl describe pod goproxy\nDefine a gRPC liveness probe\nFEATURE STATE:\nKubernetes v1.27 [stable]\nIf your application implements the\ngRPC Health Checking Protocol\n,\nthis example shows how to configure Kubernetes to use it for application liveness checks.\nSimilarly you can configure readiness and startup probes.\nHere is an example manifest:\npods/probe/grpc-liveness.yaml\napiVersion\n:\nv1\nkind\n:\nPod\nmetadata\n:\nname\n:\netcd-with-grpc\nspec\n:\ncontainers\n:\n-\nname\n:\netcd\nimage\n:\nregistry.k8s.io/etcd:3.5.1-0\ncommand\n:\n[\n\"/usr/local/bin/etcd\"\n,\n\"--data-dir\"\n,\n\"/var/lib/etcd\"\n,\n\"--listen-client-urls\"\n,\n\"http://0.0.0.0:2379\"\n,\n\"--advertise-client-urls\"\n,\n\"http://127.0.0.1:2379\"\n,\n\"--log-level\"\n,\n\"debug\"\n]\nports\n:\n-\ncontainerPort\n:\n2379\nlivenessProbe\n:\ngrpc\n:\nport\n:\n2379\ninitialDelaySeconds\n:\n10\nTo use a gRPC probe,\nport\nmust be configured. If you want to distinguish probes of different types\nand probes for different features you can use the\nservice\nfield.\nYou can set\nservice\nto the value\nliveness\nand make your gRPC Health Checking endpoint\nrespond to this request differently than when you set\nservice\nset to\nreadiness\n.\nThis lets you use the same endpoint for different kinds of container health check\nrather than listening on two different ports.\nIf you want to specify your own custom service name and also specify a probe type,\nthe Kubernetes project recommends that you use a name that concatenates\nthose. For example:\nmyservice-liveness\n(using\n-\nas a separator).\nNote:\nUnlike HTTP or TCP probes, you cannot specify the health check port by name, and you\ncannot configure a custom hostname.\nConfiguration problems (for example: incorrect port or service, unimplemented health checking protocol)\nare considered a probe failure, similar to HTTP and TCP probes.\nTo try the gRPC liveness check, create a Pod using the command below.\nIn the example below, the etcd pod is configured to use gRPC liveness probe.\nkubectl apply -f https://k8s.io/examples/pods/probe/grpc-liveness.yaml\nAfter 15 seconds, view Pod events to verify that the liveness check has not failed:\nkubectl describe pod etcd-with-grpc\nWhen using a gRPC probe, there are some technical details to be aware of:\nThe probes run against the pod IP address or its hostname.\nBe sure to configure your gRPC endpoint to listen on the Pod's IP address.\nThe probes do not support any authentication parameters (like\n-tls\n).\nThere are no error codes for built-in probes. All errors are considered as probe failures.\nIf\nExecProbeTimeout\nfeature gate is set to\nfalse\n, grpc-health-probe does\nnot\nrespect the\ntimeoutSeconds\nsetting (which defaults to 1s), while built-in probe would fail on timeout.\nUse a named port\nYou can use a named\nport\nfor HTTP and TCP probes. gRPC probes do not support named ports.\nFor example:\nports\n:\n-\nname\n:\nliveness-port\ncontainerPort\n:\n8080\nlivenessProbe\n:\nhttpGet\n:\npath\n:\n/healthz\nport\n:\nliveness-port\nProtect slow starting containers with startup probes\nSometimes, you have to deal with applications that require additional startup\ntime on their first initialization. In such cases, it can be tricky to set up\nliveness probe parameters without compromising the fast response to deadlocks\nthat motivated such a probe. The solution is to set up a startup probe with the\nsame command, HTTP or TCP check, with a\nfailureThreshold * periodSeconds\nlong\nenough to cover the worst case startup time.\nSo, the previous example would become:\nports\n:\n-\nname\n:\nliveness-port\ncontainerPort\n:\n8080\nlivenessProbe\n:\nhttpGet\n:\npath\n:\n/healthz\nport\n:\nliveness-port\nfailureThreshold\n:\n1\nperiodSeconds\n:\n10\nstartupProbe\n:\nhttpGet\n:\npath\n:\n/healthz\nport\n:\nliveness-port\nfailureThreshold\n:\n30\nperiodSeconds\n:\n10\nThanks to the startup probe, the application will have a maximum of 5 minutes\n(30 * 10 = 300s) to finish its startup.\nOnce the startup probe has succeeded once, the liveness probe takes over to\nprovide a fast response to container deadlocks.\nIf the startup probe never succeeds, the container is killed after 300s and\nsubject to the pod's\nrestartPolicy\n.\nDefine readiness probes\nSometimes, applications are temporarily unable to serve traffic.\nFor example, an application might need to load large data or configuration\nfiles during startup, or depend on external services after startup.\nIn such cases, you don't want to kill the application,\nbut you don't want to send it requests either. Kubernetes provides\nreadiness probes to detect and mitigate these situations. A pod with containers\nreporting that they are not ready does not receive traffic through Kubernetes\nServices.\nNote:\nReadiness probes runs on the container during its whole lifecycle.\nCaution:\nThe readiness and liveness probes do not depend on each other to succeed.\nIf you want to wait before executing a readiness probe, you should use\ninitialDelaySeconds\nor a\nstartupProbe\n.\nReadiness probes are configured similarly to liveness probes. The only difference\nis that you use the\nreadinessProbe\nfield instead of the\nlivenessProbe\nfield.\nreadinessProbe\n:\nexec\n:\ncommand\n:\n- cat\n- /tmp/healthy\ninitialDelaySeconds\n:\n5\nperiodSeconds\n:\n5\nConfiguration for HTTP and TCP readiness probes also remains identical to\nliveness probes.\nReadiness and liveness probes can be used in parallel for the same container.\nUsing both can ensure that traffic does not reach a container that is not ready\nfor it, and that containers are restarted when they fail.\nConfigure Probes\nProbes\nhave a number of fields that you can use to more precisely control the behavior of startup,\nliveness and readiness checks:\ninitialDelaySeconds\n: Number of seconds after the container has started before startup,\nliveness or readiness probes are initiated. If a startup probe is defined, liveness and\nreadiness probe delays do not begin until the startup probe has succeeded. In some older\nKubernetes versions, the initialDelaySeconds might be ignored if periodSeconds was set to\na value higher than initialDelaySeconds. However, in current versions, initialDelaySeconds\nis always honored and the probe will not start until after this initial delay. Defaults to\n0 seconds. Minimum value is 0.\nperiodSeconds\n: How often (in seconds) to perform the probe. Default to 10 seconds.\nThe minimum value is 1.\nWhile a container is not Ready, the\nReadinessProbe\nmay be executed at times other than\nthe configured\nperiodSeconds\ninterval. This is to make the Pod ready faster.\ntimeoutSeconds\n: Number of seconds after which the probe times out.\nDefaults to 1 second. Minimum value is 1.\nsuccessThreshold\n: Minimum consecutive successes for the probe to be considered successful\nafter having failed. Defaults to 1. Must be 1 for liveness and startup Probes.\nMinimum value is 1.\nfailureThreshold\n: After a probe fails\nfailureThreshold\ntimes in a row, Kubernetes\nconsiders that the overall check has failed: the container is\nnot\nready/healthy/live.\nDefaults to 3. Minimum value is 1.\nFor the case of a startup or liveness probe, if at least\nfailureThreshold\nprobes have\nfailed, Kubernetes treats the container as unhealthy and triggers a restart for that\nspecific container. The kubelet honors the setting of\nterminationGracePeriodSeconds\nfor that container.\nFor a failed readiness probe, the kubelet continues running the container that failed\nchecks, and also continues to run more probes; because the check failed, the kubelet\nsets the\nReady\ncondition\non the Pod to\nfalse\n.\nterminationGracePeriodSeconds\n: configure a grace period for the kubelet to wait between\ntriggering a shut down of the failed container, and then forcing the container runtime to stop\nthat container.\nThe default is to inherit the Pod-level value for\nterminationGracePeriodSeconds\n(30 seconds if not specified), and the minimum value is 1.\nSee\nprobe-level\nterminationGracePeriodSeconds\nfor more detail.\nCaution:\nIncorrect implementation of readiness probes may result in an ever growing number\nof processes in the container, and resource starvation if this is left unchecked.\nHTTP probes\nHTTP probes\nhave additional fields that can be set on\nhttpGet\n:\nhost\n: Host name to connect to, defaults to the pod IP. You probably want to\nset \"Host\" in\nhttpHeaders\ninstead.\nscheme\n: Scheme to use for connecting to the host (HTTP or HTTPS). Defaults to \"HTTP\".\npath\n: Path to access on the HTTP server. Defaults to \"/\".\nhttpHeaders\n: Custom headers to set in the request. HTTP allows repeated headers.\nport\n: Name or number of the port to access on the container. Number must be\nin the range 1 to 65535.\nFor an HTTP probe, the kubelet sends an HTTP request to the specified port and\npath to perform the check. The kubelet sends the probe to the Pod's IP address,\nunless the address is overridden by the optional\nhost\nfield in\nhttpGet\n. If\nscheme\nfield is set to\nHTTPS\n, the kubelet sends an HTTPS request skipping the\ncertificate verification. In most scenarios, you do not want to set the\nhost\nfield.\nHere's one scenario where you would set it. Suppose the container listens on 127.0.0.1\nand the Pod's\nhostNetwork\nfield is true. Then\nhost\n, under\nhttpGet\n, should be set\nto 127.0.0.1. If your pod relies on virtual hosts, which is probably the more common\ncase, you should not use\nhost\n, but rather set the\nHost\nheader in\nhttpHeaders\n.\nFor an HTTP probe, the kubelet sends two request headers in addition to the mandatory\nHost\nheader:\nUser-Agent\n: The default value is\nkube-probe/1.34\n,\nwhere\n1.34\nis the version of the kubelet.\nAccept\n: The default value is\n*/*\n.\nYou can override the default headers by defining\nhttpHeaders\nfor the probe.\nFor example:\nlivenessProbe\n:\nhttpGet\n:\nhttpHeaders\n:\n-\nname\n:\nAccept\nvalue\n:\napplication/json\nstartupProbe\n:\nhttpGet\n:\nhttpHeaders\n:\n-\nname\n:\nUser-Agent\nvalue\n:\nMyUserAgent\nYou can also remove these two headers by defining them with an empty value.\nlivenessProbe\n:\nhttpGet\n:\nhttpHeaders\n:\n-\nname\n:\nAccept\nvalue\n:\n\"\"\nstartupProbe\n:\nhttpGet\n:\nhttpHeaders\n:\n-\nname\n:\nUser-Agent\nvalue\n:\n\"\"\nNote:\nWhen the kubelet probes a Pod using HTTP, it only follows redirects if the redirect\nis to the same host. If the kubelet receives 11 or more redirects during probing, the probe is considered successful\nand a related Event is created:\nEvents:\nType     Reason        Age                     From               Message\n----     ------        ----                    ----               -------\nNormal   Scheduled     29m                     default-scheduler  Successfully assigned default/httpbin-7b8bc9cb85-bjzwn to daocloud\nNormal   Pulling       29m                     kubelet            Pulling image \"docker.io/kennethreitz/httpbin\"\nNormal   Pulled        24m                     kubelet            Successfully pulled image \"docker.io/kennethreitz/httpbin\" in 5m12.402735213s\nNormal   Created       24m                     kubelet            Created container httpbin\nNormal   Started       24m                     kubelet            Started container httpbin\nWarning  ProbeWarning  4m11s (x1197 over 24m)  kubelet            Readiness probe warning: Probe terminated redirects\nIf the kubelet receives a redirect where the hostname is different from the request, the outcome of the probe is treated as successful and kubelet creates an event to report the redirect failure.\nCaution:\nWhen processing an\nhttpGet\nprobe, the kubelet stops reading the response body after 10KiB.\nThe probe's success is determined solely by the response status code, which is found in the response headers.\nIf you probe an endpoint that returns a response body larger than\n10KiB\n,\nthe kubelet will still mark the probe as successful based on the status code,\nbut it will close the connection after reaching the 10KiB limit.\nThis abrupt closure can cause\nconnection reset by peer\nor\nbroken pipe errors\nto appear in your application's logs,\nwhich can be difficult to distinguish from legitimate network issues.\nFor reliable\nhttpGet\nprobes, it is strongly recommended to use dedicated health check endpoints\nthat return a minimal response body. If you must use an existing endpoint with a large payload,\nconsider using an\nexec\nprobe to perform a HEAD request instead.\nTCP probes\nFor a TCP probe, the kubelet makes the probe connection at the node, not in the Pod, which\nmeans that you can not use a service name in the\nhost\nparameter since the kubelet is unable\nto resolve it.\nProbe-level\nterminationGracePeriodSeconds\nFEATURE STATE:\nKubernetes v1.28 [stable]\nIn 1.25 and above, users can specify a probe-level\nterminationGracePeriodSeconds\nas part of the probe specification. When both a pod- and probe-level\nterminationGracePeriodSeconds\nare set, the kubelet will use the probe-level value.\nWhen setting the\nterminationGracePeriodSeconds\n, please note the following:\nThe kubelet always honors the probe-level\nterminationGracePeriodSeconds\nfield if\nit is present on a Pod.\nIf you have existing Pods where the\nterminationGracePeriodSeconds\nfield is set and\nyou no longer wish to use per-probe termination grace periods, you must delete\nthose existing Pods.\nFor example:\nspec\n:\nterminationGracePeriodSeconds\n:\n3600\n# pod-level\ncontainers\n:\n-\nname\n:\ntest\nimage\n:\n...\nports\n:\n-\nname\n:\nliveness-port\ncontainerPort\n:\n8080\nlivenessProbe\n:\nhttpGet\n:\npath\n:\n/healthz\nport\n:\nliveness-port\nfailureThreshold\n:\n1\nperiodSeconds\n:\n60\n# Override pod-level terminationGracePeriodSeconds #\nterminationGracePeriodSeconds\n:\n60\nProbe-level\nterminationGracePeriodSeconds\ncannot be set for readiness probes.\nIt will be rejected by the API server.\nWhat's next\nLearn more about\nContainer Probes\n.\nYou can also read the API references for:\nPod\n, and specifically:\ncontainer(s)\nprobe(s)\nFeedback\nWas this page helpful?\nYes\nNo\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on\nStack Overflow\n.\nOpen an issue in the\nGitHub Repository\nif you want to\nreport a problem\nor\nsuggest an improvement\n.\nLast modified October 16, 2025 at 9:42 AM PST:\ndocs/httpProbes: apply formatting suggestions from review (8edf73305c)\nEdit this page\nCreate child page\nCreate an issue\nPrint entire section", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"}}
