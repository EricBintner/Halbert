{"text": "Redis data types | Docs\n{\"categories\":[\"docs\",\"develop\",\"stack\",\"oss\",\"rs\",\"rc\",\"oss\",\"kubernetes\",\"clients\"],\"description\":\"Overview of data types supported by Redis\",\"title\":\"Redis data types\"}\nRedis for AI\nProducts\nProducts\nRedis Cloud\nFully managed and integrated with Google Cloud, Azure, and\nAWS.\nRedis Software\nSelf-managed software with enterprise-grade compliance and\nreliability.\nRedis Open Source\nIn-memory database for caching & streaming.\nTools\nRedis\nLangCache\nRedis\nInsight\nRedis\nData Integration\nClients\n& Connectors\nGet Redis\nDownloads\nResources\nConnect\nCustomer Stories\nPartners\nSupport\nCommunity\nEvents\n& Webinars\nProfessional Services\nLearn\nDocs\nCommands\nQuick\nstarts\nTutorials\nUniversity\nFAQs\nResources\nBlog\nLatest\nReleases\nNews\n& Updates\nSee how it works\nVisit Demo Center\nDocs\nPricing\nSearch\nLogin\nBook a\nmeeting\nTry Redis\nOpen search\nOpen main menu\nRedis data types\nOverview of data types supported by Redis\nRedis is a data structure server.\nAt its core, Redis provides a collection of native data types that help you solve a wide variety of problems, from\ncaching\nto\nqueuing\nto\nevent processing\n.\nBelow is a short description of each data type, with links to broader overviews and command references.\nEach overview includes a comprehensive tutorial with code samples.\nData types\nRedis Open Source\nimplements the following data types:\nString\nHash\nList\nSet\nSorted set\nVector set\nStream\nBitmap\nBitfield\nGeospatial\nJSON\nProbabilistic data types\nTime series\nStrings\nRedis strings\nare the most basic Redis data type, representing a sequence of bytes.\nFor more information, see:\nOverview of Redis strings\nRedis string command reference\nLists\nRedis lists\nare lists of strings sorted by insertion order.\nFor more information, see:\nOverview of Redis lists\nRedis list command reference\nSets\nRedis sets\nare unordered collections of unique strings that act like the sets from your favorite programming language (for example,\nJava HashSets\n,\nPython sets\n, and so on).\nWith a Redis set, you can add, remove, and test for existence in O(1) time (in other words, regardless of the number of set elements).\nFor more information, see:\nOverview of Redis sets\nRedis set command reference\nHashes\nRedis hashes\nare record types modeled as collections of field-value pairs.\nAs such, Redis hashes resemble\nPython dictionaries\n,\nJava HashMaps\n, and\nRuby hashes\n.\nFor more information, see:\nOverview of Redis hashes\nRedis hashes command reference\nSorted sets\nRedis sorted sets\nare collections of unique strings that maintain order by each string's associated score.\nFor more information, see:\nOverview of Redis sorted sets\nRedis sorted set command reference\nVector sets\nRedis vector sets\nare a specialized data type designed for managing high-dimensional vector data, enabling fast and efficient vector similarity search within Redis. Vector sets are optimized for use cases involving machine learning, recommendation systems, and semantic search, where each vector represents a data point in multi-dimensional space. Vector sets supports the\nHNSW\n(hierarchical navigable small world) algorithm, allowing you to store, index, and query vectors based on the cosine similarity metric. With vector sets, Redis provides native support for hybrid search, combining vector similarity with structured\nfilters\n.\nFor more information, see:\nOverview of Redis vector sets\nRedis vector set command reference\nStreams\nA\nRedis stream\nis a data structure that acts like an append-only log.\nStreams help record events in the order they occur and then syndicate them for processing.\nFor more information, see:\nOverview of Redis Streams\nRedis Streams command reference\nGeospatial indexes\nRedis geospatial indexes\nare useful for finding locations within a given geographic radius or bounding box.\nFor more information, see:\nOverview of Redis geospatial indexes\nRedis geospatial indexes command reference\nBitmaps\nRedis bitmaps\nlet you perform bitwise operations on strings.\nFor more information, see:\nOverview of Redis bitmaps\nRedis bitmap command reference\nBitfields\nRedis bitfields\nefficiently encode multiple counters in a string value.\nBitfields provide atomic get, set, and increment operations and support different overflow policies.\nFor more information, see:\nOverview of Redis bitfields\nThe\nBITFIELD\ncommand.\nJSON\nRedis JSON\nprovides\nstructured, hierarchical arrays and key-value objects that match\nthe popular\nJSON\ntext file\nformat. You can import JSON text into Redis objects and access,\nmodify, and query individual data elements.\nFor more information, see:\nOverview of Redis JSON\nJSON command reference\nProbabilistic data types\nThese data types let you gather and calculate statistics in a way\nthat is approximate but highly efficient. The following types are\navailable:\nHyperLogLog\nBloom filter\nCuckoo filter\nt-digest\nTop-K\nCount-min sketch\nHyperLogLog\nThe\nRedis HyperLogLog\ndata structures provide probabilistic estimates of the cardinality (i.e., number of elements) of large sets. For more information, see:\nOverview of Redis HyperLogLog\nRedis HyperLogLog command reference\nBloom filter\nRedis Bloom filters\nlet you check for the presence or absence of an element in a set. For more\ninformation, see:\nOverview of Redis Bloom filters\nBloom filter command reference\nCuckoo filter\nRedis Cuckoo filters\nlet you check for the presence or absence of an element in a set. They are similar to\nBloom filters\nbut with slightly different trade-offs between features\nand performance. For more information, see:\nOverview of Redis Cuckoo filters\nCuckoo filter command reference\nt-digest\nRedis t-digest\nstructures estimate percentiles from a stream of data values. For more\ninformation, see:\nRedis t-digest overview\nt-digest command reference\nTop-K\nRedis Top-K\nstructures estimate the ranking of a data point within a stream of values.\nFor more information, see:\nRedis Top-K overview\nTop-K command reference\nCount-min sketch\nRedis Count-min sketch\nestimate the frequency of a data point within a stream of values.\nFor more information, see:\nRedis Count-min sketch overview\nCount-min sketch command reference\nTime series\nRedis time series\nstructures let you store and query timestamped data points.\nFor more information, see:\nRedis time series overview\nCount-min sketch command reference\nAdding extensions\nTo extend the features provided by the included data types, use one of these options:\nWrite your own custom\nserver-side functions in Lua\n.\nWrite your own Redis module using the\nmodules API\nor check out the\ncommunity-supported modules\n.\nRATE THIS PAGE\n★\n★\n★\n★\n★\nBack to top â\nSubmit\nOn this page\nAll products\nRedis Enterprise\nRedis Cloud\nRedis Open Source\nRedis Insight\nRedis Enterprise for K8s\nRedis Data Integration\nClient Libraries\nESC", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://redis.io/docs/latest/develop/data-types/"}}
{"text": "Redis persistence | Docs\n{\"categories\":[\"docs\",\"operate\",\"stack\",\"oss\"],\"description\":\"How Redis writes data to disk\",\"title\":\"Redis persistence\"}\nRedis for AI\nProducts\nProducts\nRedis Cloud\nFully managed and integrated with Google Cloud, Azure, and\nAWS.\nRedis Software\nSelf-managed software with enterprise-grade compliance and\nreliability.\nRedis Open Source\nIn-memory database for caching & streaming.\nTools\nRedis\nLangCache\nRedis\nInsight\nRedis\nData Integration\nClients\n& Connectors\nGet Redis\nDownloads\nResources\nConnect\nCustomer Stories\nPartners\nSupport\nCommunity\nEvents\n& Webinars\nProfessional Services\nLearn\nDocs\nCommands\nQuick\nstarts\nTutorials\nUniversity\nFAQs\nResources\nBlog\nLatest\nReleases\nNews\n& Updates\nSee how it works\nVisit Demo Center\nDocs\nPricing\nSearch\nLogin\nBook a\nmeeting\nTry Redis\nOpen search\nOpen main menu\nRedis persistence\nHow Redis writes data to disk\nRedis Open Source\nPersistence refers to the writing of data to durable storage, such as a solid-state disk (SSD). Redis provides a range of persistence options. These include:\nRDB\n(Redis Database): RDB persistence performs point-in-time snapshots of your dataset at specified intervals.\nAOF\n(Append Only File): AOF persistence logs every write operation received by the server. These operations can then be replayed again at server startup, reconstructing the original dataset. Commands are logged using the same format as the Redis protocol itself.\nNo persistence\n: You can disable persistence completely. This is sometimes used when caching.\nRDB + AOF\n: You can also combine both AOF and RDB in the same instance.\nIf you'd rather not think about the tradeoffs between these different persistence strategies, you may want to consider\nRedis Enterprise's persistence options\n, which can be pre-configured using a UI.\nTo learn more about how to evaluate your Redis persistence strategy, read on.\nRDB advantages\nRDB is a very compact single-file point-in-time representation of your Redis data. RDB files are perfect for backups. For instance you may want to archive your RDB files every hour for the latest 24 hours, and to save an RDB snapshot every day for 30 days. This allows you to easily restore different versions of the data set in case of disasters.\nRDB is very good for disaster recovery, being a single compact file that can be transferred to far data centers, or onto Amazon S3 (possibly encrypted).\nRDB maximizes Redis performances since the only work the Redis parent process needs to do in order to persist is forking a child that will do all the rest. The parent process will never perform disk I/O or alike.\nRDB allows faster restarts with big datasets compared to AOF.\nOn replicas, RDB supports\npartial resynchronizations after restarts and failovers\n.\nRDB disadvantages\nRDB is NOT good if you need to minimize the chance of data loss in case Redis stops working (for example after a power outage). You can configure different\nsave points\nwhere an RDB is produced (for instance after at least five minutes and 100 writes against the data set, you can have multiple save points). However you'll usually create an RDB snapshot every five minutes or more, so in case of Redis stopping working without a correct shutdown for any reason you should be prepared to lose the latest minutes of data.\nRDB needs to fork() often in order to persist on disk using a child process. fork() can be time consuming if the dataset is big, and may result in Redis stopping serving clients for some milliseconds or even for one second if the dataset is very big and the CPU performance is not great. AOF also needs to fork() but less frequently and you can tune how often you want to rewrite your logs without any trade-off on durability.\nAOF advantages\nUsing AOF Redis is much more durable: you can have different fsync policies: no fsync at all, fsync every second, fsync at every query. With the default policy of fsync every second, write performance is still great. fsync is performed using a background thread and the main thread will try hard to perform writes when no fsync is in progress, so you can only lose one second worth of writes.\nThe AOF log is an append-only log, so there are no seeks, nor corruption problems if there is a power outage. Even if the log ends with a half-written command for some reason (disk full or other reasons) the redis-check-aof tool is able to fix it easily.\nRedis is able to automatically rewrite the AOF in background when it gets too big. The rewrite is completely safe as while Redis continues appending to the old file, a completely new one is produced with the minimal set of operations needed to create the current data set, and once this second file is ready Redis switches the two and starts appending to the new one.\nAOF contains a log of all the operations one after the other in an easy to understand and parse format. You can even easily export an AOF file. For instance even if you've accidentally flushed everything using the\nFLUSHALL\ncommand, as long as no rewrite of the log was performed in the meantime, you can still save your data set just by stopping the server, removing the latest command, and restarting Redis again.\nAOF disadvantages\nAOF files are usually bigger than the equivalent RDB files for the same dataset.\nAOF can be slower than RDB depending on the exact fsync policy. In general with fsync set to\nevery second\nperformance is still very high, and with fsync disabled it should be exactly as fast as RDB even under high load. Still RDB is able to provide more guarantees about the maximum latency even in the case of a huge write load.\nRedis < 7.0\nAOF can use a lot of memory if there are writes to the database during a rewrite (these are buffered in memory and written to the new AOF at the end).\nAll write commands that arrive during rewrite are written to disk twice.\nRedis could freeze writing and fsyncing these write commands to the new AOF file at the end of the rewrite.\nOk, so what should I use?\nThe general indication you should use both persistence methods is if\nyou want a degree of data safety comparable to what PostgreSQL can provide you.\nIf you care a lot about your data, but still can live with a few minutes of\ndata loss in case of disasters, you can simply use RDB alone.\nThere are many users using AOF alone, but we discourage it since to have an\nRDB snapshot from time to time is a great idea for doing database backups,\nfor faster restarts, and in the event of bugs in the AOF engine.\nThe following sections will illustrate a few more details about the two persistence models.\nSnapshotting\nBy default Redis saves snapshots of the dataset on disk, in a binary\nfile called\ndump.rdb\n. You can configure Redis to have it save the\ndataset every N seconds if there are at least M changes in the dataset,\nor you can manually call the\nSAVE\nor\nBGSAVE\ncommands.\nFor example, this configuration will make Redis automatically dump the\ndataset to disk every 60 seconds if at least 1000 keys changed:\nsave 60 1000\nThis strategy is known as\nsnapshotting\n.\nHow it works\nWhenever Redis needs to dump the dataset to disk, this is what happens:\nRedis\nforks\n. We now have a child\nand a parent process.\nThe child starts to write the dataset to a temporary RDB file.\nWhen the child is done writing the new RDB file, it replaces the old\none.\nThis method allows Redis to benefit from copy-on-write semantics.\nAppend-only file\nSnapshotting is not very durable. If your computer running Redis stops,\nyour power line fails, or you accidentally\nkill -9\nyour instance, the\nlatest data written to Redis will be lost.  While this may not be a big\ndeal for some applications, there are use cases for full durability, and\nin these cases Redis snapshotting alone is not a viable option.\nThe\nappend-only file\nis an alternative, fully-durable strategy for\nRedis.  It became available in version 1.1.\nYou can turn on the AOF in your configuration file:\nappendonly yes\nFrom now on, every time Redis receives a command that changes the\ndataset (e.g.\nSET\n) it will append it to the AOF.  When you restart\nRedis it will re-play the AOF to rebuild the state.\nSince Redis 7.0.0, Redis uses a multi part AOF mechanism.\nThat is, the original single AOF file is split into base file (at most one) and incremental files (there may be more than one).\nThe base file represents an initial (RDB or AOF format) snapshot of the data present when the AOF is\nrewritten\n.\nThe incremental files contains incremental changes since the last base AOF file was created. All these files are put in a separate directory and are tracked by a manifest file.\nLog rewriting\nThe AOF gets bigger and bigger as write operations are\nperformed.  For example, if you are incrementing a counter 100 times,\nyou'll end up with a single key in your dataset containing the final\nvalue, but 100 entries in your AOF. 99 of those entries are not needed\nto rebuild the current state.\nThe rewrite is completely safe.\nWhile Redis continues appending to the old file,\na completely new one is produced with the minimal set of operations needed to create the current data set,\nand once this second file is ready Redis switches the two and starts appending to the new one.\nSo Redis supports an interesting feature: it is able to rebuild the AOF\nin the background without interrupting service to clients. Whenever\nyou issue a\nBGREWRITEAOF\n, Redis will write the shortest sequence of\ncommands needed to rebuild the current dataset in memory.  If you're\nusing the AOF with Redis 2.2 you'll need to run\nBGREWRITEAOF\nfrom time to\ntime. Since Redis 2.4 is able to trigger log rewriting automatically (see the\nexample configuration file for more information).\nSince Redis 7.0.0, when an AOF rewrite is scheduled, the Redis parent process opens a new incremental AOF file to continue writing.\nThe child process executes the rewrite logic and generates a new base AOF.\nRedis will use a temporary manifest file to track the newly generated base file and incremental file.\nWhen they are ready, Redis will perform an atomic replacement operation to make this temporary manifest file take effect.\nIn order to avoid the problem of creating many incremental files in case of repeated failures and retries of an AOF rewrite,\nRedis introduces an AOF rewrite limiting mechanism to ensure that failed AOF rewrites are retried at a slower and slower rate.\nHow durable is the append only file?\nYou can configure how many times Redis will\nfsync\ndata on disk. There are\nthree options:\nappendfsync always\n:\nfsync\nevery time new commands are appended to the AOF. Very very slow, very safe. Note that the commands are appended to the AOF after a batch of commands from multiple clients or a pipeline are executed, so it means a single write and a single fsync (before sending the replies).\nappendfsync everysec\n:\nfsync\nevery second. Fast enough (since version 2.4 likely to be as fast as snapshotting), and you may lose 1 second of data if there is a disaster.\nappendfsync no\n: Never\nfsync\n, just put your data in the hands of the Operating System. The faster and less safe method. Normally Linux will flush data every 30 seconds with this configuration, but it's up to the kernel's exact tuning.\nThe suggested (and default) policy is to\nfsync\nevery second. It is\nboth fast and relatively safe. The\nalways\npolicy is very slow in\npractice, but it supports group commit, so if there are multiple parallel\nwrites Redis will try to perform a single\nfsync\noperation.\nWhat should I do if my AOF gets truncated?\nIt is possible the server crashed while writing the AOF file, or the\nvolume where the AOF file is stored was full at the time of writing. When this happens the\nAOF still contains consistent data representing a given point-in-time version\nof the dataset (that may be old up to one second with the default AOF fsync\npolicy), but the last command in the AOF could be truncated.\nThe latest major versions of Redis will be able to load the AOF anyway, just\ndiscarding the last non well formed command in the file. In this case the\nserver will emit a log like the following:\n* Reading RDB preamble from AOF file...\n* Reading the remaining AOF tail...\n# !!! Warning: short read while loading the AOF file !!!\n# !!! Truncating the AOF at offset 439 !!!\n# AOF loaded anyway because aof-load-truncated is enabled\nYou can change the default configuration to force Redis to stop in such\ncases if you want, but the default configuration is to continue regardless of\nthe fact the last command in the file is not well-formed, in order to guarantee\navailability after a restart.\nOlder versions of Redis may not recover, and may require the following steps:\nMake a backup copy of your AOF file.\nFix the original file using the\nredis-check-aof\ntool that ships with Redis:\n$ redis-check-aof --fix <filename>\nOptionally use\ndiff -u\nto check what is the difference between two files.\nRestart the server with the fixed file.\nWhat should I do if my AOF gets corrupted?\nIf the AOF file is not just truncated, but corrupted with invalid byte\nsequences in the middle, things are more complex. Redis will complain\nat startup and will abort:\n* Reading the remaining AOF tail...\n# Bad file format reading the append only file: make a backup of your AOF file, then use ./redis-check-aof --fix <filename>\nThe best thing to do is to run the\nredis-check-aof\nutility, initially without\nthe\n--fix\noption, then understand the problem, jump to the given\noffset in the file, and see if it is possible to manually repair the file:\nThe AOF uses the same format of the Redis protocol and is quite simple to fix\nmanually. Otherwise it is possible to let the utility fix the file for us, but\nin that case all the AOF portion from the invalid part to the end of the\nfile may be discarded, leading to a massive amount of data loss if the\ncorruption happened to be in the initial part of the file.\nHow it works\nLog rewriting uses the same copy-on-write trick already in use for\nsnapshotting.  This is how it works:\nRedis >= 7.0\nRedis\nforks\n, so now we have a child\nand a parent process.\nThe child starts writing the new base AOF in a temporary file.\nThe parent opens a new increments AOF file to continue writing updates.\nIf the rewriting fails, the old base and increment files (if there are any) plus this newly opened increment file represent the complete updated dataset,\nso we are safe.\nWhen the child is done rewriting the base file, the parent gets a signal,\nand uses the newly opened increment file and child generated base file to build a temp manifest,\nand persist it.\nProfit! Now Redis does an atomic exchange of the manifest files so that the result of this AOF rewrite takes effect. Redis also cleans up the old base file and any unused increment files.\nRedis < 7.0\nRedis\nforks\n, so now we have a child\nand a parent process.\nThe child starts writing the new AOF in a temporary file.\nThe parent accumulates all the new changes in an in-memory buffer (but\nat the same time it writes the new changes in the old append-only file,\nso if the rewriting fails, we are safe).\nWhen the child is done rewriting the file, the parent gets a signal,\nand appends the in-memory buffer at the end of the file generated by the\nchild.\nNow Redis atomically renames the new file into the old one,\nand starts appending new data into the new file.\nHow I can switch to AOF, if I'm currently using dump.rdb snapshots?\nIf you want to enable AOF in a server that is currently using RDB snapshots, you need to convert the data by enabling AOF via CONFIG command on the live server first.\nIMPORTANT:\nnot following this procedure (e.g. just changing the config and restarting the server) can result in data loss!\nRedis >= 2.2\nPreparations:\nMake a backup of your latest dump.rdb file.\nTransfer this backup to a safe place.\nSwitch to AOF on live database:\nEnable AOF:\nredis-cli config set appendonly yes\nOptionally disable RDB:\nredis-cli config set save \"\"\nMake sure writes are appended to the append only file correctly.\nIMPORTANT:\nUpdate your\nredis.conf\n(potentially through\nCONFIG REWRITE\n) and ensure that it matches the configuration above.\nIf you forget this step, when you restart the server, the configuration changes will be lost and the server will start again with the old configuration, resulting in a loss of your data.\nNext time you restart the server:\nBefore restarting the server, wait for AOF rewrite to finish persisting the data.\nYou can do that by watching\nINFO persistence\n, waiting for\naof_rewrite_in_progress\nand\naof_rewrite_scheduled\nto be\n0\n, and validating that\naof_last_bgrewrite_status\nis\nok\n.\nAfter restarting the server, check that your database contains the same number of keys it contained previously.\nRedis 2.0\nMake a backup of your latest dump.rdb file.\nTransfer this backup into a safe place.\nStop all the writes against the database!\nIssue a\nredis-cli BGREWRITEAOF\n. This will create the append only file.\nStop the server when Redis finished generating the AOF dump.\nEdit redis.conf end enable append only file persistence.\nRestart the server.\nMake sure that your database contains the same number of keys it contained before the switch.\nMake sure that writes are appended to the append only file correctly.\nInteractions between AOF and RDB persistence\nRedis >= 2.4 makes sure to avoid triggering an AOF rewrite when an RDB\nsnapshotting operation is already in progress, or allowing a\nBGSAVE\nwhile the\nAOF rewrite is in progress. This prevents two Redis background processes\nfrom doing heavy disk I/O at the same time.\nWhen snapshotting is in progress and the user explicitly requests a log\nrewrite operation using\nBGREWRITEAOF\nthe server will reply with an OK\nstatus code telling the user the operation is scheduled, and the rewrite\nwill start once the snapshotting is completed.\nIn the case both AOF and RDB persistence are enabled and Redis restarts the\nAOF file will be used to reconstruct the original dataset since it is\nguaranteed to be the most complete.\nBacking up Redis data\nBefore starting this section, make sure to read the following sentence:\nMake Sure to Backup Your Database\n. Disks break, instances in the cloud disappear, and so forth: no backups means huge risk of data disappearing into /dev/null.\nRedis is very data backup friendly since you can copy RDB files while the\ndatabase is running: the RDB is never modified once produced, and while it\ngets produced it uses a temporary name and is renamed into its final destination\natomically using rename(2) only when the new snapshot is complete.\nThis means that copying the RDB file is completely safe while the server is\nrunning. This is what we suggest:\nCreate a cron job in your server creating hourly snapshots of the RDB file in one directory, and daily snapshots in a different directory.\nEvery time the cron script runs, make sure to call the\nfind\ncommand to make sure too old snapshots are deleted: for instance you can take hourly snapshots for the latest 48 hours, and daily snapshots for one or two months. Make sure to name the snapshots with date and time information.\nAt least one time every day make sure to transfer an RDB snapshot\noutside your data center\nor at least\noutside the physical machine\nrunning your Redis instance.\nBacking up AOF persistence\nIf you run a Redis instance with only AOF persistence enabled, you can still perform backups.\nSince Redis 7.0.0, AOF files are split into multiple files which reside in a single directory determined by the\nappenddirname\nconfiguration.\nDuring normal operation all you need to do is copy/tar the files in this directory to achieve a backup. However, if this is done during a\nrewrite\n, you might end up with an invalid backup.\nTo work around this you must disable AOF rewrites during the backup:\nTurn off automatic rewrites with\nCONFIG SET\nauto-aof-rewrite-percentage 0\nMake sure you don't manually start a rewrite (using\nBGREWRITEAOF\n) during this time.\nCheck there's no current rewrite in progress using\nINFO\npersistence\nand verifying\naof_rewrite_in_progress\nis 0. If it's 1, then you'll need to wait for the rewrite to complete.\nNow you can safely copy the files in the\nappenddirname\ndirectory.\nRe-enable rewrites when done:\nCONFIG SET\nauto-aof-rewrite-percentage <prev-value>\nNote:\nIf you want to minimize the time AOF rewrites are disabled you may create hard links to the files in\nappenddirname\n(in step 3 above) and then re-enable rewrites (step 4) after the hard links are created.\nNow you can copy/tar the hardlinks and delete them when done. This works because Redis guarantees that it\nonly appends to files in this directory, or completely replaces them if necessary, so the content should be\nconsistent at any given point in time.\nNote:\nIf you want to handle the case of the server being restarted during the backup and make sure no rewrite will automatically start after the restart you can change step 1 above to also persist the updated configuration via\nCONFIG REWRITE\n.\nJust make sure to re-enable automatic rewrites when done (step 4) and persist it with another\nCONFIG REWRITE\n.\nPrior to version 7.0.0 backing up the AOF file can be done simply by copying the aof file (like backing up the RDB snapshot). The file may lack the final part\nbut Redis will still be able to load it (see the previous sections about\ntruncated AOF files\n).\nDisaster recovery\nDisaster recovery in the context of Redis is basically the same story as\nbackups, plus the ability to transfer those backups in many different external\ndata centers. This way data is secured even in the case of some catastrophic\nevent affecting the main data center where Redis is running and producing its\nsnapshots.\nWe'll review the most interesting disaster recovery techniques\nthat don't have too high costs.\nAmazon S3 and other similar services are a good way for implementing your disaster recovery system. Simply transfer your daily or hourly RDB snapshot to S3 in an encrypted form. You can encrypt your data using\ngpg -c\n(in symmetric encryption mode). Make sure to store your password in many different safe places (for instance give a copy to the most important people of your organization). It is recommended to use multiple storage services for improved data safety.\nTransfer your snapshots using SCP (part of SSH) to far servers. This is a fairly simple and safe route: get a small VPS in a place that is very far from you, install ssh there, and generate a ssh client key without passphrase, then add it in the\nauthorized_keys\nfile of your small VPS. You are ready to transfer backups in an automated fashion. Get at least two VPS in two different providers\nfor best results.\nIt is important to understand that this system can easily fail if not\nimplemented in the right way. At least, make absolutely sure that after the\ntransfer is completed you are able to verify the file size (that should match\nthe one of the file you copied) and possibly the SHA1 digest, if you are using\na VPS.\nYou also need some kind of independent alert system if the transfer of fresh\nbackups is not working for some reason.\nRATE THIS PAGE\n★\n★\n★\n★\n★\nBack to top â\nSubmit\nOn this page\nAll products\nRedis Enterprise\nRedis Cloud\nRedis Open Source\nRedis Insight\nRedis Enterprise for K8s\nRedis Data Integration\nClient Libraries\nESC", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://redis.io/docs/latest/operate/oss_and_stack/management/persistence/"}}
{"text": "memcached - a distributed memory object caching system\nHome\nAbout\nDownloads\nBlog\nMailing List\nDocs\nBugs\nSponsor Us!\nAbout Memcached\nmemcached is a high-performance, distributed memory object caching\nsystem, generic in nature, but originally intended for use in speeding\nup dynamic web applications by alleviating database load.\nYou can think of it as a short-term memory for your applications.\nWhat it Does\nmemcached allows you to take memory from parts of your system where\nyou have more than you need and make it accessible to areas where\nyou have less than you need.\nmemcached also allows you to make better use of your memory.  If you\nconsider the diagram to the right, you can see two deployment\nscenarios:\nEach node is completely independent (top).\nEach node can make use of memory from other nodes (bottom).\nThe first scenario illustrates the classic deployment strategy,\nhowever you'll find that it's both wasteful in the sense that the\ntotal cache size is a fraction of the actual capacity of your web\nfarm, but also in the amount of effort required to keep the\ncache consistent across all of those nodes.\nWith memcached, you can see that all of the servers are looking into\nthe same virtual pool of memory.  This means that a given item is\nalways stored and always retrieved from the same location in your\nentire web cluster.\nAlso, as the demand for your application grows\nto the point where you need to have more servers, it generally also\ngrows in terms of the data that must be regularly accessed.  A\ndeployment strategy where these two aspects of your system scale\ntogether just makes sense.\nThe illustration to the right only shows two web servers for\nsimplicity, but the property remains the same as the number\nincreases.  If you had fifty web servers, you'd still have a usable\ncache size of 64MB in the first example, but in the second, you'd\nhave 3.2GB of usable cache.\nOf course, you aren't required to use your web server's memory for\ncache.  Many memcached users have dedicated machines that are built\nto only be memcached servers.\nOrigin\nMemcached was originally developed by Brad Fitzpatrick for\nLiveJournal\nin 2003.\nContributors\ndormando\n(1105)\nDustin Sallings\n(214)\nBrad Fitzpatrick\n(164)\nTrond Norbye\n(130)\nPaul Lindner\n(58)\nToru Maesaka\n(34)\nSteven Grimm\n(25)\nDavid Carlier\n(23)\nKevin Lin\n(17)\nStanisÅaw Pitucha\n(16)\nAnatoly Vorobey\n(15)\nBrian Aker\n(15)\nSteve Yen\n(15)\nTomash Brechko\n(12)\nminkikim89\n(12)\nFei Hu\n(9)\nTharanga Gamaethige\n(9)\nKanak Kshetri\n(7)\nTomas Korbar\n(7)\nhachi\n(7)\nAaron Stone\n(6)\nDan McGee\n(6)\nGuillaume Delacour\n(6)\nOla Jeppsson\n(6)\nPeter (Stig) Edwards\n(6)\nSteve Wills\n(6)\nDaniel Schemmel\n(5)\nEvan Martin\n(5)\nFabrice Fontaine\n(5)\nFinn Frankis\n(5)\nMatt Ingenthron\n(5)\nMiroslav Lichvar\n(5)\nSailesh Mukil\n(5)\nxuesenliang\n(5)\nChris Goffinet\n(4)\nCraig Andrews\n(4)\nDavid CARLIER\n(4)\nEric McConville\n(4)\nJefty Negapatan\n(4)\nRemi Collet\n(4)\nTianon Gravi\n(4)\nVictor Kirkebo\n(4)\nAndrei Nigmatulin\n(3)\nCalin Iorgulescu\n(3)\nCameron Norman\n(3)\nDaniel PaÃ±eda\n(3)\nEric Lambert\n(3)\nJay Grizzard\n(3)\nPaul Furtado\n(3)\nQu Chen\n(3)\nsergiocarlos\n(3)\nAntony Dovgal\n(2)\nBernhard M. Wiedemann\n(2)\nBrion Vibber\n(2)\nCarl Myers\n(2)\nClint Byrum\n(2)\nColin Pitrat\n(2)\nCosimo Streppone\n(2)\nDavid Bremner\n(2)\nDmitry Volodin\n(2)\nDoug Porter\n(2)\nEric Hodel\n(2)\nEvan Miller\n(2)\nGiovanni Bechis\n(2)\nIqram Mahmud\n(2)\nJ. Grizzard\n(2)\nJason CHAN\n(2)\nJay Bonci\n(2)\nJean-Francois BUSTARRET\n(2)\nJosh Soref\n(2)\nKhem Raj\n(2)\nLinkerist\n(2)\nMat Hostetter\n(2)\nMonty Taylor\n(2)\nOlof Nord\n(2)\nPaolo Borelli\n(2)\nRicky Zhou\n(2)\nShiv Nagarajan\n(2)\nTyson Andre\n(2)\nVadim Pushtaev\n(2)\nYufei Hu\n(2)\njinyaoguo\n(2)\nmugitya03\n(2)\npkarumanchi9\n(2)\nç¥å°\n(2)\nAdam Chainz\n(1)\nAdam Dixon\n(1)\nAdam Szkoda\n(1)\nAdam Thomason\n(1)\nAlex Leone\n(1)\nAlexander Pyhalov\n(1)\nAli Saidi\n(1)\nAlwayswithme\n(1)\nAndre Azevedo Pinto\n(1)\nAndrew Drake\n(1)\nAndrew Glinskiy\n(1)\nAndrey Niakhaichyk\n(1)\nAnthony Ryan\n(1)\nArtemIsmagilov\n(1)\nArtur Bergman\n(1)\nBaptiste Mille-Mathias\n(1)\nBen Evans\n(1)\nCaleb Shay\n(1)\nCaptTofu\n(1)\nChang Song\n(1)\nCharmander\n(1)\nChen-Yu Tsai\n(1)\nClinton Webb\n(1)\nDagobert Michelsen\n(1)\nDan Christian\n(1)\nDaniel Byrne\n(1)\nDaniel Vasquez-Lopez\n(1)\nDanny Kopping\n(1)\nDavid Bohman\n(1)\nDavid J. M. Karlsen\n(1)\nDavid Oliveira\n(1)\nDavid Phillips\n(1)\nDavid Schoen\n(1)\nDmitry Isaykin\n(1)\nDon MacAskill\n(1)\nEiichi Tsukata\n(1)\nEli Bingham\n(1)\nElizabeth Mattijsen\n(1)\nEvan Klitzke\n(1)\nFangrui Song\n(1)\nFilipe Laborde\n(1)\nFordy\n(1)\nFumihiro Ito\n(1)\nGabe Van Engel\n(1)\nGabriel A. Samfira\n(1)\nGleicon Moraes\n(1)\nGordon Franke\n(1)\nGrant Mathews\n(1)\nGregor Jasny\n(1)\nGuido Iaquinti\n(1)\nHemal Shah\n(1)\nHervÃ© Beraud\n(1)\nHuzaifa Sidhpurwala\n(1)\nIain Wade\n(1)\nIan Miell\n(1)\nIliya\n(1)\nIng-eoking\n(1)\nJames Cohen\n(1)\nJamie McCarthy\n(1)\nJason Titus\n(1)\nJeff Lawson\n(1)\nJeremy Sowden\n(1)\nJoe Orton\n(1)\nJohan BergstrÃ¶m\n(1)\nJohn Leslie\n(1)\nJon Jensen\n(1)\nJonathan Bastien-Filiatrault\n(1)\nJonathan Steinert\n(1)\nJosh Kupershmidt\n(1)\nJuliy V. Chirkov\n(1)\nJunji Hashimoto\n(1)\nJÃ¸rgen Austvik\n(1)\nKenneth Steele\n(1)\nKeyur\n(1)\nKissPeter\n(1)\nKleber\n(1)\nLSmithx2\n(1)\nLevente Polyak\n(1)\nLisa Seelye\n(1)\nLÃ©on Brocard\n(1)\nMaksim Zhylinski\n(1)\nManish Katiyar\n(1)\nMark Hagger\n(1)\nMartin Tzvetanov Grigorov\n(1)\nMate Borcsok\n(1)\nMathieu CARBONNEAUX\n(1)\nMatt Fowles Kulukundis\n(1)\nMatthew Shafer\n(1)\nMattias Geniar\n(1)\nMaxim Dounin\n(1)\nMenghan\n(1)\nMichael Alan Dorman\n(1)\nMike Dillon\n(1)\nMiklos Vajna\n(1)\nNatanael Copa\n(1)\nNate\n(1)\nNathan Neulinger\n(1)\nNick\n(1)\nNick Frost\n(1)\nNick Pillitteri\n(1)\nOri Shalev\n(1)\nOskari Saarenmaa\n(1)\nPatrice Duroux\n(1)\nPaul Querna\n(1)\nPeter van Dijk\n(1)\nPierre-Yves Rofes\n(1)\nPiotr Balcer\n(1)\nPrudhviraj K\n(1)\nQian Li\n(1)\nRamasai\n(1)\nRaphael Isemann\n(1)\nRichard Russo\n(1)\nRiver Tarnell\n(1)\nRoman Mueller\n(1)\nRyan McCullagh\n(1)\nRyan T. Dean\n(1)\nRyan Tomayko\n(1)\nRyuichi Watanabe\n(1)\nSaman Barghi\n(1)\nSergei Trofimovich\n(1)\nSergio Durigan Junior\n(1)\nSharif Nassar\n(1)\nSimon Liu\n(1)\nSjon Hortensius\n(1)\nSridhar Samudrala\n(1)\nSteve Peters\n(1)\nTao Hui\n(1)\nTed Schundler\n(1)\nTheo Najim\n(1)\nThomas van Gulick\n(1)\nTim Yardley\n(1)\nTom Stellard\n(1)\nTomas Kalibera\n(1)\nTorsten Foertsch\n(1)\nVladimir\n(1)\nWing Lian\n(1)\nYongyue Sun\n(1)\nZheng Gu\n(1)\najccosta\n(1)\nakisssa\n(1)\nbitground\n(1)\nclark.kang\n(1)\ngithublvv\n(1)\nhayashier\n(1)\nhiracy\n(1)\niqr4m\n(1)\njs\n(1)\njwbee\n(1)\nkenvifire\n(1)\nkokke\n(1)\nkun\n(1)\nliu bo\n(1)\nliwenlong05\n(1)\nmckelvin\n(1)\nmdl\n(1)\nmeteorgan\n(1)\nmiwasson\n(1)\nneal-zhu\n(1)\nnirvanazc\n(1)\nphantom9999\n(1)\nprudhvi\n(1)\nq66\n(1)\ntheblop\n(1)\ntom\n(1)\nwangkang-xy\n(1)\nyuryur\n(1)\nzhoutai\n(1)\nä¼è¤æ´ä¹\n(1)\nThis page is maintained by Dormando. Logo/Banner images are Copyright (c) 2009-2018\nDormando, and may not be used without permission.\nLayout forked from Scott Chacon and Petr Baudis'\ngit-scm.com\nPlease contact the mailing list with suggestions and comments.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://memcached.org/about"}}
{"text": "The Varnish Users Guide — Varnish version trunk documentation\nNavigation\nindex\nnext\n|\nprevious\n|\nVarnish version trunk documentation\n»\nThe Varnish Users Guide\nThe Varnish Users Guide\n¶\nThe Varnish documentation consists of three main documents:\nThe Varnish Tutorial\nexplains the basics and gets you started with Varnish.\nThe Varnish Users Guide\n(this document), explains how Varnish works\nand how you can use it to improve your website.\nThe Varnish Reference Manual\ncontains hard facts and is useful for\nlooking up specific questions.\nAfter\nThe Big Varnish Picture\n, this Users Guide is organized in sections\nfollowing the major interfaces to Varnish as a service:\nStarting and running Varnish\nis about getting Varnish configured, with\nrespect to storage, sockets, security and how you can control and\ncommunicate with Varnish once it is running.\nVCL - Varnish Configuration Language\nis about getting Varnish to handle the\nHTTP requests the way you want, what to cache, how to cache it,\nmodifying HTTP headers etc. etc.\nReporting and statistics\nexplains how you can monitor what Varnish does,\nfrom a transactional level to aggregating statistics.\nVarnish and Website Performance\nis about tuning your website with Varnish.\nTroubleshooting Varnish\nis for locating and fixing common issues with Varnish.\nThe Big Varnish Picture\nStarting and running Varnish\nSecurity first\nRequired command line arguments\nCLI - bossing Varnish around\nStorage backends\nTransient Storage\nParameters\nSizing your cache\nVCL - Varnish Configuration Language\nVCL Syntax\nBuilt-in VCL\nRequest and response VCL objects\nBackend servers\nThe “none” backend\nMultiple backends\nBackends and virtual hosts in Varnish\nConnecting Through a Proxy\nDirectors\nHealth checks\nLayering\nDirector Resolution\nConnection Pooling\nHashing\nGrace mode and keep\nSeparate VCL files\nUsing inline C to extend Varnish\nVCL Examples\nDevice detection\nReporting and statistics\nLogging in Varnish\nStatistics\nVarnish and Website Performance\nAchieving a high hitrate\nThe role of HTTP Headers\nHTTP Vary\nCache misses\nUncacheable content\nPurging and banning\nCompression\nContent composition with Edge Side Includes\nExample: esi:include\nExample: esi:remove and <!–esi … –>\nWhat happens when it fails ?\nCan an ESI fragment also use ESI-includes ?\nDoing ESI on JSON and other non-XML’ish content\nIgnoring BOM in ESI objects\nESI on invalid XML\nESI includes with HTTPS protocol\nESI on partial responses (206)\nESI and return(vcl(…))\nESI and gzip compression\nTroubleshooting Varnish\nWhen Varnish won’t start\nVarnish is crashing - panics\nVarnish is crashing - stack overflows\nVarnish is crashing - segfaults\nVarnish gives me Guru meditation\nVarnish doesn’t cache\nPrevious topic\nNow what?\nNext topic\nThe Big Varnish Picture\nThis Page\nShow Source\nQuick search\nNavigation\nindex\nnext\n|\nprevious\n|\nVarnish version trunk documentation\n»\nThe Varnish Users Guide\n© Copyright 2010-2024, The Varnish Cache Contributors.\nCreated using\nSphinx\n5.3.0.", "metadata": {"source_type": "vendor_docs", "trust": "vendor_docs", "os": "linux", "distro": "ubuntu-22.04", "license": "CC BY-SA (assumed; verify per site)", "attribution_url": "https://varnish-cache.org/docs/trunk/users-guide/"}}
